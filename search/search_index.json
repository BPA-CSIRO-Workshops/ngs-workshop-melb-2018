{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Introduction to NGS Workshop The Introduction to NGS Data Analysis is a three-day, hands-on workshop that offers attendees a basic understanding of NGS data analysis workflows. The workshop provides hands-on computational experience in analysis of NGS data using common analytical approaches for ChIP-Seq, RNA-Seq data and de novo genome assembly. Workshop Content Topics covered by this workshop include: * An introduction to the command line interface and NGS file formats * Assessment of the quality of NGS sequence reads * Sequence alignment algorithms * Basic ChIP-Seq analysis * Basic RNA-Seq analysis * Short and long read de novo genome assembly This workshop will be delivered using a mixture of lectures, hands-on practical sessions, and open discussions. Acknowledgements This workshop was developed by Bioplatforms Australia, in partnership with CSIRO, and with support from the European Bioinformatics Institute (EBI), a member of the European Molecular Biology Laboratory (EMBL) in the UK. The workshop content has been maintained and updated by a network of dedicated [bioinformatics trainers] ( http://www.bioplatforms.com/bioinformatics-training/ ) from around Australia. The Bioinformatics Training Platform was developed in collaboration with the Monash Bioinformatics Platform and this workshop has been hosted on Interset infrastructure. This workshop is possible thanks to funding support from the NSW Office of Health and Medical Research and the Commonwealth Government National Collaborative Research Infrastructure Strategy (NCRIS).","title":"Home"},{"location":"#welcome-to-the-introduction-to-ngs-workshop","text":"The Introduction to NGS Data Analysis is a three-day, hands-on workshop that offers attendees a basic understanding of NGS data analysis workflows. The workshop provides hands-on computational experience in analysis of NGS data using common analytical approaches for ChIP-Seq, RNA-Seq data and de novo genome assembly.","title":"Welcome to the Introduction to NGS Workshop"},{"location":"#workshop-content","text":"Topics covered by this workshop include: * An introduction to the command line interface and NGS file formats * Assessment of the quality of NGS sequence reads * Sequence alignment algorithms * Basic ChIP-Seq analysis * Basic RNA-Seq analysis * Short and long read de novo genome assembly This workshop will be delivered using a mixture of lectures, hands-on practical sessions, and open discussions.","title":"Workshop Content"},{"location":"#acknowledgements","text":"This workshop was developed by Bioplatforms Australia, in partnership with CSIRO, and with support from the European Bioinformatics Institute (EBI), a member of the European Molecular Biology Laboratory (EMBL) in the UK. The workshop content has been maintained and updated by a network of dedicated [bioinformatics trainers] ( http://www.bioplatforms.com/bioinformatics-training/ ) from around Australia. The Bioinformatics Training Platform was developed in collaboration with the Monash Bioinformatics Platform and this workshop has been hosted on Interset infrastructure. This workshop is possible thanks to funding support from the NSW Office of Health and Medical Research and the Commonwealth Government National Collaborative Research Infrastructure Strategy (NCRIS).","title":"Acknowledgements"},{"location":"license/","text":"This work is licensed under a Creative Commons Attribution 3.0 Unported License and the below text is a summary of the main terms of the full Legal Code (the full license) available at http://creativecommons.org/licenses/by/3.0/legalcode . You are free: to copy, distribute, display, and perform the work to make derivative works to make commercial use of the work Under the following conditions: Attribution - You must give the original author credit. With the understanding that: Waiver - Any of the above conditions can be waived if you get permission from the copyright holder. Public Domain - Where the work or any of its elements is in the public domain under applicable law, that status is in no way affected by the license. Other Rights - In no way are any of the following rights affected by the license: Your fair dealing or fair use rights, or other applicable copyright exceptions and limitations; The author\u2019s moral rights; Rights other persons may have either in the work itself or in how the work is used, such as publicity or privacy rights. Notice - For any reuse or distribution, you must make clear to others the license terms of this work.","title":"License"},{"location":"alignment/","text":"NGS Read Mapping Bioinformatics Training Platform (BTP) Module: NGS Read Mapping with Bowtie2 Topic NGS Read Mapping with Bowtie2 Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes Perform the simple NGS data alignment task against one interested reference data Interpret and manipulate the mapping output using SAMtools Visualise the alignment via a standard genome browser, e.g. IGV browser Time Required 2 hrs License The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"NGS Read Mapping"},{"location":"alignment/#ngs-read-mapping","text":"Bioinformatics Training Platform (BTP) Module: NGS Read Mapping with Bowtie2 Topic NGS Read Mapping with Bowtie2 Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes Perform the simple NGS data alignment task against one interested reference data Interpret and manipulate the mapping output using SAMtools Visualise the alignment via a standard genome browser, e.g. IGV browser Time Required 2 hrs","title":"NGS Read Mapping"},{"location":"alignment/#license","text":"The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"License"},{"location":"alignment/handout/","text":"NGS Read Mapping Acknowledgements This module was developed by Bioplatforms Australia, in partnership with CSIRO, and with support from the European Bioinformatics Institute (EBI), a member of the European Molecular Biology Laboratory (EMBL) in the UK. The module content has been maintained and updated by a network of dedicated [bioinformatics trainers] ( http://www.bioplatforms.com/bioinformatics-training/ ) from around Australia. The Bioinformatics Training Platform was developed in collaboration with the Monash Bioinformatics Platform. The development of this module is also supported by the NSW Office of Health and Medical Research and the Commonwealth Government National Collaborative Research Infrastructure Strategy (NCRIS).","title":"NGS Read Mapping"},{"location":"alignment/handout/#ngs-read-mapping","text":"","title":"NGS Read Mapping"},{"location":"alignment/handout/#acknowledgements","text":"This module was developed by Bioplatforms Australia, in partnership with CSIRO, and with support from the European Bioinformatics Institute (EBI), a member of the European Molecular Biology Laboratory (EMBL) in the UK. The module content has been maintained and updated by a network of dedicated [bioinformatics trainers] ( http://www.bioplatforms.com/bioinformatics-training/ ) from around Australia. The Bioinformatics Training Platform was developed in collaboration with the Monash Bioinformatics Platform. The development of this module is also supported by the NSW Office of Health and Medical Research and the Commonwealth Government National Collaborative Research Infrastructure Strategy (NCRIS).","title":"Acknowledgements"},{"location":"alignment/handout/handout/","text":"Key Learning Outcomes After completing this practical the trainee should be able to: Perform the simple NGS data alignment task against reference data. Learn about the SAM/BAM formats for further manipulation. Be able to sort and index BAM format for visualisation purposes. Resources You\u2019ll be Using Tools Used BWA Burrows-Wheeler Algorithm: http://bio-bwa.sourceforge.net Samtools: http://picard.sourceforge.net/ Useful Links SAM Specification: http://samtools.sourceforge.net/SAM1.pdf Explain SAM Flags: https://broadinstitute.github.io/picard/explain-flags.html Sources of Data http://sra.dnanexus.com/studies/ERP001071 Author Information Primary Author(s): Sonika Tyagi sonika.tyagi@agrf.org.au Gayle Philip gkphilip@unimelb.edu.au Contributor(s): Introduction The goal of this hands-on session is to perform an NGS alignment on the sequencing data coming from a tumour and normal group of samples. We will align raw sequencing data to the human genome using the BWA aligner and then we will discuss the sequence alignment and mapping format (SAM). SAM to BAM conversion, indexing and sorting will also be demonstrated. These are important and essential steps for downstream processing of the aligned BAM files. This data is the whole genome sequencing of a lung adenocarcinoma patient AK55. It was downloaded from ERP001071 . Only the HiSeq2000 data for Blood and liverMets were analysed. Accession numbers associated with read data are assigned by the European Bioinformatics Institute (EBI) and start with \u2019ER\u2019. e.g. ERP is the study ID and ERR is the run ID. The original FASTQ files downloaded had the ERR number in front of each read name in the FASTQ file (e.g. @ERRxx HWI-ST478_xxxx). The read name had to be edited to remove the ERR number at the start of the name. This had caused problems for downstream programs such as Picard for marking optical duplicates. We have used 4 Blood samples (8 paired-end (PE) *.fastq.gz files) and 5 Liver samples (10 PE *.fastq.gz files) data from this study to perform the whole genome alignment using the BWA aligner. The whole process took >150K CPU seconds per sample and the precomputed alignment will be used in different sections of this workshop. Prepare the Environment By now you know about the raw sequence FASTQ format generated by the Illumina sequencers. Next we will see how FASTQ files are aligned to the reference genome and what the resulting standard alignment format is. In the interest of time, we have selected only 1 million paired reads from a Blood sample to demonstrate a BWA command. The remaining alignments have already been performed for you and will be required in the subsequent modules of the workshop. The input data for this section can be found in the alignment directory on your desktop. Please follow the commands below to go to the right folder and view the top 10 lines of the input FASTQ file: Open the Terminal. First, go to the right folder, where the data are stored. cd /home/trainee/alignment ls zless input/SM_Blood_ID_ERR059356.subset_R1.fastq.gz Press q to stop the zless command. Overview of the Process Figure 1: A flow diagram showing the steps that will be performed in this practical. Alignment You already know that there are a number of competing tools for short read alignment, each with its own set of strengths, weaknesses, and caveats. Here we will use BWA , a widely used aligner based on the Burrows-Wheeler Algorithm. The alignment involves two steps: 1) Indexing the genome. 2) Running the alignment command. BWA is a software package for mapping low-divergent sequences against a large reference genome, such as the human genome. It consists of three algorithms: BWA-backtrack, BWA-SW and BWA-MEM. The first algorithm is designed for Illumina sequence reads up to 100bp, while the other two are for longer sequences ranging from 70bp to 1Mbp. BWA-MEM and BWA-SW share similar features such as long-read support and split alignment, but BWA-MEM, which is the latest, is generally recommended for high-quality queries as it is faster and more accurate. BWA-MEM also has better performance than BWA-backtrack for 70-100bp Illumina reads. For more details see the BWA manual . BWA has a number of parameters in order to perform the alignment. To view them all, type bwa &lt;press enter&gt; BWA uses an indexed genome for the alignment in order to keep its memory footprint small. Indexing a genome is similar in concept to indexing a book. If you want to know on which page a certain word appears or a chapter begins, it is much more efficient/faster to look it up in a pre-built index than going through every page of the book until you find it. Indices allow the aligner to narrow down the potential origin of a query sequence within the genome, saving both time and memory. Due to time constraints, we will NOT be running the indexing command. It is run only once for a version of a genome, and the complete command to index the human genome version hg19 is given below. STOP You DO NOT need to run this command. This has already been run for you. bwa index -p bwaIndex/human_g1k_v37.fasta -a bwtsw human_g1k_v37.fasta We have used the following arguments for the indexing of the genome. -p : Prefix of the output database [same as db filename]. -a : Algorithm for constructing BWT index. This method works with the whole human genome. Ref genome filename : the last argument is the name of the reference genome file in the fasta format. This command will output 6 files that constitute the index. These files have the prefix human_g1k_v37.fasta and are stored in the bwaIndex subdirectory. To view the precomputed index files, type: ls -l bwaIndex Now that the genome is indexed we can move on to the actual alignment. Make a directory to store the output from your aligner. mkdir outputs The first argument for bwa is the basename of the index for the genome to be searched. In our case this is human_g1k_v37.fasta . Align the reads from the Blood sample using the following command: bwa mem -M -t 4 -R '@RG\\tSM:Blood\\tID:ERR059356.subset\\tLB:lb\\tPL:ILLUMINA' bwaIndex / human_g1k_v37 . fasta input / SM_Blood_ID_ERR059356 . subset_R1 . fastq . gz input / SM_Blood_ID_ERR059356 . subset_R2 . fastq . gz & gt ; outputs / SM_Blood_ID_ERR059356 . subset . sam The above command outputs the alignment in SAM format and stores them in the file SM_Blood_ID_ERR059356.subset.sam in the subdirectory outputs . We have used the following arguments for the alignment of the reads. mem : fast mode of high quality input such the Illumina -M : flags extra hits as secondary. This is needed for compatibility with other tools downstream. -t : Number of threads. -R : Complete read group header line. The SAM (Sequence Alignment/Map) format is currently the de facto standard for storing large nucleotide sequence alignments. It is a TAB-delimited text format consisting of a header section, which is optional, and an alignment section. If present, the header must be prior to the alignments. Header lines start with @ , while alignment lines do not. Each alignment line has 11 mandatory fields with essential alignment information such as mapping position. Navigate into your outputs directory and look at the top 10 lines of the SAM file by typing: cd outputs head -n 10 SM_Blood_ID_ERR059356.subset.sam Question Can you distinguish between the header of the SAM format and the actual alignments? Answer The header line starts with the letter \u2018@\u2019 i.e. @SQ SN:GL000192.1 LN:547496 @RG SM:Blood ID:ERR059356 LB:lb PL:ILLUMINA @PG ID:bwa PN:bwa VN:0.7.15-r1140 CL:bwa mem -M -t 4 -R @RG\\tSM:Blood\\tID:ERR059356.subset\\tLB:lb\\tPL:ILLUMINA bwaIndex/human_g1k_v37.fasta input/SM_Blood_ID_ERR059356.subset_R1.fastq.gz input/SM_Blood_ID_ERR059356.subset_R2.fastq.gz The actual alignments start with read ID i.e. HWI-ST478_0133:3:1101:1374:2056#0 147 11 HWI-ST478_0133:3:1101:1352:2070#0 163 14 Question What kind of information does the header provide? Answer @HD: Header line; VN: Format version; SO: the sort order of alignments. @SQ: Reference sequence information; SN: reference sequence name; LN: reference sequence length. @PG: Program; ID: Program record identifier; VN: Program version; CL: the command line that produces the alignment. Question To which chromosome are the reads mapped? Answer All chromosomes are represented (look at the 3 rd field). grep -v \u201c^@\u201d SM_Blood_ID_ERR059356.subset.sam | cut -f3 | sort | uniq Manipulating SAM output SAM files are rather big and when dealing with a high volume of NGS data, storage space can become an issue. As we have already seen, we can convert SAM to BAM files (their binary equivalent that are not human readable) that occupy much less space. Convert SAM to BAM using samtools view and store the output in the file SM_Blood_ID_ERR059356.subset.bam . You have to instruct samtools view that the input is in SAM format ( -S ), the output should be in BAM format ( -b ) and that you want the output to be stored in the file specified by the -o option: samtools view -bSo SM_Blood_ID_ERR059356.subset.bam SM_Blood_ID_ERR059356.subset.sam BAM files are not human-readable but can be viewed with the samtools view command. Advanced exercise Compute summary stats for the Flag values associated with the alignments using: samtools flagstat SM_Blood_ID_ERR059356.subset.bam Post Alignment Visualisation option IGV is a stand-alone genome browser that can be used to visualise the BAM outputs. Please check their website for all the formats that IGV can display. We will be using IGV later in the workshop for viewing a BAM file in the genome browser. It requires the index of the BAM file to be in the same folder as where the BAM file is. The index file should have the same name as the BAM file and the suffix .bai . Finally, to create the index of a BAM file you need to make sure that the file is sorted according to chromosomal coordinates. Sort alignments according to chromosomal position and store the result in the file with the prefix SM_Blood_ID_ERR059356.subset.sorted : samtools sort SM_Blood_ID_ERR059356.subset.bam SM_Blood_ID_ERR059356.subset.sorted Index the sorted file. samtools index SM_Blood_ID_ERR059356.subset.sorted.bam The indexing will create a file called SM_Blood_ID_ERR059356.subset.sorted.bam.bai . Note that you don\u2019t have to specify the name of the index file when running samtools index , it simply appends a .bai suffix to the input BAM file. Question How can you quickly find out whether a BAM file is already coordinate sorted or not? Answer Use samtools view -h command to look at the SAM header. It will have an SO field (e.g. SO:coordinate)","title":"Read Alignment"},{"location":"alignment/handout/handout/#key-learning-outcomes","text":"After completing this practical the trainee should be able to: Perform the simple NGS data alignment task against reference data. Learn about the SAM/BAM formats for further manipulation. Be able to sort and index BAM format for visualisation purposes.","title":"Key Learning Outcomes"},{"location":"alignment/handout/handout/#resources-youll-be-using","text":"","title":"Resources You\u2019ll be Using"},{"location":"alignment/handout/handout/#tools-used","text":"BWA Burrows-Wheeler Algorithm: http://bio-bwa.sourceforge.net Samtools: http://picard.sourceforge.net/","title":"Tools Used"},{"location":"alignment/handout/handout/#useful-links","text":"SAM Specification: http://samtools.sourceforge.net/SAM1.pdf Explain SAM Flags: https://broadinstitute.github.io/picard/explain-flags.html","title":"Useful Links"},{"location":"alignment/handout/handout/#sources-of-data","text":"http://sra.dnanexus.com/studies/ERP001071","title":"Sources of Data"},{"location":"alignment/handout/handout/#author-information","text":"Primary Author(s): Sonika Tyagi sonika.tyagi@agrf.org.au Gayle Philip gkphilip@unimelb.edu.au Contributor(s):","title":"Author Information"},{"location":"alignment/handout/handout/#introduction","text":"The goal of this hands-on session is to perform an NGS alignment on the sequencing data coming from a tumour and normal group of samples. We will align raw sequencing data to the human genome using the BWA aligner and then we will discuss the sequence alignment and mapping format (SAM). SAM to BAM conversion, indexing and sorting will also be demonstrated. These are important and essential steps for downstream processing of the aligned BAM files. This data is the whole genome sequencing of a lung adenocarcinoma patient AK55. It was downloaded from ERP001071 . Only the HiSeq2000 data for Blood and liverMets were analysed. Accession numbers associated with read data are assigned by the European Bioinformatics Institute (EBI) and start with \u2019ER\u2019. e.g. ERP is the study ID and ERR is the run ID. The original FASTQ files downloaded had the ERR number in front of each read name in the FASTQ file (e.g. @ERRxx HWI-ST478_xxxx). The read name had to be edited to remove the ERR number at the start of the name. This had caused problems for downstream programs such as Picard for marking optical duplicates. We have used 4 Blood samples (8 paired-end (PE) *.fastq.gz files) and 5 Liver samples (10 PE *.fastq.gz files) data from this study to perform the whole genome alignment using the BWA aligner. The whole process took >150K CPU seconds per sample and the precomputed alignment will be used in different sections of this workshop.","title":"Introduction"},{"location":"alignment/handout/handout/#prepare-the-environment","text":"By now you know about the raw sequence FASTQ format generated by the Illumina sequencers. Next we will see how FASTQ files are aligned to the reference genome and what the resulting standard alignment format is. In the interest of time, we have selected only 1 million paired reads from a Blood sample to demonstrate a BWA command. The remaining alignments have already been performed for you and will be required in the subsequent modules of the workshop. The input data for this section can be found in the alignment directory on your desktop. Please follow the commands below to go to the right folder and view the top 10 lines of the input FASTQ file: Open the Terminal. First, go to the right folder, where the data are stored. cd /home/trainee/alignment ls zless input/SM_Blood_ID_ERR059356.subset_R1.fastq.gz Press q to stop the zless command.","title":"Prepare the Environment"},{"location":"alignment/handout/handout/#overview-of-the-process","text":"Figure 1: A flow diagram showing the steps that will be performed in this practical.","title":"Overview of the Process"},{"location":"alignment/handout/handout/#alignment","text":"You already know that there are a number of competing tools for short read alignment, each with its own set of strengths, weaknesses, and caveats. Here we will use BWA , a widely used aligner based on the Burrows-Wheeler Algorithm. The alignment involves two steps: 1) Indexing the genome. 2) Running the alignment command. BWA is a software package for mapping low-divergent sequences against a large reference genome, such as the human genome. It consists of three algorithms: BWA-backtrack, BWA-SW and BWA-MEM. The first algorithm is designed for Illumina sequence reads up to 100bp, while the other two are for longer sequences ranging from 70bp to 1Mbp. BWA-MEM and BWA-SW share similar features such as long-read support and split alignment, but BWA-MEM, which is the latest, is generally recommended for high-quality queries as it is faster and more accurate. BWA-MEM also has better performance than BWA-backtrack for 70-100bp Illumina reads. For more details see the BWA manual . BWA has a number of parameters in order to perform the alignment. To view them all, type bwa &lt;press enter&gt; BWA uses an indexed genome for the alignment in order to keep its memory footprint small. Indexing a genome is similar in concept to indexing a book. If you want to know on which page a certain word appears or a chapter begins, it is much more efficient/faster to look it up in a pre-built index than going through every page of the book until you find it. Indices allow the aligner to narrow down the potential origin of a query sequence within the genome, saving both time and memory. Due to time constraints, we will NOT be running the indexing command. It is run only once for a version of a genome, and the complete command to index the human genome version hg19 is given below. STOP You DO NOT need to run this command. This has already been run for you. bwa index -p bwaIndex/human_g1k_v37.fasta -a bwtsw human_g1k_v37.fasta We have used the following arguments for the indexing of the genome. -p : Prefix of the output database [same as db filename]. -a : Algorithm for constructing BWT index. This method works with the whole human genome. Ref genome filename : the last argument is the name of the reference genome file in the fasta format. This command will output 6 files that constitute the index. These files have the prefix human_g1k_v37.fasta and are stored in the bwaIndex subdirectory. To view the precomputed index files, type: ls -l bwaIndex Now that the genome is indexed we can move on to the actual alignment. Make a directory to store the output from your aligner. mkdir outputs The first argument for bwa is the basename of the index for the genome to be searched. In our case this is human_g1k_v37.fasta . Align the reads from the Blood sample using the following command: bwa mem -M -t 4 -R '@RG\\tSM:Blood\\tID:ERR059356.subset\\tLB:lb\\tPL:ILLUMINA' bwaIndex / human_g1k_v37 . fasta input / SM_Blood_ID_ERR059356 . subset_R1 . fastq . gz input / SM_Blood_ID_ERR059356 . subset_R2 . fastq . gz & gt ; outputs / SM_Blood_ID_ERR059356 . subset . sam The above command outputs the alignment in SAM format and stores them in the file SM_Blood_ID_ERR059356.subset.sam in the subdirectory outputs . We have used the following arguments for the alignment of the reads. mem : fast mode of high quality input such the Illumina -M : flags extra hits as secondary. This is needed for compatibility with other tools downstream. -t : Number of threads. -R : Complete read group header line. The SAM (Sequence Alignment/Map) format is currently the de facto standard for storing large nucleotide sequence alignments. It is a TAB-delimited text format consisting of a header section, which is optional, and an alignment section. If present, the header must be prior to the alignments. Header lines start with @ , while alignment lines do not. Each alignment line has 11 mandatory fields with essential alignment information such as mapping position. Navigate into your outputs directory and look at the top 10 lines of the SAM file by typing: cd outputs head -n 10 SM_Blood_ID_ERR059356.subset.sam Question Can you distinguish between the header of the SAM format and the actual alignments? Answer The header line starts with the letter \u2018@\u2019 i.e. @SQ SN:GL000192.1 LN:547496 @RG SM:Blood ID:ERR059356 LB:lb PL:ILLUMINA @PG ID:bwa PN:bwa VN:0.7.15-r1140 CL:bwa mem -M -t 4 -R @RG\\tSM:Blood\\tID:ERR059356.subset\\tLB:lb\\tPL:ILLUMINA bwaIndex/human_g1k_v37.fasta input/SM_Blood_ID_ERR059356.subset_R1.fastq.gz input/SM_Blood_ID_ERR059356.subset_R2.fastq.gz The actual alignments start with read ID i.e. HWI-ST478_0133:3:1101:1374:2056#0 147 11 HWI-ST478_0133:3:1101:1352:2070#0 163 14 Question What kind of information does the header provide? Answer @HD: Header line; VN: Format version; SO: the sort order of alignments. @SQ: Reference sequence information; SN: reference sequence name; LN: reference sequence length. @PG: Program; ID: Program record identifier; VN: Program version; CL: the command line that produces the alignment. Question To which chromosome are the reads mapped? Answer All chromosomes are represented (look at the 3 rd field). grep -v \u201c^@\u201d SM_Blood_ID_ERR059356.subset.sam | cut -f3 | sort | uniq","title":"Alignment"},{"location":"alignment/handout/handout/#manipulating-sam-output","text":"SAM files are rather big and when dealing with a high volume of NGS data, storage space can become an issue. As we have already seen, we can convert SAM to BAM files (their binary equivalent that are not human readable) that occupy much less space. Convert SAM to BAM using samtools view and store the output in the file SM_Blood_ID_ERR059356.subset.bam . You have to instruct samtools view that the input is in SAM format ( -S ), the output should be in BAM format ( -b ) and that you want the output to be stored in the file specified by the -o option: samtools view -bSo SM_Blood_ID_ERR059356.subset.bam SM_Blood_ID_ERR059356.subset.sam BAM files are not human-readable but can be viewed with the samtools view command. Advanced exercise Compute summary stats for the Flag values associated with the alignments using: samtools flagstat SM_Blood_ID_ERR059356.subset.bam","title":"Manipulating SAM output"},{"location":"alignment/handout/handout/#post-alignment-visualisation-option","text":"IGV is a stand-alone genome browser that can be used to visualise the BAM outputs. Please check their website for all the formats that IGV can display. We will be using IGV later in the workshop for viewing a BAM file in the genome browser. It requires the index of the BAM file to be in the same folder as where the BAM file is. The index file should have the same name as the BAM file and the suffix .bai . Finally, to create the index of a BAM file you need to make sure that the file is sorted according to chromosomal coordinates. Sort alignments according to chromosomal position and store the result in the file with the prefix SM_Blood_ID_ERR059356.subset.sorted : samtools sort SM_Blood_ID_ERR059356.subset.bam SM_Blood_ID_ERR059356.subset.sorted Index the sorted file. samtools index SM_Blood_ID_ERR059356.subset.sorted.bam The indexing will create a file called SM_Blood_ID_ERR059356.subset.sorted.bam.bai . Note that you don\u2019t have to specify the name of the index file when running samtools index , it simply appends a .bai suffix to the input BAM file. Question How can you quickly find out whether a BAM file is already coordinate sorted or not? Answer Use samtools view -h command to look at the SAM header. It will have an SO field (e.g. SO:coordinate)","title":"Post Alignment Visualisation option"},{"location":"alignment/handout/license/","text":"This work is licensed under a Creative Commons Attribution 3.0 Unported License and the below text is a summary of the main terms of the full Legal Code (the full license) available at http://creativecommons.org/licenses/by/3.0/legalcode . You are free: to copy, distribute, display, and perform the work to make derivative works to make commercial use of the work Under the following conditions: Attribution - You must give the original author credit. With the understanding that: Waiver - Any of the above conditions can be waived if you get permission from the copyright holder. Public Domain - Where the work or any of its elements is in the public domain under applicable law, that status is in no way affected by the license. Other Rights - In no way are any of the following rights affected by the license: Your fair dealing or fair use rights, or other applicable copyright exceptions and limitations; The author\u2019s moral rights; Rights other persons may have either in the work itself or in how the work is used, such as publicity or privacy rights. Notice - For any reuse or distribution, you must make clear to others the license terms of this work.","title":"License"},{"location":"chip-seq/","text":"btp-module-chip-seq Bioinformatics Training Platform (BTP) Module: ChIP-Seq Topic ChIP-Seq Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites btp-module-ngs-mapping Key Learning Outcomes *Perform ChIP-Seq analysis, e.g. the detection of immuno-enriched areas using the chosen R package: ChIP-seq processing pipeline (SPP) Visualize the peak regions through a genome browser, e.g. IGV or Ensembl, and identify the real peak regions Perform functional annotation using biomaRt R package and detect potential binding sites (motif) in the predicted binding regions using motif discovery tool, e.g. Trawler or MEME. Time Required 3.5 hrs License The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"btp-module-chip-seq"},{"location":"chip-seq/#btp-module-chip-seq","text":"Bioinformatics Training Platform (BTP) Module: ChIP-Seq Topic ChIP-Seq Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites btp-module-ngs-mapping Key Learning Outcomes *Perform ChIP-Seq analysis, e.g. the detection of immuno-enriched areas using the chosen R package: ChIP-seq processing pipeline (SPP) Visualize the peak regions through a genome browser, e.g. IGV or Ensembl, and identify the real peak regions Perform functional annotation using biomaRt R package and detect potential binding sites (motif) in the predicted binding regions using motif discovery tool, e.g. Trawler or MEME. Time Required 3.5 hrs","title":"btp-module-chip-seq"},{"location":"chip-seq/#license","text":"The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"License"},{"location":"chip-seq/handout/chip-seq-draft-SL_ST/","text":"Key Learning Outcomes After completing this practical the trainee should be able to: Perform ChIP-Seq analysis, e.g. the detection of immuno-enriched areas using the chosen R package: ChIP-seq processing pipeline (SPP) Visualize the peak regions through a genome browser, e.g. IGV or Ensembl, and identify the real peak regions Perform functional annotation using biomaRt R package and detect potential binding sites (motif) in the predicted binding regions using motif discovery tool, e.g. Trawler or MEME. Resources You\u2019ll be Using Tools Used SPP : \\ http://compbio.med.harvard.edu/Supplements/ChIP-seq/ IGV : \\ http://software.broadinstitute.org/software/igv/ Ensembl : \\ http://www.ensembl.org Trawler : \\ https://trawler.erc.monash.edu.au/index.html MEME : \\ http://meme.ebi.edu.au/meme/cgi-bin/meme.cgi TOMTOM : \\ http://meme.ebi.edu/meme/cgi-bin/tomtom.cgi DAVID : \\ http://david.abcc.ncifcrf.gov GOstat : \\ http://gostat.wehi.edu.au Sources of Data http://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-11431 Introduction The goal of this hands-on session is to perform some basic tasks in the analysis of ChIP-seq data. In fact, you already performed the first step, alignment of the reads to the genome, in the previous session. We start from the aligned reads and we will find immuno-enriched areas using SPP. We will visualize the identified regions in a genome browser and perform functional annotation and motif analysis on the predicted binding regions. Prepare the Environment The material for this practical can be found in the ChIP-seq directory on your desktop. Please make sure that this directory also contains the SAM/BAM files you produced during the alignment practical. If you didn\u2019t have time to align the control file called gfp.fastq during the alignment practical, please do it now. Follow the same steps, from the bowtie alignment step, as for the Oct4.fastq file. In ChIP-seq analysis (unlike in other applications such as RNA-seq) it can be useful to exclude all reads that map to more than one location in the genome. When using Bowtie, this can be done using the -m 1 option, which tells it to report only unique matches (See bowtie \u2013help for more details). Open the Terminal and go to the ChIP-seq directory: cd /home/trainee/ChIP-seq ls R Finding enriched areas using SPP Terminology used in the tutorial: fragment: overlapping fragments obtaining in the IP (immuno precipitation) experiments. tag: sequenced part of the fragment which could be from one end (in case of single end sequencing ) or both ends in the paired end data. alignment: a process to determine the position of the tags, which typically should be around the binding site. peaks: spatial distribution of the tags densities around the binding sites on the genome. You would see two separate peaks of tags on the positive and negative strand around the binding site. The distance between the two peaks should reflect the size of the protected region. SPP is a Chip-seq processing pipeline implemented using R. The main functions of SPP include locating quality tag alignment by screening overall DNA-binding signals, removing or restricting certain positions with extremely high number of tags, estimating significant enrichment regions through genome-wide profiling, providing appropriate outputs for visualization, and determining statistically significant binding positions with saturation criteria assessment. Moreover, the processing of ChIP-seq data can require considerable amount of CPU time, it is often necessary to make use of parallel processing. SPP supports parallel processing if the cluster option is configured. Since our example data is relatively small, we will use single CPU and omit the cluster parameters for simplicity. The following steps will work you through the SPP pipeline. In your R terminal, load spp and biomaRt packages and make sure to set your working directory correctly: library(spp); library(biomaRt); setwd('/home/trainee/ChIP-seq'); 1. Loading tag data, selecting choosing alignment quality, removing anomalies \\ The first stage in SPP are 1) load input data; 2) choose alignment quality and 3) remove anomalies. SPP can read output from the following aligners and file formats: ELAND, MAQ, bowtie, Arachne, tagAlign format and BAM format (Note: because BAM standard doesn\u2019t specify a flag for a uniquely-mapped read, the aligner has to generate a BAM file that would contain only unique reads.) STEP1 Loading data and quality filter the informative tags First load Oct4 and gfp bam files. Here GFP are the control or input samples, these are usually mock IP DNA where you do not expect to see any binding peaks. oct4.data&lt;-read.bam.tags(\"Oct4.sorted.bam\"); gfp.data&lt;-read.bam.tags(\"gfp.sorted.bam\"); The statistical significance of tags clustering observed for a putative protein binding site depends on the expected background. Therefore, use of a input or control DNA is highly recommended in the experiment design. This provides an experimental assessment of the background tag distribution. The next step uses cross-correlation profile to calculate binding peak separation distance, and assess whether inclusion of tags with non-perfect alignment quality improves the cross-correlation peak. This is done by shifting the strands relative to each other by increasing distance within a given range. cross-correlation of the positive and negative strand tag densities is plotted. The cross-correlation plot should show the predominant size of the protected region. binding.characteristics &lt;- get.binding.characteristics(oct4.data,srange=c(50,500),bin=5); The binding.characteristics provides the estimate of the binding peak separation distance, cross-correlation profile itself and tag quality bin acceptance information. The srange parameter defines the possible range for the size of the protected region. It is supposed to be higher than tag length. However, the upper boundary (500) cannot be too high, which will increase the running time. The bin parameter tags within the specified number of base pairs to speed up calculation. The increase of bin size will decrease the accuracy of the determined parameters. Then, print out binding peak separation distance and we can plot cross-correlation profile: print(paste(\"binding peak separation distance=\",binding.characteristics$peak$x)); pdf(file=\"oct4.crosscorrelation.pdf\",width=5,height=5); par(mar = c(3.5,3.5,1.0,0.5), mgp = c(2,0.65,0), cex = 0.8); plot(binding.characteristics$cross.correlation,type='l',xlab=\"strand shift\",ylab=\"cross-correlation\"); abline(v=binding.characteristics$peak$x,lty=2,col=2); dev.off(); A set of tags informative about the binding positions should increase cross correlation magnitude whereas a randonmly mapped set of tags should decrease it. The following calls will select tags with acceptable alignment quality based on the binding characteristics: chip.data &lt;- select.informative.tags(oct4.data,binding.characteristics); gfpcontrol.data &lt;- select.informative.tags(gfp.data,binding.characteristics); The last step below will scan along the chromosomes calculating local density of region (can be specified using window.size parameter, default is 200bp), removing or restricting singular positions with extremely high tag count relative to the neighborhood: chip.data &lt;- remove.local.tag.anomalies(chip.data); gfpcontrol.data &lt;- remove.local.tag.anomalies(gfpcontrol.data); STEP2 Calculating genome-wide tag density and tag enrichment/depletion profiles The following commands will calculate smoothed tag density and output it into a WIG file that can be read with genome browsers, such as IGV (Note: the tags are shifted by half of the peak separation distance): tag.shift &lt;- round(binding.characteristics$peak$x/2) smoothed.density &lt;- get.smoothed.tag.density (chip.data,control.tags=gfpcontrol.data,bandwidth=200,step=100,tag.shift=tag.shift); writewig(smoothed.density,\"oct4.density.wig\",\"Smoothed, background-subtracted tag density\"); rm(smoothed.density); To provide a rough estimate of the enrichment profile (i.e. ChIP signal over input), we can use the get.smoothed.enrichment.mle() method: smoothed.enrichment.estimate &lt;- get.smoothed.enrichment.mle (chip.data,gfpcontrol.data,bandwidth=200,step=100,tag.shift=tag.shift); writewig(smoothed.enrichment.estimate,\"oct4.enrichment.wig\",\"Smoothed maximum likelihood log2 enrichment estimate\"); Next, we will scan ChIP and signal tag density to estimate lower bounds of tag enrichment (and upper bound of tag depletion if it is significant) along the genome. The resulting profile gives conservative statistical estimates of log2 fold-enrichment ratios along the genome. The example below uses a window of 500bp (and background windows of 1, 5, 25 and 50 times that size) and a confidence interval corresponding to 1%. enrichment.estimates &lt;- get.conservative.fold.enrichment.profile(chip.data,gfpcontrol.data,fws=500,step=100,alpha=0.01); writewig(enrichment.estimates,\"oct4.Enrichment.estimates.wig\",\"Conservative fold-enrichment/depletion estimates shown on log2 scale\"); rm(enrichment.estimates); Also, broad regions of enrichment for a specified scale can be quickly identified and output in broadPeak format using the following commands: broad.clusters &lt;- get.broad.enrichment.clusters(chip.data,gfpcontrol.data,window.size=1e3,z.thr=3,tag.shift=round(binding.characteristics$peak$x/2)); write.broadpeak.info(broad.clusters,\"oct4.broadPeak\"); write out in bed format write.table(cbind(rep(\"1\", length(broad.clusters$chr1$s)), broad.clusters$chr1$s, broad.clusters$chr1$e), file = paste0(\"oct4\",\"_enrich_broad_chr1.bed\"),quote = FALSE, row.names = FALSE, col.names = FALSE, sep = \"\\t\"); The tasks below will use window tag density (WTD) method to call binding positions, using FDR of 1% and a window size estimated by the binding.characteristics. We set the binding detection parameters: FDR (1%) (Note: we can use an E-value to the method calls below instead of the fdr), the binding.characteristics contains the optimized half-size for binding detection window: fdr &lt;- 1e-2; detection.window.halfsize &lt;- binding.characteristics$whs; Identify binding positions using WTD method and write narrow peaks in BED format: bp &lt;- find.binding.positions(signal.data=chip.data,control.data=gfpcontrol.data,fdr=fdr,whs=detection.window.halfsize); print(paste(\"detected\",sum(unlist(lapply(bp$npl,function(d) length(d$x)))),\"peaks\")); bp.short &lt;- add.broad.peak.regions(chip.data,gfpcontrol.data,bp,window.size=500,z.thr=3); //set the window size to 500. write.table(na.omit(data.frame(cbind(rep(\"1\", length(bp.short$npl$chr1$rs)), bp.short$npl$chr1$rs, bp.short$npl$chr1$re))), file = paste0(\"oct4\",\"_enrich_narrow_chr1.bed\"),quote = FALSE, row.names = FALSE, col.names = FALSE, sep = \"\\t\"); STEP3 Comparing Binding Sites to Annotations Using the biomaRt package In order to biologically interpret the results of ChIP-seq experiments, it is usually recommended to look at the genes and other annotated elements that are located in proximity to the identified enriched regions. This can be easily done using the R biomaRt package, which serves as an interface to perform comprehensive data analysis from gene annotation to data mining through wealth number of biological databases integrated by the BioMart software suite ( http://www.biomart.org ). It provides fast access to large amount of data without touching the underlying database or using complex database queries. These major databases including Ensembl, COSMIC, HGNC, Gramene, Wormbase and dbSNP mapped to Emsembl. you should make sure that ensembl has the same version of reference as you used in bowtie aligner. We will download the ENSEMBLE mouse genome annotations and generate a list of ENSEMBLE gene information on chromosome 1 including start position, end position, strand and description ensembl = useMart(host=\"asia.ensembl.org\", \"ENSEMBL_MART_ENSEMBL\", dataset = \"mmusculus_gene_ensembl\"); genes.chr1 = getBM(attributes = c(\"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"description\"), filters = \"chromosome_name\", values= \"1\", mart = ensembl); Next, we\u2019re going to take our binding sites from the bp list and use it to determine the set of genes that contain significantly enriched Pol II within 2kb of their TSS. In order to compare PolII sites to TSS sites, we need to write an overlap function where bs represents a binding site position, ts is the annotated TSS and l is the allowed distance of the binding site from the TSS. overlap = function(bs, ts, l) { if ((bs &gt; ts - l) &amp;&amp; (bs &lt; ts + l)) { TRUE; } else { FALSE; } } Now we\u2019ll write a function that takes a vector of binding site values, start positions, end positions and strands of the genes on chromosome X as well as our distance cutoff. l and outputs a logical vector of the genes that contain a Pol II site within l bp (i.e., TRUE value) or do not contain a Pol II site (i.e., FALSE value). fivePrimeGenes = function(bs, ts, te, s, l) { fivePrimeVec = logical(); for (i in 1:length(ts)) { fivePrime = FALSE; for (j in 1:length(bs)) { if (s[i] == 1) { fivePrime = fivePrime || overlap(bs[j], ts[i], l); } else { fivePrime = fivePrime || overlap(bs[j], te[i], l); } } fivePrimeVec = c(fivePrimeVec, fivePrime); } fivePrimeVec; } Using the fivePrimeGenes function, generate a vector of the TSSs and genes that contain Pol II within .2kb of their TSS (i.e., l = 2000). fivePrimeGenesLogical = fivePrimeGenes(bp$npl$chr1$x, genes.chr1$start_position, genes.chr1$end_position, genes.chr1$strand, 2000); Find the gene located on the plus strand fivePrimeStartsPlus = genes.chr1$start_position[fivePrimeGenesLogical &amp; genes.chr1$strand == 1]; Find the gene located on the minus strand fivePrimeStartsMinus = genes.chr1$end_position[fivePrimeGenesLogical &amp; genes.chr1$strand == -1]; Combine the start positions together fivePrimeStarts = sort(c(fivePrimeStartsPlus, fivePrimeStartsMinus)) Get all the gene names fivePrimeGenes = genes.chr1$description[fivePrimeGenesLogical] Viewing results with the Genome browser It is often instructive to look at your data in a genome browser, which will allow you to get a \u2018feel\u2019 for the data, as well as detecting abnormalities and problems. Also, exploring the data in such a way may give you ideas for further analyses. Well known web-based genome browsers, like Ensembl or the UCSC browser do not only allow for more polished and flexible visualization, but also provide access to a wealth of annotations and external data sources. This makes it straightforward to relate your data with information about repeat regions, known genes, epigenetic features or areas of cross-species conservation, to name just a few. As such, they are useful tools for exploratory analysis, even though could be relatively slow. In this section, we will guide you though using IGV, a stand-alone browser, which has the advantage of being installed locally, easy to use and fast access to visualize your in-house data. We alo provide the workflow of how to use Ensembl for visualization. You can practise after the workshop. IGV Visualization Double click the IGV 2.3 icon on your Desktop. Ignore any warnings and when it opens you have to load the genome of interest. On the top left of your screen choose from the drop down menu Mouse (mm10). If it doesn\u2019t appear in list, click More .., type mm10 in the Filter section, choose the mouse genome and press OK. We have generated bigWig files in advance for you. Instead of choosing the \u2019Load from File\u2019 option, we are going to use \u2019Load from URL\u2019 to upload to IGV. The first file is at the following URL: http://www.ebi.ac.uk/~remco/ChIP-Seq_course/Oct4.bw To visualise the data: Select chr1 in the chromosome drop-down box next to the \u2019Mouse mm10\u2019 box. Click File then choose Load from URL\u2026 Paste the location above in the field File URL . Click OK and close the window to return to the genome browser. You should see Oct4.bw has been loaded in the track region below the genome region. Move the mouse to track region over Oct4.bw. Right click the mouse, Change the track colour on your own perference. Right click again, in the Windowing Function , choose Maxmum and set to Autoscale . Repeat the process for the gfp control sample, located at: http://www.ebi.ac.uk/~remco/ChIP-Seq_course/gfp.bw . Go to a region on chromosome 1 (e.g. 1 : 34823162 - 35323161 ), and zoom in and out to view the signal and peak regions. Be aware that the y-axis of each track is auto-scaled independently of each other, so bigger-looking peaks may not actually be bigger! Always look at the values on the left hand side axis. What can you say about the profile of Oct4 peaks in this region? There are no significant Oct4 peaks over the selected region. Compare it with H3K4me3 histone modification wig file we have generated at http://www.ebi.ac.uk/~remco/ChIP-Seq_course/H3K4me3.bw . H3K4me3 has a region that contains relatively high peaks than Oct4. Jump to 1 : 36066594 - 36079728 for a sample peak. Do you think H3K4me3 peaks regions contain one or more modification sites? What about Oct4? Yes. There are roughly three peaks, which indicate the possibility of having more than one modification sites in this region. For Oct4, no peak can be observed. Advanced Session Ensembl Visualization Launch a web browser and go to the Ensembl website at http://www.ensembl.org/index.html . Choose the genome of interest (in this case, mouse) on the left side of the page, browse to any location in the genome or click one of the demo links provided on the web page. Click on the Add your data link on the left, then choose Attach remote file . Wig files are large so are inconvenient for uploading directly to the Ensemble Genome browser. Instead, we will convert it to an indexed binary format and put this into a web accessible place such as on a HTTP, HTTPS, or FTP server. This makes all the browsing process much faster. Detailed instructions for generating a bigWig from a wig type file can be found at: http://genome.ucsc.edu/goldenPath/help/bigWig.html . We have generated bigWig files in advance for you to upload to the Ensembl browser. They are at the following URL: http://www.ebi.ac.uk/~remco/ChIP-Seq_course/Oct4.bw To visualise the data: Paste the location above in the field File URL. Choose data format bigWig. Choose some informative name and in the next window choose the colour of your preference. Click Save and close the window to return to the genome browser. Repeat the process for the gfp control sample, located at: http://www.ebi.ac.uk/~remco/ChIP-Seq_course/gfp.bw . If can not see your tracks: Click on \u2019Configure this page\u2019 in left panel. In \u2019Configure region Overview\u2019 tab click on \u2019Ypur data\u2019 in left panel. Check the boxes in \u2019Enable/Disable all tracks\u2019 for you *.bw files by selecting \u2019wiggle plot in the pop up menu. After uploading, choose Configure this page , and under Your data tick both boxes. Closing the window will save these changes. Go to a region on chromosome 1 (e.g. 1 : 34823162 - 35323161 ), and zoom in and out to view the signal and peak regions. Be aware that the y-axis of each track is auto-scaled independently of each other, so bigger-looking peaks may not actually be bigger! Always look at the values on the left hand side axis. MACS generates its peak files in a file format called bed file. This is a simple text format containing genomic locations, specified by chromosome, begin and end positions, and some more optional information. See http://genome.ucsc.edu/FAQ/FAQformat.html#format1 for details. Bed files can also be uploaded to the Ensembl browser. Motif analysis It is often interesting to find out whether we can identify transcription factor binding sites (TFBSs) from the input DNA sequences. TFBSs which share a similar sequence pattern (motif) are presumed to have biological functions. Here we introduce three motif discovery tools named Trawler, RSAT peak-motifs, and MEME-ChIP, which use different searching algorithms. This might lead to varying results. Hence, it is generally a good idea to use several tools to identify TFBSs. The more tools confirm the same result, the better, which is also called an orthogonal approach. Eventually, you probably want to validate your in silico findings in vivo or in vitro. Motif discovery with Trawler Trawler is a fast, yet accurate motif discovery tool that accepts both, BED and FASTA files as input file formats. BED files are generated when you process and analyse your NGS data. Thus, it is handy to use them directly in Trawler. Other tools do not accept BED files as input. With Trawler, BED files can be converted into FASTA files that can then be used for other motif discovery tools (e.g. RSAT peak-motifs and MEME ChIP). Running Trawler Go to the website https://trawler.erc.monash.edu.au Run Trawler with BED file as input, and wait for the results Download both sample and background files in FASTA format. Right click and choose: \u2019Save the link as\u2026\u2019 Which motif was found to be the most similar to your motif? Sox2 Optional: Motif discovery with RSAT peak-motif The motif discovery tool RSAT peak-motifs uses FASTA files as input. An optional background can be uploaded in FASTA format. RSAT peak-motifs automatically outputs motifs of 6 and 7 nucleotides length (two separate files). While still accurate, the running time is longer compared to Trawler (up to 20 minutes depending on the size of the files). Running RSAT peak-motifs Go to the website http://floresta.eead.csic.es/rsat/peak-motifs~f~orm.cgi Upload input and background FASTA files just downloaded from Trawler Wait until the discovery finishes. Optional: Motif discovery with MEME-ChIP MEME-ChIP is a popular motif discovery tool and part of MEME Suite. MEME-ChIP accepts input files in FASTA format. It is not necessary to upload your own background because MEME-ChIP uses its own. Although MEME-ChIP is one of the most popular motif discovery tools, the identified motifs are not very accurate and the motif search might take up to one hour. MEME-ChIP outputs the three motifs with the lowest E-Value. Running MEME-ChIP Go to the website http://meme-suite.org/tools/meme-chip Upload input and background FASTA files just downloaded from Trawler Wait until the discovery finishes. Reference Chen, X et al.: Integration of external signaling pathways with the core transcriptional network in embryonic stem cells. Cell 133:6, 1106-17 (2008).","title":"chip seq draft SL ST"},{"location":"chip-seq/handout/chip-seq-draft-SL_ST/#key-learning-outcomes","text":"After completing this practical the trainee should be able to: Perform ChIP-Seq analysis, e.g. the detection of immuno-enriched areas using the chosen R package: ChIP-seq processing pipeline (SPP) Visualize the peak regions through a genome browser, e.g. IGV or Ensembl, and identify the real peak regions Perform functional annotation using biomaRt R package and detect potential binding sites (motif) in the predicted binding regions using motif discovery tool, e.g. Trawler or MEME.","title":"Key Learning Outcomes"},{"location":"chip-seq/handout/chip-seq-draft-SL_ST/#resources-youll-be-using","text":"","title":"Resources You\u2019ll be Using"},{"location":"chip-seq/handout/chip-seq-draft-SL_ST/#tools-used","text":"SPP : \\ http://compbio.med.harvard.edu/Supplements/ChIP-seq/ IGV : \\ http://software.broadinstitute.org/software/igv/ Ensembl : \\ http://www.ensembl.org Trawler : \\ https://trawler.erc.monash.edu.au/index.html MEME : \\ http://meme.ebi.edu.au/meme/cgi-bin/meme.cgi TOMTOM : \\ http://meme.ebi.edu/meme/cgi-bin/tomtom.cgi DAVID : \\ http://david.abcc.ncifcrf.gov GOstat : \\ http://gostat.wehi.edu.au","title":"Tools Used"},{"location":"chip-seq/handout/chip-seq-draft-SL_ST/#sources-of-data","text":"http://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-11431","title":"Sources of Data"},{"location":"chip-seq/handout/chip-seq-draft-SL_ST/#introduction","text":"The goal of this hands-on session is to perform some basic tasks in the analysis of ChIP-seq data. In fact, you already performed the first step, alignment of the reads to the genome, in the previous session. We start from the aligned reads and we will find immuno-enriched areas using SPP. We will visualize the identified regions in a genome browser and perform functional annotation and motif analysis on the predicted binding regions.","title":"Introduction"},{"location":"chip-seq/handout/chip-seq-draft-SL_ST/#prepare-the-environment","text":"The material for this practical can be found in the ChIP-seq directory on your desktop. Please make sure that this directory also contains the SAM/BAM files you produced during the alignment practical. If you didn\u2019t have time to align the control file called gfp.fastq during the alignment practical, please do it now. Follow the same steps, from the bowtie alignment step, as for the Oct4.fastq file. In ChIP-seq analysis (unlike in other applications such as RNA-seq) it can be useful to exclude all reads that map to more than one location in the genome. When using Bowtie, this can be done using the -m 1 option, which tells it to report only unique matches (See bowtie \u2013help for more details). Open the Terminal and go to the ChIP-seq directory: cd /home/trainee/ChIP-seq ls R","title":"Prepare the Environment"},{"location":"chip-seq/handout/chip-seq-draft-SL_ST/#finding-enriched-areas-using-spp","text":"Terminology used in the tutorial: fragment: overlapping fragments obtaining in the IP (immuno precipitation) experiments. tag: sequenced part of the fragment which could be from one end (in case of single end sequencing ) or both ends in the paired end data. alignment: a process to determine the position of the tags, which typically should be around the binding site. peaks: spatial distribution of the tags densities around the binding sites on the genome. You would see two separate peaks of tags on the positive and negative strand around the binding site. The distance between the two peaks should reflect the size of the protected region. SPP is a Chip-seq processing pipeline implemented using R. The main functions of SPP include locating quality tag alignment by screening overall DNA-binding signals, removing or restricting certain positions with extremely high number of tags, estimating significant enrichment regions through genome-wide profiling, providing appropriate outputs for visualization, and determining statistically significant binding positions with saturation criteria assessment. Moreover, the processing of ChIP-seq data can require considerable amount of CPU time, it is often necessary to make use of parallel processing. SPP supports parallel processing if the cluster option is configured. Since our example data is relatively small, we will use single CPU and omit the cluster parameters for simplicity. The following steps will work you through the SPP pipeline. In your R terminal, load spp and biomaRt packages and make sure to set your working directory correctly: library(spp); library(biomaRt); setwd('/home/trainee/ChIP-seq'); 1. Loading tag data, selecting choosing alignment quality, removing anomalies \\ The first stage in SPP are 1) load input data; 2) choose alignment quality and 3) remove anomalies. SPP can read output from the following aligners and file formats: ELAND, MAQ, bowtie, Arachne, tagAlign format and BAM format (Note: because BAM standard doesn\u2019t specify a flag for a uniquely-mapped read, the aligner has to generate a BAM file that would contain only unique reads.)","title":"Finding enriched areas using SPP"},{"location":"chip-seq/handout/chip-seq-draft-SL_ST/#step1-loading-data-and-quality-filter-the-informative-tags","text":"First load Oct4 and gfp bam files. Here GFP are the control or input samples, these are usually mock IP DNA where you do not expect to see any binding peaks. oct4.data&lt;-read.bam.tags(\"Oct4.sorted.bam\"); gfp.data&lt;-read.bam.tags(\"gfp.sorted.bam\"); The statistical significance of tags clustering observed for a putative protein binding site depends on the expected background. Therefore, use of a input or control DNA is highly recommended in the experiment design. This provides an experimental assessment of the background tag distribution. The next step uses cross-correlation profile to calculate binding peak separation distance, and assess whether inclusion of tags with non-perfect alignment quality improves the cross-correlation peak. This is done by shifting the strands relative to each other by increasing distance within a given range. cross-correlation of the positive and negative strand tag densities is plotted. The cross-correlation plot should show the predominant size of the protected region. binding.characteristics &lt;- get.binding.characteristics(oct4.data,srange=c(50,500),bin=5); The binding.characteristics provides the estimate of the binding peak separation distance, cross-correlation profile itself and tag quality bin acceptance information. The srange parameter defines the possible range for the size of the protected region. It is supposed to be higher than tag length. However, the upper boundary (500) cannot be too high, which will increase the running time. The bin parameter tags within the specified number of base pairs to speed up calculation. The increase of bin size will decrease the accuracy of the determined parameters. Then, print out binding peak separation distance and we can plot cross-correlation profile: print(paste(\"binding peak separation distance=\",binding.characteristics$peak$x)); pdf(file=\"oct4.crosscorrelation.pdf\",width=5,height=5); par(mar = c(3.5,3.5,1.0,0.5), mgp = c(2,0.65,0), cex = 0.8); plot(binding.characteristics$cross.correlation,type='l',xlab=\"strand shift\",ylab=\"cross-correlation\"); abline(v=binding.characteristics$peak$x,lty=2,col=2); dev.off(); A set of tags informative about the binding positions should increase cross correlation magnitude whereas a randonmly mapped set of tags should decrease it. The following calls will select tags with acceptable alignment quality based on the binding characteristics: chip.data &lt;- select.informative.tags(oct4.data,binding.characteristics); gfpcontrol.data &lt;- select.informative.tags(gfp.data,binding.characteristics); The last step below will scan along the chromosomes calculating local density of region (can be specified using window.size parameter, default is 200bp), removing or restricting singular positions with extremely high tag count relative to the neighborhood: chip.data &lt;- remove.local.tag.anomalies(chip.data); gfpcontrol.data &lt;- remove.local.tag.anomalies(gfpcontrol.data);","title":"STEP1 Loading data and quality filter the informative tags"},{"location":"chip-seq/handout/chip-seq-draft-SL_ST/#step2-calculating-genome-wide-tag-density-and-tag-enrichmentdepletion-profiles","text":"The following commands will calculate smoothed tag density and output it into a WIG file that can be read with genome browsers, such as IGV (Note: the tags are shifted by half of the peak separation distance): tag.shift &lt;- round(binding.characteristics$peak$x/2) smoothed.density &lt;- get.smoothed.tag.density (chip.data,control.tags=gfpcontrol.data,bandwidth=200,step=100,tag.shift=tag.shift); writewig(smoothed.density,\"oct4.density.wig\",\"Smoothed, background-subtracted tag density\"); rm(smoothed.density); To provide a rough estimate of the enrichment profile (i.e. ChIP signal over input), we can use the get.smoothed.enrichment.mle() method: smoothed.enrichment.estimate &lt;- get.smoothed.enrichment.mle (chip.data,gfpcontrol.data,bandwidth=200,step=100,tag.shift=tag.shift); writewig(smoothed.enrichment.estimate,\"oct4.enrichment.wig\",\"Smoothed maximum likelihood log2 enrichment estimate\"); Next, we will scan ChIP and signal tag density to estimate lower bounds of tag enrichment (and upper bound of tag depletion if it is significant) along the genome. The resulting profile gives conservative statistical estimates of log2 fold-enrichment ratios along the genome. The example below uses a window of 500bp (and background windows of 1, 5, 25 and 50 times that size) and a confidence interval corresponding to 1%. enrichment.estimates &lt;- get.conservative.fold.enrichment.profile(chip.data,gfpcontrol.data,fws=500,step=100,alpha=0.01); writewig(enrichment.estimates,\"oct4.Enrichment.estimates.wig\",\"Conservative fold-enrichment/depletion estimates shown on log2 scale\"); rm(enrichment.estimates); Also, broad regions of enrichment for a specified scale can be quickly identified and output in broadPeak format using the following commands: broad.clusters &lt;- get.broad.enrichment.clusters(chip.data,gfpcontrol.data,window.size=1e3,z.thr=3,tag.shift=round(binding.characteristics$peak$x/2)); write.broadpeak.info(broad.clusters,\"oct4.broadPeak\"); write out in bed format write.table(cbind(rep(\"1\", length(broad.clusters$chr1$s)), broad.clusters$chr1$s, broad.clusters$chr1$e), file = paste0(\"oct4\",\"_enrich_broad_chr1.bed\"),quote = FALSE, row.names = FALSE, col.names = FALSE, sep = \"\\t\"); The tasks below will use window tag density (WTD) method to call binding positions, using FDR of 1% and a window size estimated by the binding.characteristics. We set the binding detection parameters: FDR (1%) (Note: we can use an E-value to the method calls below instead of the fdr), the binding.characteristics contains the optimized half-size for binding detection window: fdr &lt;- 1e-2; detection.window.halfsize &lt;- binding.characteristics$whs; Identify binding positions using WTD method and write narrow peaks in BED format: bp &lt;- find.binding.positions(signal.data=chip.data,control.data=gfpcontrol.data,fdr=fdr,whs=detection.window.halfsize); print(paste(\"detected\",sum(unlist(lapply(bp$npl,function(d) length(d$x)))),\"peaks\")); bp.short &lt;- add.broad.peak.regions(chip.data,gfpcontrol.data,bp,window.size=500,z.thr=3); //set the window size to 500. write.table(na.omit(data.frame(cbind(rep(\"1\", length(bp.short$npl$chr1$rs)), bp.short$npl$chr1$rs, bp.short$npl$chr1$re))), file = paste0(\"oct4\",\"_enrich_narrow_chr1.bed\"),quote = FALSE, row.names = FALSE, col.names = FALSE, sep = \"\\t\");","title":"STEP2 Calculating genome-wide tag density and tag enrichment/depletion profiles"},{"location":"chip-seq/handout/chip-seq-draft-SL_ST/#step3-comparing-binding-sites-to-annotations-using-the-biomart-package","text":"In order to biologically interpret the results of ChIP-seq experiments, it is usually recommended to look at the genes and other annotated elements that are located in proximity to the identified enriched regions. This can be easily done using the R biomaRt package, which serves as an interface to perform comprehensive data analysis from gene annotation to data mining through wealth number of biological databases integrated by the BioMart software suite ( http://www.biomart.org ). It provides fast access to large amount of data without touching the underlying database or using complex database queries. These major databases including Ensembl, COSMIC, HGNC, Gramene, Wormbase and dbSNP mapped to Emsembl. you should make sure that ensembl has the same version of reference as you used in bowtie aligner. We will download the ENSEMBLE mouse genome annotations and generate a list of ENSEMBLE gene information on chromosome 1 including start position, end position, strand and description ensembl = useMart(host=\"asia.ensembl.org\", \"ENSEMBL_MART_ENSEMBL\", dataset = \"mmusculus_gene_ensembl\"); genes.chr1 = getBM(attributes = c(\"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"description\"), filters = \"chromosome_name\", values= \"1\", mart = ensembl); Next, we\u2019re going to take our binding sites from the bp list and use it to determine the set of genes that contain significantly enriched Pol II within 2kb of their TSS. In order to compare PolII sites to TSS sites, we need to write an overlap function where bs represents a binding site position, ts is the annotated TSS and l is the allowed distance of the binding site from the TSS. overlap = function(bs, ts, l) { if ((bs &gt; ts - l) &amp;&amp; (bs &lt; ts + l)) { TRUE; } else { FALSE; } } Now we\u2019ll write a function that takes a vector of binding site values, start positions, end positions and strands of the genes on chromosome X as well as our distance cutoff. l and outputs a logical vector of the genes that contain a Pol II site within l bp (i.e., TRUE value) or do not contain a Pol II site (i.e., FALSE value). fivePrimeGenes = function(bs, ts, te, s, l) { fivePrimeVec = logical(); for (i in 1:length(ts)) { fivePrime = FALSE; for (j in 1:length(bs)) { if (s[i] == 1) { fivePrime = fivePrime || overlap(bs[j], ts[i], l); } else { fivePrime = fivePrime || overlap(bs[j], te[i], l); } } fivePrimeVec = c(fivePrimeVec, fivePrime); } fivePrimeVec; } Using the fivePrimeGenes function, generate a vector of the TSSs and genes that contain Pol II within .2kb of their TSS (i.e., l = 2000). fivePrimeGenesLogical = fivePrimeGenes(bp$npl$chr1$x, genes.chr1$start_position, genes.chr1$end_position, genes.chr1$strand, 2000); Find the gene located on the plus strand fivePrimeStartsPlus = genes.chr1$start_position[fivePrimeGenesLogical &amp; genes.chr1$strand == 1]; Find the gene located on the minus strand fivePrimeStartsMinus = genes.chr1$end_position[fivePrimeGenesLogical &amp; genes.chr1$strand == -1]; Combine the start positions together fivePrimeStarts = sort(c(fivePrimeStartsPlus, fivePrimeStartsMinus)) Get all the gene names fivePrimeGenes = genes.chr1$description[fivePrimeGenesLogical]","title":"STEP3 Comparing Binding Sites to Annotations Using the biomaRt package"},{"location":"chip-seq/handout/chip-seq-draft-SL_ST/#viewing-results-with-the-genome-browser","text":"It is often instructive to look at your data in a genome browser, which will allow you to get a \u2018feel\u2019 for the data, as well as detecting abnormalities and problems. Also, exploring the data in such a way may give you ideas for further analyses. Well known web-based genome browsers, like Ensembl or the UCSC browser do not only allow for more polished and flexible visualization, but also provide access to a wealth of annotations and external data sources. This makes it straightforward to relate your data with information about repeat regions, known genes, epigenetic features or areas of cross-species conservation, to name just a few. As such, they are useful tools for exploratory analysis, even though could be relatively slow. In this section, we will guide you though using IGV, a stand-alone browser, which has the advantage of being installed locally, easy to use and fast access to visualize your in-house data. We alo provide the workflow of how to use Ensembl for visualization. You can practise after the workshop. IGV Visualization Double click the IGV 2.3 icon on your Desktop. Ignore any warnings and when it opens you have to load the genome of interest. On the top left of your screen choose from the drop down menu Mouse (mm10). If it doesn\u2019t appear in list, click More .., type mm10 in the Filter section, choose the mouse genome and press OK. We have generated bigWig files in advance for you. Instead of choosing the \u2019Load from File\u2019 option, we are going to use \u2019Load from URL\u2019 to upload to IGV. The first file is at the following URL: http://www.ebi.ac.uk/~remco/ChIP-Seq_course/Oct4.bw To visualise the data: Select chr1 in the chromosome drop-down box next to the \u2019Mouse mm10\u2019 box. Click File then choose Load from URL\u2026 Paste the location above in the field File URL . Click OK and close the window to return to the genome browser. You should see Oct4.bw has been loaded in the track region below the genome region. Move the mouse to track region over Oct4.bw. Right click the mouse, Change the track colour on your own perference. Right click again, in the Windowing Function , choose Maxmum and set to Autoscale . Repeat the process for the gfp control sample, located at: http://www.ebi.ac.uk/~remco/ChIP-Seq_course/gfp.bw . Go to a region on chromosome 1 (e.g. 1 : 34823162 - 35323161 ), and zoom in and out to view the signal and peak regions. Be aware that the y-axis of each track is auto-scaled independently of each other, so bigger-looking peaks may not actually be bigger! Always look at the values on the left hand side axis. What can you say about the profile of Oct4 peaks in this region? There are no significant Oct4 peaks over the selected region. Compare it with H3K4me3 histone modification wig file we have generated at http://www.ebi.ac.uk/~remco/ChIP-Seq_course/H3K4me3.bw . H3K4me3 has a region that contains relatively high peaks than Oct4. Jump to 1 : 36066594 - 36079728 for a sample peak. Do you think H3K4me3 peaks regions contain one or more modification sites? What about Oct4? Yes. There are roughly three peaks, which indicate the possibility of having more than one modification sites in this region. For Oct4, no peak can be observed.","title":"Viewing results with the Genome browser"},{"location":"chip-seq/handout/chip-seq-draft-SL_ST/#advanced-session","text":"Ensembl Visualization Launch a web browser and go to the Ensembl website at http://www.ensembl.org/index.html . Choose the genome of interest (in this case, mouse) on the left side of the page, browse to any location in the genome or click one of the demo links provided on the web page. Click on the Add your data link on the left, then choose Attach remote file . Wig files are large so are inconvenient for uploading directly to the Ensemble Genome browser. Instead, we will convert it to an indexed binary format and put this into a web accessible place such as on a HTTP, HTTPS, or FTP server. This makes all the browsing process much faster. Detailed instructions for generating a bigWig from a wig type file can be found at: http://genome.ucsc.edu/goldenPath/help/bigWig.html . We have generated bigWig files in advance for you to upload to the Ensembl browser. They are at the following URL: http://www.ebi.ac.uk/~remco/ChIP-Seq_course/Oct4.bw To visualise the data: Paste the location above in the field File URL. Choose data format bigWig. Choose some informative name and in the next window choose the colour of your preference. Click Save and close the window to return to the genome browser. Repeat the process for the gfp control sample, located at: http://www.ebi.ac.uk/~remco/ChIP-Seq_course/gfp.bw . If can not see your tracks: Click on \u2019Configure this page\u2019 in left panel. In \u2019Configure region Overview\u2019 tab click on \u2019Ypur data\u2019 in left panel. Check the boxes in \u2019Enable/Disable all tracks\u2019 for you *.bw files by selecting \u2019wiggle plot in the pop up menu. After uploading, choose Configure this page , and under Your data tick both boxes. Closing the window will save these changes. Go to a region on chromosome 1 (e.g. 1 : 34823162 - 35323161 ), and zoom in and out to view the signal and peak regions. Be aware that the y-axis of each track is auto-scaled independently of each other, so bigger-looking peaks may not actually be bigger! Always look at the values on the left hand side axis. MACS generates its peak files in a file format called bed file. This is a simple text format containing genomic locations, specified by chromosome, begin and end positions, and some more optional information. See http://genome.ucsc.edu/FAQ/FAQformat.html#format1 for details. Bed files can also be uploaded to the Ensembl browser.","title":"Advanced Session"},{"location":"chip-seq/handout/chip-seq-draft-SL_ST/#motif-analysis","text":"It is often interesting to find out whether we can identify transcription factor binding sites (TFBSs) from the input DNA sequences. TFBSs which share a similar sequence pattern (motif) are presumed to have biological functions. Here we introduce three motif discovery tools named Trawler, RSAT peak-motifs, and MEME-ChIP, which use different searching algorithms. This might lead to varying results. Hence, it is generally a good idea to use several tools to identify TFBSs. The more tools confirm the same result, the better, which is also called an orthogonal approach. Eventually, you probably want to validate your in silico findings in vivo or in vitro.","title":"Motif analysis"},{"location":"chip-seq/handout/chip-seq-draft-SL_ST/#motif-discovery-with-trawler","text":"Trawler is a fast, yet accurate motif discovery tool that accepts both, BED and FASTA files as input file formats. BED files are generated when you process and analyse your NGS data. Thus, it is handy to use them directly in Trawler. Other tools do not accept BED files as input. With Trawler, BED files can be converted into FASTA files that can then be used for other motif discovery tools (e.g. RSAT peak-motifs and MEME ChIP). Running Trawler Go to the website https://trawler.erc.monash.edu.au Run Trawler with BED file as input, and wait for the results Download both sample and background files in FASTA format. Right click and choose: \u2019Save the link as\u2026\u2019 Which motif was found to be the most similar to your motif? Sox2","title":"Motif discovery with Trawler"},{"location":"chip-seq/handout/chip-seq-draft-SL_ST/#optional-motif-discovery-with-rsat-peak-motif","text":"The motif discovery tool RSAT peak-motifs uses FASTA files as input. An optional background can be uploaded in FASTA format. RSAT peak-motifs automatically outputs motifs of 6 and 7 nucleotides length (two separate files). While still accurate, the running time is longer compared to Trawler (up to 20 minutes depending on the size of the files). Running RSAT peak-motifs Go to the website http://floresta.eead.csic.es/rsat/peak-motifs~f~orm.cgi Upload input and background FASTA files just downloaded from Trawler Wait until the discovery finishes.","title":"Optional: Motif discovery with RSAT peak-motif"},{"location":"chip-seq/handout/chip-seq-draft-SL_ST/#optional-motif-discovery-with-meme-chip","text":"MEME-ChIP is a popular motif discovery tool and part of MEME Suite. MEME-ChIP accepts input files in FASTA format. It is not necessary to upload your own background because MEME-ChIP uses its own. Although MEME-ChIP is one of the most popular motif discovery tools, the identified motifs are not very accurate and the motif search might take up to one hour. MEME-ChIP outputs the three motifs with the lowest E-Value. Running MEME-ChIP Go to the website http://meme-suite.org/tools/meme-chip Upload input and background FASTA files just downloaded from Trawler Wait until the discovery finishes.","title":"Optional: Motif discovery with MEME-ChIP"},{"location":"chip-seq/handout/chip-seq-draft-SL_ST/#reference","text":"Chen, X et al.: Integration of external signaling pathways with the core transcriptional network in embryonic stem cells. Cell 133:6, 1106-17 (2008).","title":"Reference"},{"location":"chip-seq/handout/handout/","text":"Key Learning Outcomes After completing this practical the trainee should be able to: Perform simple ChIP-Seq analysis, e.g. the detection of immuno-enriched areas using the chosen peak caller program MACS Visualize the peak regions through a genome browser, e.g. Ensembl, and identify the real peak regions Perform functional annotation and detect potential binding sites (motif) in the predicted binding regions using motif discovery tool, e.g. MEME. Resources You\u2019ll be Using Tools Used MACS : \\ http://liulab.dfci.harvard.edu/MACS/index.html Ensembl : \\ http://www.ensembl.org PeakAnalyzer : \\ http://www.ebi.ac.uk/bertone/software MEME : \\ http://meme.ebi.edu.au/meme/tools/meme TOMTOM : \\ http://meme.ebi.edu.au/meme/tools/tomtom DAVID : \\ http://david.abcc.ncifcrf.gov GOstat : \\ http://gostat.wehi.edu.au Sources of Data http://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-11431 Introduction The goal of this hands-on session is to perform some basic tasks in the analysis of ChIP-seq data. In fact, you already performed the first step, alignment of the reads to the genome, in the previous session. We start from the aligned reads and we will find immuno-enriched areas using the peak caller MACS. We will visualize the identified regions in a genome browser and perform functional annotation and motif analysis on the predicted binding regions. Prepare the Environment The material for this practical can be found in the ChIP-seq section of your manual. Please make sure that this directory also contains the SAM/BAM files you produced during the alignment practical. If you didn\u2019t have time to align the control file called gfp.fastq during the alignment practical, please do it now. Follow the same steps, from the bowtie2 alignment step, as for the Oct4.fastq file. Open the Terminal and go to the chipseq directory: cd /home/trainee/chipseq Finding enriched areas using MACS MACS stands for Model based analysis of ChIP-seq. It was designed for identifying transcription factor binding sites. MACS captures the influence of genome complexity to evaluate the significance of enriched ChIP regions, and improves the spatial resolution of binding sites through combining the information of both sequencing tag position and orientation. MACS can be easily used for ChIP-Seq data alone, or with a control sample to increase specificity. Consult the MACS help file to see the options and parameters: macs --help The input for MACS can be in ELAND, BED, SAM, BAM or BOWTIE formats (you just have to set the \u2013format option). Options that you will have to use include: -t : To indicate the input ChIP file. -c : To indicate the name of the control file. \u2013format : To change the file format. The default format is bed. \u2013name : To set the name of the output files. \u2013gsize : This is the mappable genome size. With the read length we have, $70\\%$ of the genome is a fair estimation. Since in this analysis we include only reads from chromosome 1 (197Mbases), we will use a \u2013gsize of 138Mbases (70% of 197Mbases). \u2013tsize : To set the read length (look at the FASTQ files to check the length). \u2013wig : To generate signal wig files for viewing in a genome browser. Since this process is time consuming, it is recommended to run MACS first with this flag off, and once you decide on the values of the parameters, run MACS again with this flag on. \u2013diag : To generate a saturation table, which gives an indication whether the sequenced reads give a reliable representation of the possible peaks. Now run macs using the following command: macs -t &lt;Oct4_aligned_bam_file&gt; -c &lt;gfp_aligned_bam_file&gt; --format=BAM --name=Oct4 --gsize=138000000 --tsize=26 --diag --wig Look at the output saturation table ( Oct4_diag.xls ). To open this file file, right-click on it and choose \u201cOpen with\u201d and select LibreOffice. Do you think that more sequencing is necessary? Open the Excel peak file and view the peak details. Note that the number of tags (column 6) refers to the number of reads in the whole peak region and not the peak height. Viewing results with the Ensembl genome browser It is often instructive to look at your data in a genome browser. Before, we used IGV, a stand-alone browser, which has the advantage of being installed locally and providing fast access. Web-based genome browsers, like Ensembl or the UCSC browser, are slower, but provide more functionality. They do not only allow for more polished and flexible visualisation, but also provide easy access to a wealth of annotations and external data sources. This makes it straightforward to relate your data with information about repeat regions, known genes, epigenetic features or areas of cross-species conservation, to name just a few. As such, they are useful tools for exploratory analysis. They will allow you to get a \u2018feel\u2019 for the data, as well as detecting abnormalities and problems. Also, exploring the data in such a way may give you ideas for further analyses. Launch a web browser and go to the Ensembl website at http://www.ensembl.org/index.html Choose the genome of interest (in this case, mouse) on the left side of the page, browse to any location in the genome or click one of the demo links provided on the web page. [H] [fig:MouseHome] Click on the Add your data link on the left, then choose Add your data in the Custom Data tab. [H] [fig:AddData] Wig files are large so are inconvenient for uploading directly to the Ensemble Genome browser. Instead, we will convert it to an indexed binary format and put this into a web accessible place such as on a HTTP, HTTPS, or FTP server. This makes all the browsing process much faster. Detailed instructions for generating a bigWig from a wig type file can be found at: http://genome.ucsc.edu/goldenPath/help/bigWig.html . We have generated bigWig files in advance for you to upload to the Ensembl browser. They are at the following URL: http://www.ebi.ac.uk/~remco/ChIP-Seq_course/Oct4.bw (Please right click and choose \u201cCopy Link Address\u201d to copy the URL). To visualise the data: Paste the location above in the Data field. Set data format bigWig. Choose some informative name Click Add data and in the next window choose the colour of your preference. Click Save and close the window to return to the genome browser. [H] [fig:UploadURL] Repeat the process for the gfp control sample, located at: http://www.ebi.ac.uk/~remco/ChIP-Seq_course/gfp.bw (Please right click and choose \u201cCopy Link Address\u201d to copy the URL). After uploading, to make sure your data is visible: Click in the left hand panel Configure this page Select the first tab in the pop up Configure Region Image Click Your data in the left panel Choose each of the uploaded *.bw files to confirm the Wiggle plot in Change track style pop up menu has been choosen. Closing the window will save these changes. [H] [fig:ConfigureYourData] Go to a region on chromosome 1 (e.g. 1 : 34823162 - 35323161 ), and zoom in and out to view the signal and peak regions. Be aware that the y-axis of each track is auto-scaled independently of each other, so bigger-looking peaks may not actually be bigger! Always look at the values on the left hand side axis. What can you say about the profile of Oct4 peaks in this region? There are no significant Oct4 peaks over the selected region. Compare it with H3K4me3 histone modification wig file we have generated at http://www.ebi.ac.uk/~remco/ChIP-Seq_course/H3K4me3.bw . H3K4me3 has a region that contains relatively high peaks than Oct4. Jump to 1 : 36066594 - 36079728 for a sample peak. Do you think H3K4me3 peaks regions contain one or more modification sites? What about Oct4? Yes. There are roughly three peaks, which indicate the possibility of having more than one modification sites in this region. For Oct4, no peak can be observed. MACS generates its peak files in a file format called bed file. This is a simple text format containing genomic locations, specified by chromosome, begin and end positions, and some more optional information. See http://genome.ucsc.edu/FAQ/FAQformat.html#format1 for details. Bed files can also be uploaded to the Ensembl browser. Try uploading the peak file generated by MACS to Ensembl. Find the first peak and the peak with the highest score (the fifth column) in the file and see if the peak looks convincing to you. Annotation: From peaks to biological interpretation In order to biologically interpret the results of ChIP-seq experiments, it is usually recommended to look at the genes and other annotated elements that are located in proximity to the identified enriched regions. This can be easily done using PeakAnalyzer. Go to the PeakAnalyzer tool directory: cd /home/trainee/chipseq/peakanalyzer/1.4 Launch the PeakAnalyzer program by typing: java -jar PeakAnalyzer.jar &amp; Click okay in the pop up error message \u201cCan\u2019t reach ensembl.org\u201d. The first window allows you to choose between the split application (which we will try next) and peak annotation. Choose the peak annotation option and click Next . We would like to find the closest downstream genes to each peak, and the genes that overlap with the peak region. For that purpose you should choose the NDG option and click Next . Fill in the location of the peak file Oct4_peaks.bed , and choose the mouse GTF as the annotation file. You don\u2019t have to define a symbol file since gene symbols are included in the GTF file. Choose the output directory and run the program. When the program has finished running, you will have the option to generate plots, by pressing the Generate plots button. This is only possible if R is installed on your computer, as it is on this system. A PDF file with the plots will be generated in the output folder. You could generate similar plots with Excel using the output files that were generated by PeakAnalyzer. This list of closest downstream genes (contained in the file Oct4_peaks.ndg.bed ) can be the basis of further analysis. For instance, you could look at the Gene Ontology terms associated with these genes to get an idea of the biological processes that may be affected. Web-based tools like DAVID ( http://david.abcc.ncifcrf.gov ) or GOstat ( http://gostat.wehi.edu.au ) take a list of genes and return the enriched GO categories. We can pull out Ensemble Transcript IDs from the Oct4_peaks.ndg.bed file and write them to another file ready for use with DAVID or GOstat: cut -f 5 Oct4_peaks.ndg.bed | sed '1 d' &gt; Oct4_peaks.ndg.tid Motif analysis It is often interesting to find out whether we can associate the identified binding sites with a sequence pattern or motif. We will use MEME for motif analysis. The input for MEME should be a file in FASTA format containing the sequences of interest. In our case, these are the sequences of the identified peaks that probably contain Oct4 binding sites. Since many peak-finding tools merge overlapping areas of enrichment, the resulting peaks tend to be much wider than the actual binding sites. Sub-dividing the enriched areas by accurately partitioning enriched loci into a finer-resolution set of individual binding sites, and fetching sequences from the summit region where binding motifs are most likely to appear enhances the quality of the motif analysis. Sub-peak summit sequences can normally be retrieved directly from the Ensembl database using PeakAnalyzer. Due to the connection error we have provided the fasta sequences. If you have closed the PeakAnalyzer running window, open it again. If it is still open, just go back to the first window. Choose the split peaks utility and click Next . The input consists of files generated by most peak-finding tools: a file containing the chromosome, start and end locations of the enriched regions, and a .wig signal file describing the size and shape of each peak. Fill in the location of both files Oct4_peaks.bed and the wig file generated by MACS, which is under the Oct4_MACS_wiggle/treat/ directory. Normally we would check the option to Fetch subpeak sequences and click Next . As we have no connection to ensembl We have provided the subpeak sequences. In the next window you have to set some parameters for splitting the peaks. Separation float : Keep the default value. This value determines when a peak will be separated into sub-peaks. This is the ratio between a valley and its neighbouring summit (the lower summit of the two). For example, if you set this height to be 0.5 , two sub-peaks will be separated only if the height of the lower summit is twice the height of the valley. Minimum height : Set this to be 5 . Only sub-peaks with at least this number of tags in their summit region will be separated. Change the organism name from the default human to mouse and run the program. Organism : Choose Mus_musculus from the drop down list. Since the program has to read large wig files, it will take a few minutes to run. Once the run is finished, two output files will be produced. The first describes the location of the sub-peaks, and the second is a FASTA file containing 300 sequences of length 61 bases, taken from the summit regions of the highest sub-peaks. The supplied fasta file can be found in Desktop/chipseq/peakanalyzer/1.4/Data/Oct4_peaks.bestSubPeaks.fa Open a web bowser and go to the MEME website at http://meme-suite.org/tools/meme-chip , and fill in the necessary details, such as: Your e-mail address The sub-peaks FASTA file Oct4_peaks.bestSubPeaks.fa (will need uploading), or just paste in the sequences Select the Jaspar vertebrates and UniPROBE Mosue data base Under the MEME options section enter the following: The number of motifs we expect to find (1 per sequence) The maximum number of motifs to find (3 by default). For Oct4 one classical motif is known Set the width of the desired motif (between 6 to 20) The html link will be returned on completion and you will also receive the results by e-mail. This usually doesn\u2019t take more than a few minutes. Open the link that leads to the HTML results page. Scroll down until you see the first motif logo. We would like to know if this motif is similar to any other known motif. Scroll down to the TOMTOM html results. Which motif was found to be the most similar to your motif? Sox2 Reference Chen, X et al.: Integration of external signaling pathways with the core transcriptional network in embryonic stem cells. Cell 133:6, 1106-17 (2008).","title":"ChIP-Seq"},{"location":"chip-seq/handout/handout/#key-learning-outcomes","text":"After completing this practical the trainee should be able to: Perform simple ChIP-Seq analysis, e.g. the detection of immuno-enriched areas using the chosen peak caller program MACS Visualize the peak regions through a genome browser, e.g. Ensembl, and identify the real peak regions Perform functional annotation and detect potential binding sites (motif) in the predicted binding regions using motif discovery tool, e.g. MEME.","title":"Key Learning Outcomes"},{"location":"chip-seq/handout/handout/#resources-youll-be-using","text":"","title":"Resources You\u2019ll be Using"},{"location":"chip-seq/handout/handout/#tools-used","text":"MACS : \\ http://liulab.dfci.harvard.edu/MACS/index.html Ensembl : \\ http://www.ensembl.org PeakAnalyzer : \\ http://www.ebi.ac.uk/bertone/software MEME : \\ http://meme.ebi.edu.au/meme/tools/meme TOMTOM : \\ http://meme.ebi.edu.au/meme/tools/tomtom DAVID : \\ http://david.abcc.ncifcrf.gov GOstat : \\ http://gostat.wehi.edu.au","title":"Tools Used"},{"location":"chip-seq/handout/handout/#sources-of-data","text":"http://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-11431","title":"Sources of Data"},{"location":"chip-seq/handout/handout/#introduction","text":"The goal of this hands-on session is to perform some basic tasks in the analysis of ChIP-seq data. In fact, you already performed the first step, alignment of the reads to the genome, in the previous session. We start from the aligned reads and we will find immuno-enriched areas using the peak caller MACS. We will visualize the identified regions in a genome browser and perform functional annotation and motif analysis on the predicted binding regions.","title":"Introduction"},{"location":"chip-seq/handout/handout/#prepare-the-environment","text":"The material for this practical can be found in the ChIP-seq section of your manual. Please make sure that this directory also contains the SAM/BAM files you produced during the alignment practical. If you didn\u2019t have time to align the control file called gfp.fastq during the alignment practical, please do it now. Follow the same steps, from the bowtie2 alignment step, as for the Oct4.fastq file. Open the Terminal and go to the chipseq directory: cd /home/trainee/chipseq","title":"Prepare the Environment"},{"location":"chip-seq/handout/handout/#finding-enriched-areas-using-macs","text":"MACS stands for Model based analysis of ChIP-seq. It was designed for identifying transcription factor binding sites. MACS captures the influence of genome complexity to evaluate the significance of enriched ChIP regions, and improves the spatial resolution of binding sites through combining the information of both sequencing tag position and orientation. MACS can be easily used for ChIP-Seq data alone, or with a control sample to increase specificity. Consult the MACS help file to see the options and parameters: macs --help The input for MACS can be in ELAND, BED, SAM, BAM or BOWTIE formats (you just have to set the \u2013format option). Options that you will have to use include: -t : To indicate the input ChIP file. -c : To indicate the name of the control file. \u2013format : To change the file format. The default format is bed. \u2013name : To set the name of the output files. \u2013gsize : This is the mappable genome size. With the read length we have, $70\\%$ of the genome is a fair estimation. Since in this analysis we include only reads from chromosome 1 (197Mbases), we will use a \u2013gsize of 138Mbases (70% of 197Mbases). \u2013tsize : To set the read length (look at the FASTQ files to check the length). \u2013wig : To generate signal wig files for viewing in a genome browser. Since this process is time consuming, it is recommended to run MACS first with this flag off, and once you decide on the values of the parameters, run MACS again with this flag on. \u2013diag : To generate a saturation table, which gives an indication whether the sequenced reads give a reliable representation of the possible peaks. Now run macs using the following command: macs -t &lt;Oct4_aligned_bam_file&gt; -c &lt;gfp_aligned_bam_file&gt; --format=BAM --name=Oct4 --gsize=138000000 --tsize=26 --diag --wig Look at the output saturation table ( Oct4_diag.xls ). To open this file file, right-click on it and choose \u201cOpen with\u201d and select LibreOffice. Do you think that more sequencing is necessary? Open the Excel peak file and view the peak details. Note that the number of tags (column 6) refers to the number of reads in the whole peak region and not the peak height.","title":"Finding enriched areas using MACS"},{"location":"chip-seq/handout/handout/#viewing-results-with-the-ensembl-genome-browser","text":"It is often instructive to look at your data in a genome browser. Before, we used IGV, a stand-alone browser, which has the advantage of being installed locally and providing fast access. Web-based genome browsers, like Ensembl or the UCSC browser, are slower, but provide more functionality. They do not only allow for more polished and flexible visualisation, but also provide easy access to a wealth of annotations and external data sources. This makes it straightforward to relate your data with information about repeat regions, known genes, epigenetic features or areas of cross-species conservation, to name just a few. As such, they are useful tools for exploratory analysis. They will allow you to get a \u2018feel\u2019 for the data, as well as detecting abnormalities and problems. Also, exploring the data in such a way may give you ideas for further analyses. Launch a web browser and go to the Ensembl website at http://www.ensembl.org/index.html Choose the genome of interest (in this case, mouse) on the left side of the page, browse to any location in the genome or click one of the demo links provided on the web page. [H] [fig:MouseHome] Click on the Add your data link on the left, then choose Add your data in the Custom Data tab. [H] [fig:AddData] Wig files are large so are inconvenient for uploading directly to the Ensemble Genome browser. Instead, we will convert it to an indexed binary format and put this into a web accessible place such as on a HTTP, HTTPS, or FTP server. This makes all the browsing process much faster. Detailed instructions for generating a bigWig from a wig type file can be found at: http://genome.ucsc.edu/goldenPath/help/bigWig.html . We have generated bigWig files in advance for you to upload to the Ensembl browser. They are at the following URL: http://www.ebi.ac.uk/~remco/ChIP-Seq_course/Oct4.bw (Please right click and choose \u201cCopy Link Address\u201d to copy the URL). To visualise the data: Paste the location above in the Data field. Set data format bigWig. Choose some informative name Click Add data and in the next window choose the colour of your preference. Click Save and close the window to return to the genome browser. [H] [fig:UploadURL] Repeat the process for the gfp control sample, located at: http://www.ebi.ac.uk/~remco/ChIP-Seq_course/gfp.bw (Please right click and choose \u201cCopy Link Address\u201d to copy the URL). After uploading, to make sure your data is visible: Click in the left hand panel Configure this page Select the first tab in the pop up Configure Region Image Click Your data in the left panel Choose each of the uploaded *.bw files to confirm the Wiggle plot in Change track style pop up menu has been choosen. Closing the window will save these changes. [H] [fig:ConfigureYourData] Go to a region on chromosome 1 (e.g. 1 : 34823162 - 35323161 ), and zoom in and out to view the signal and peak regions. Be aware that the y-axis of each track is auto-scaled independently of each other, so bigger-looking peaks may not actually be bigger! Always look at the values on the left hand side axis. What can you say about the profile of Oct4 peaks in this region? There are no significant Oct4 peaks over the selected region. Compare it with H3K4me3 histone modification wig file we have generated at http://www.ebi.ac.uk/~remco/ChIP-Seq_course/H3K4me3.bw . H3K4me3 has a region that contains relatively high peaks than Oct4. Jump to 1 : 36066594 - 36079728 for a sample peak. Do you think H3K4me3 peaks regions contain one or more modification sites? What about Oct4? Yes. There are roughly three peaks, which indicate the possibility of having more than one modification sites in this region. For Oct4, no peak can be observed. MACS generates its peak files in a file format called bed file. This is a simple text format containing genomic locations, specified by chromosome, begin and end positions, and some more optional information. See http://genome.ucsc.edu/FAQ/FAQformat.html#format1 for details. Bed files can also be uploaded to the Ensembl browser. Try uploading the peak file generated by MACS to Ensembl. Find the first peak and the peak with the highest score (the fifth column) in the file and see if the peak looks convincing to you.","title":"Viewing results with the Ensembl genome browser"},{"location":"chip-seq/handout/handout/#annotation-from-peaks-to-biological-interpretation","text":"In order to biologically interpret the results of ChIP-seq experiments, it is usually recommended to look at the genes and other annotated elements that are located in proximity to the identified enriched regions. This can be easily done using PeakAnalyzer. Go to the PeakAnalyzer tool directory: cd /home/trainee/chipseq/peakanalyzer/1.4 Launch the PeakAnalyzer program by typing: java -jar PeakAnalyzer.jar &amp; Click okay in the pop up error message \u201cCan\u2019t reach ensembl.org\u201d. The first window allows you to choose between the split application (which we will try next) and peak annotation. Choose the peak annotation option and click Next . We would like to find the closest downstream genes to each peak, and the genes that overlap with the peak region. For that purpose you should choose the NDG option and click Next . Fill in the location of the peak file Oct4_peaks.bed , and choose the mouse GTF as the annotation file. You don\u2019t have to define a symbol file since gene symbols are included in the GTF file. Choose the output directory and run the program. When the program has finished running, you will have the option to generate plots, by pressing the Generate plots button. This is only possible if R is installed on your computer, as it is on this system. A PDF file with the plots will be generated in the output folder. You could generate similar plots with Excel using the output files that were generated by PeakAnalyzer. This list of closest downstream genes (contained in the file Oct4_peaks.ndg.bed ) can be the basis of further analysis. For instance, you could look at the Gene Ontology terms associated with these genes to get an idea of the biological processes that may be affected. Web-based tools like DAVID ( http://david.abcc.ncifcrf.gov ) or GOstat ( http://gostat.wehi.edu.au ) take a list of genes and return the enriched GO categories. We can pull out Ensemble Transcript IDs from the Oct4_peaks.ndg.bed file and write them to another file ready for use with DAVID or GOstat: cut -f 5 Oct4_peaks.ndg.bed | sed '1 d' &gt; Oct4_peaks.ndg.tid","title":"Annotation: From peaks to biological interpretation"},{"location":"chip-seq/handout/handout/#motif-analysis","text":"It is often interesting to find out whether we can associate the identified binding sites with a sequence pattern or motif. We will use MEME for motif analysis. The input for MEME should be a file in FASTA format containing the sequences of interest. In our case, these are the sequences of the identified peaks that probably contain Oct4 binding sites. Since many peak-finding tools merge overlapping areas of enrichment, the resulting peaks tend to be much wider than the actual binding sites. Sub-dividing the enriched areas by accurately partitioning enriched loci into a finer-resolution set of individual binding sites, and fetching sequences from the summit region where binding motifs are most likely to appear enhances the quality of the motif analysis. Sub-peak summit sequences can normally be retrieved directly from the Ensembl database using PeakAnalyzer. Due to the connection error we have provided the fasta sequences. If you have closed the PeakAnalyzer running window, open it again. If it is still open, just go back to the first window. Choose the split peaks utility and click Next . The input consists of files generated by most peak-finding tools: a file containing the chromosome, start and end locations of the enriched regions, and a .wig signal file describing the size and shape of each peak. Fill in the location of both files Oct4_peaks.bed and the wig file generated by MACS, which is under the Oct4_MACS_wiggle/treat/ directory. Normally we would check the option to Fetch subpeak sequences and click Next . As we have no connection to ensembl We have provided the subpeak sequences. In the next window you have to set some parameters for splitting the peaks. Separation float : Keep the default value. This value determines when a peak will be separated into sub-peaks. This is the ratio between a valley and its neighbouring summit (the lower summit of the two). For example, if you set this height to be 0.5 , two sub-peaks will be separated only if the height of the lower summit is twice the height of the valley. Minimum height : Set this to be 5 . Only sub-peaks with at least this number of tags in their summit region will be separated. Change the organism name from the default human to mouse and run the program. Organism : Choose Mus_musculus from the drop down list. Since the program has to read large wig files, it will take a few minutes to run. Once the run is finished, two output files will be produced. The first describes the location of the sub-peaks, and the second is a FASTA file containing 300 sequences of length 61 bases, taken from the summit regions of the highest sub-peaks. The supplied fasta file can be found in Desktop/chipseq/peakanalyzer/1.4/Data/Oct4_peaks.bestSubPeaks.fa Open a web bowser and go to the MEME website at http://meme-suite.org/tools/meme-chip , and fill in the necessary details, such as: Your e-mail address The sub-peaks FASTA file Oct4_peaks.bestSubPeaks.fa (will need uploading), or just paste in the sequences Select the Jaspar vertebrates and UniPROBE Mosue data base Under the MEME options section enter the following: The number of motifs we expect to find (1 per sequence) The maximum number of motifs to find (3 by default). For Oct4 one classical motif is known Set the width of the desired motif (between 6 to 20) The html link will be returned on completion and you will also receive the results by e-mail. This usually doesn\u2019t take more than a few minutes. Open the link that leads to the HTML results page. Scroll down until you see the first motif logo. We would like to know if this motif is similar to any other known motif. Scroll down to the TOMTOM html results. Which motif was found to be the most similar to your motif? Sox2","title":"Motif analysis"},{"location":"chip-seq/handout/handout/#reference","text":"Chen, X et al.: Integration of external signaling pathways with the core transcriptional network in embryonic stem cells. Cell 133:6, 1106-17 (2008).","title":"Reference"},{"location":"cli/","text":"Introduction to Command Line Bioinformatics Training Platform (BTP) Module: Introduction to Command Line Topic Introduction to Command Line Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes Familiarise yourself with the command line environment on a Linux operating system. Run some basic linux system and file operation commands Navigation of biological data files structure and manipulation Time Required 1 hr License The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"Introduction to Command Line"},{"location":"cli/#introduction-to-command-line","text":"Bioinformatics Training Platform (BTP) Module: Introduction to Command Line Topic Introduction to Command Line Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes Familiarise yourself with the command line environment on a Linux operating system. Run some basic linux system and file operation commands Navigation of biological data files structure and manipulation Time Required 1 hr","title":"Introduction to Command Line"},{"location":"cli/#license","text":"The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"License"},{"location":"cli/datasets/","text":"Information about what datasets are required for a module\u2019s tutorial exercises are described in a data.yaml file. As well describing where to source the dataset files from, we also need to describe where on the filesystem the files should reside and who should own them.","title":"Home"},{"location":"cli/handout/","text":"Introduction to Command Line Acknowledgements This module was developed by Bioplatforms Australia, in partnership with CSIRO, and with support from the European Bioinformatics Institute (EBI), a member of the European Molecular Biology Laboratory (EMBL) in the UK. The module content has been maintained and updated by a network of dedicated [bioinformatics trainers] ( http://www.bioplatforms.com/bioinformatics-training/ ) from around Australia. The Bioinformatics Training Platform was developed in collaboration with the Monash Bioinformatics Platform. The development of this module is also supported by the NSW Office of Health and Medical Research and the Commonwealth Government National Collaborative Research Infrastructure Strategy (NCRIS).","title":"Introduction to Command Line"},{"location":"cli/handout/#introduction-to-command-line","text":"","title":"Introduction to Command Line"},{"location":"cli/handout/#acknowledgements","text":"This module was developed by Bioplatforms Australia, in partnership with CSIRO, and with support from the European Bioinformatics Institute (EBI), a member of the European Molecular Biology Laboratory (EMBL) in the UK. The module content has been maintained and updated by a network of dedicated [bioinformatics trainers] ( http://www.bioplatforms.com/bioinformatics-training/ ) from around Australia. The Bioinformatics Training Platform was developed in collaboration with the Monash Bioinformatics Platform. The development of this module is also supported by the NSW Office of Health and Medical Research and the Commonwealth Government National Collaborative Research Infrastructure Strategy (NCRIS).","title":"Acknowledgements"},{"location":"cli/handout/handout/","text":"Key Learning Outcomes After completing this practical the trainee should be able to: Familiarise yourself with the command line environment on a Linux operating system. Run some basic linux system and file operation commands Navigation of biological data files structure and manipulation Resources Tools Basic Linux system commands on an Ubuntu OS. Basic file operation commands Links Software Carpentry Example 1000Genome Project data Author Information Primary Author(s): Matt Field matt.field@anu.edu.au Shell Exercise Let\u2019s try out your new shell skills on some real data. The file 1000gp.vcf is a small sample (1%) of a very large text file containing human genetics data. Specifically, it describes genetic variation in three African individuals sequenced as part of the 1000 Genomes Project . The \u2019vcf\u2019 extension lets us know that it\u2019s in a specific text format, namely \u2019Variant Call Format\u2019. The file starts with a bunch of comment lines (they start with \u2019#\u2019 or \u2019##\u2019), and then a large number of data lines. This VCF file lists the differences between the three African individuals and a standard \u2019individual\u2019 called the reference (actually based upon a few different people). Each line in the file corresponds to a difference. The line tells us the position of the difference (chromosome and position), the genetic sequence in the reference, and the corresponding sequence in each of the three Africans. Before we start processing the file, let\u2019s get a high-level view of the file that we\u2019re about to work with. Open the Terminal and go to the directory where the data are stored: cd /home/trainee/cli ls pwd ls -lh 1000gp.vcf wc -l 1000gp.vcf Question What is the file size (in kilo-bytes), and how many lines are in the file?. Hint man ls , man wc Answer 3.6M 45034 lines Because this file is so large, you\u2019re going to almost always want to pipe (\u2018|\u2019) the result of any command to less (a simple text viewer, type q to exit) or head (to print the first 10 lines) so that you don\u2019t accidentally print 45,000 lines to the screen. Let\u2019s start by printing the first 5 lines to see what it looks like. head -5 1000gp.vcf That isn\u2019t very interesting; it\u2019s just a bunch of the comments at the beginning of the file (they all start with # )! Print the first 20 lines to see more of the file. head -20 1000gp.vcf Okay, so now we can see the basic structure of the file. A few comment lines that start with \u2019#\u2019 or \u2019##\u2019 and then a bunch of lines of data that contain all the data and are pretty hard to understand. Each line of data contains the same number of fields, and all fields are separated with TABs. These fields are: the chromosome (which volume the difference is in) the position (which character in the volume the difference starts at) the ID of the difference the sequence in the reference human(s) The rest of the columns tell us, in a rather complex way, a bunch of additional information about that position, including: the predicted sequence for each of the three Africans and how confident the scientists are that these sequences are correct. To start analyzing the actual data, we have to remove the header. Question How can we print the first 10 non-header lines (those that don\u2019t start with a \u2019#\u2019)? Hint man grep (remember to use pipes \u2018|\u2019) Answer grep -v \"^#\" 1000gp.vcf | head This is an advanced section. Question How many lines of data are in the file (rather than counting the number of header lines and subtracting, try just counting the number of data lines)? Answer grep -v \"^#\" 1000gp.vcf | wc -l (should print 45024) Where these differences are located can be important. If all the differences between two encyclopedias were in just the first volume, that would be interesting. The first field of each data line is the name of the chromosome that the difference occurs on (which volume we\u2019re on). Question Print the first 10 chromosomes, one per line. Hint man cut (remember to remove header lines first) Answer grep -v \"^#\" 1000gp.vcf | cut -f 1 | head As you should have observed, the first 10 lines are on numbered chromosomes. Every normal cell in your body has 23 pairs of chromosomes, 22 pairs of \u2018autosomal\u2019 chromosomes (these are numbered 1-22) and a pair of sex chromosomes (two Xs if you\u2019re female, an X and a Y if you\u2019re male). Let\u2019s look at which chromosomes these variations are on. Question Print a list of the chromosomes that are in the file (each chromosome name should only be printed once, so you should only print 23 lines). Hint Remove all duplicates from your previous answer ( man sort ) Answer grep -v \"^#\" 1000gp.vcf | cut -f 1 | sort -u Rather than using sort to print unique results, a common pipeline is to first sort and then pipe to another UNIX command, uniq . The uniq command takes sorted input and prints only unique lines, but it provides more flexibility than just using sort by itself. Keep in mind, if the input isn\u2019t sorted, uniq won\u2019t work properly. Question Using sort and uniq , print the number of times each chromosome occurs in the file. Hint man uniq Answer grep -v \"^#\" 1000gp.vcf | cut -f 1 | sort | uniq -c Question Add to your previous solution to list the chromosomes from most frequently observed to least frequently observed. Hint Make sure you\u2019re sorting in descending order. By default, sort sorts in ascending order. Answer grep -v \"^#\" 1000gp.vcf | cut -f 1 | sort | uniq -c | sort -n -r This is great, but biologists might also like to see the chromosomes ordered by their number (not dictionary order), since different chromosomes have different attributes and this ordering allows them to find a specific chromosome more easily. Question Sort the previous output by chromosome number Hint A lot of the power of sort comes from the fact that you can specify which fields to sort on, and the order in which to sort them. In this case you only need to sort on one field. Answer grep -v \"^#\" 1000gp.vcf | cut -f 1 | sort | uniq -c | sort -k 2n","title":"Introduction to Command Line"},{"location":"cli/handout/handout/#key-learning-outcomes","text":"After completing this practical the trainee should be able to: Familiarise yourself with the command line environment on a Linux operating system. Run some basic linux system and file operation commands Navigation of biological data files structure and manipulation","title":"Key Learning Outcomes"},{"location":"cli/handout/handout/#resources","text":"","title":"Resources"},{"location":"cli/handout/handout/#tools","text":"Basic Linux system commands on an Ubuntu OS. Basic file operation commands","title":"Tools"},{"location":"cli/handout/handout/#links","text":"Software Carpentry Example 1000Genome Project data","title":"Links"},{"location":"cli/handout/handout/#author-information","text":"Primary Author(s): Matt Field matt.field@anu.edu.au","title":"Author Information"},{"location":"cli/handout/handout/#shell-exercise","text":"Let\u2019s try out your new shell skills on some real data. The file 1000gp.vcf is a small sample (1%) of a very large text file containing human genetics data. Specifically, it describes genetic variation in three African individuals sequenced as part of the 1000 Genomes Project . The \u2019vcf\u2019 extension lets us know that it\u2019s in a specific text format, namely \u2019Variant Call Format\u2019. The file starts with a bunch of comment lines (they start with \u2019#\u2019 or \u2019##\u2019), and then a large number of data lines. This VCF file lists the differences between the three African individuals and a standard \u2019individual\u2019 called the reference (actually based upon a few different people). Each line in the file corresponds to a difference. The line tells us the position of the difference (chromosome and position), the genetic sequence in the reference, and the corresponding sequence in each of the three Africans. Before we start processing the file, let\u2019s get a high-level view of the file that we\u2019re about to work with. Open the Terminal and go to the directory where the data are stored: cd /home/trainee/cli ls pwd ls -lh 1000gp.vcf wc -l 1000gp.vcf Question What is the file size (in kilo-bytes), and how many lines are in the file?. Hint man ls , man wc Answer 3.6M 45034 lines Because this file is so large, you\u2019re going to almost always want to pipe (\u2018|\u2019) the result of any command to less (a simple text viewer, type q to exit) or head (to print the first 10 lines) so that you don\u2019t accidentally print 45,000 lines to the screen. Let\u2019s start by printing the first 5 lines to see what it looks like. head -5 1000gp.vcf That isn\u2019t very interesting; it\u2019s just a bunch of the comments at the beginning of the file (they all start with # )! Print the first 20 lines to see more of the file. head -20 1000gp.vcf Okay, so now we can see the basic structure of the file. A few comment lines that start with \u2019#\u2019 or \u2019##\u2019 and then a bunch of lines of data that contain all the data and are pretty hard to understand. Each line of data contains the same number of fields, and all fields are separated with TABs. These fields are: the chromosome (which volume the difference is in) the position (which character in the volume the difference starts at) the ID of the difference the sequence in the reference human(s) The rest of the columns tell us, in a rather complex way, a bunch of additional information about that position, including: the predicted sequence for each of the three Africans and how confident the scientists are that these sequences are correct. To start analyzing the actual data, we have to remove the header. Question How can we print the first 10 non-header lines (those that don\u2019t start with a \u2019#\u2019)? Hint man grep (remember to use pipes \u2018|\u2019) Answer grep -v \"^#\" 1000gp.vcf | head This is an advanced section. Question How many lines of data are in the file (rather than counting the number of header lines and subtracting, try just counting the number of data lines)? Answer grep -v \"^#\" 1000gp.vcf | wc -l (should print 45024) Where these differences are located can be important. If all the differences between two encyclopedias were in just the first volume, that would be interesting. The first field of each data line is the name of the chromosome that the difference occurs on (which volume we\u2019re on). Question Print the first 10 chromosomes, one per line. Hint man cut (remember to remove header lines first) Answer grep -v \"^#\" 1000gp.vcf | cut -f 1 | head As you should have observed, the first 10 lines are on numbered chromosomes. Every normal cell in your body has 23 pairs of chromosomes, 22 pairs of \u2018autosomal\u2019 chromosomes (these are numbered 1-22) and a pair of sex chromosomes (two Xs if you\u2019re female, an X and a Y if you\u2019re male). Let\u2019s look at which chromosomes these variations are on. Question Print a list of the chromosomes that are in the file (each chromosome name should only be printed once, so you should only print 23 lines). Hint Remove all duplicates from your previous answer ( man sort ) Answer grep -v \"^#\" 1000gp.vcf | cut -f 1 | sort -u Rather than using sort to print unique results, a common pipeline is to first sort and then pipe to another UNIX command, uniq . The uniq command takes sorted input and prints only unique lines, but it provides more flexibility than just using sort by itself. Keep in mind, if the input isn\u2019t sorted, uniq won\u2019t work properly. Question Using sort and uniq , print the number of times each chromosome occurs in the file. Hint man uniq Answer grep -v \"^#\" 1000gp.vcf | cut -f 1 | sort | uniq -c Question Add to your previous solution to list the chromosomes from most frequently observed to least frequently observed. Hint Make sure you\u2019re sorting in descending order. By default, sort sorts in ascending order. Answer grep -v \"^#\" 1000gp.vcf | cut -f 1 | sort | uniq -c | sort -n -r This is great, but biologists might also like to see the chromosomes ordered by their number (not dictionary order), since different chromosomes have different attributes and this ordering allows them to find a specific chromosome more easily. Question Sort the previous output by chromosome number Hint A lot of the power of sort comes from the fact that you can specify which fields to sort on, and the order in which to sort them. In this case you only need to sort on one field. Answer grep -v \"^#\" 1000gp.vcf | cut -f 1 | sort | uniq -c | sort -k 2n","title":"Shell Exercise"},{"location":"cli/handout/license/","text":"This work is licensed under a Creative Commons Attribution 3.0 Unported License and the below text is a summary of the main terms of the full Legal Code (the full license) available at http://creativecommons.org/licenses/by/3.0/legalcode . You are free: to copy, distribute, display, and perform the work to make derivative works to make commercial use of the work Under the following conditions: Attribution - You must give the original author credit. With the understanding that: Waiver - Any of the above conditions can be waived if you get permission from the copyright holder. Public Domain - Where the work or any of its elements is in the public domain under applicable law, that status is in no way affected by the license. Other Rights - In no way are any of the following rights affected by the license: Your fair dealing or fair use rights, or other applicable copyright exceptions and limitations; The author\u2019s moral rights; Rights other persons may have either in the work itself or in how the work is used, such as publicity or privacy rights. Notice - For any reuse or distribution, you must make clear to others the license terms of this work.","title":"License"},{"location":"cli/presentations/","text":"This directory is intended to store presentation files used to provide context to the hands-on aspect of the module.","title":"Home"},{"location":"cli/tools/","text":"The tools used in the module\u2019s tutorial exercises are captured in a tools.yaml file. This file can then be used by a workshop deployment script to download and install the tools required by the module.","title":"Home"},{"location":"de-novo/","text":"btp-module-velvet Bioinformatics Training Platform (BTP) Module: Velvet for de novo assembly Topic de novo genome assembly with Velvet Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes Compile velvet with appropriate compile-time parameters set for a specific analysis Be able to choose appropriate assembly parameters Assemble a set of single-ended reads Assemble a set of paired-end reads from a single insert-size library Be able to visualise an assembly in AMOS Hawkeye Understand the importance of using paired-end libraries in de novo genome assembly Time Required 1 day License The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"btp-module-velvet"},{"location":"de-novo/#btp-module-velvet","text":"Bioinformatics Training Platform (BTP) Module: Velvet for de novo assembly Topic de novo genome assembly with Velvet Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes Compile velvet with appropriate compile-time parameters set for a specific analysis Be able to choose appropriate assembly parameters Assemble a set of single-ended reads Assemble a set of paired-end reads from a single insert-size library Be able to visualise an assembly in AMOS Hawkeye Understand the importance of using paired-end libraries in de novo genome assembly Time Required 1 day","title":"btp-module-velvet"},{"location":"de-novo/#license","text":"The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"License"},{"location":"de-novo/handout/handout/","text":"Key Learning Outcomes After completing this practical the trainee should be able to: Compile velvet with appropriate compile-time parameters set for a specific analysis Be able to choose appropriate assembly parameters Assemble a set of single-ended reads Assemble a set of paired-end reads from a single insert-size library Be able to visualise an assembly in AMOS Hawkeye Understand the importance of using paired-end libraries in de novo genome assembly Resources You\u2019ll be Using Although we have provided you with an environment which contains all the tools and data you will be using in this module, you may like to know where we have sourced those tools and data from. Tools Used Velvet : \\ http://www.ebi.ac.uk/~zerbino/velvet/ AMOS Hawkeye : \\ http://apps.sourceforge.net/mediawiki/amos/index.php?title=Hawkeye gnx-tools : \\ https://github.com/mh11/gnx-tools FastQC : \\ http://www.bioinformatics.bbsrc.ac.uk/projects/fastqc/ R : \\ http://www.r-project.org/ Sources of Data ftp://ftp.ensemblgenomes.org/pub/release-8/bacteria/fasta/Staphylococcus/s_aureus_mrsa252/dna/s_aureus_mrsa252.EB1_s_aureus_mrsa252.dna.chromosome.Chromosome.fa.gz http://www.ebi.ac.uk/ena/data/view/SRS004748 ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022825/SRR022825.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022823/SRR022823.fastq.gz http://www.ebi.ac.uk/ena/data/view/SRX008042 ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022852/SRR022852_1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022852/SRR022852_2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR023/SRR023408/SRR023408_1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR023/SRR023408/SRR023408_2.fastq.gz http://www.ebi.ac.uk/ena/data/view/SRX000181 ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR000/SRR000892/SRR000892.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR000/SRR000893/SRR000893.fastq.gz http://www.ebi.ac.uk/ena/data/view/SRX007709 ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022863/SRR022863_1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022863/SRR022863_2.fastq.gz Introduction The aim of this module is to become familiar with performing de novo genome assembly using Velvet, a de Bruijn graph based assembler, on a variety of sequence data. Prepare the Environment The first exercise should get you a little more comfortable with the computer environment and the command line. First make sure that you are in the denovo working directory by typing: cd /home/trainee/denovo and making absolutely sure you\u2019re there by typing: pwd Now create sub-directories for this and the two other velvet practicals. All these directories will be made as sub-directories of a directory for the whole course called NGS. For this you can use the following commands: mkdir -p NGS/velvet/{part1,part2,part3} The -p tells mkdir (make directory) to make any parent directories if they don\u2019t already exist. You could have created the above directories one-at-a-time by doing this instead: mkdir NGS mkdir NGS/velvet mkdir NGS/velvet/part1 mkdir NGS/velvet/part2 mkdir NGS/velvet/part3 After creating the directories, examine the structure and move into the directory ready for the first velvet exercise by typing: ls -R NGS cd NGS/velvet/part1 pwd Downloading and Compiling Velvet For the duration of this workshop, all the software you require has been set up for you already. This might not be the case when you return to \u201creal life\u201d. Many of the programs you will need, including velvet, are quite easy to set up, it might be instructive to try a couple. Although you will be using the preinstalled version of velvet, it is useful to know how to compile velvet as some of the parameters you might like to control can only be set at compile time. You can find the latest version of velvet at: http://www.ebi.ac.uk/~zerbino/velvet/ You could go to this URL and download the latest velvet version, or equivalently, you could type the following, which will download, unpack, inspect, compile and execute your locally compiled version of velvet: cd /home/trainee/denovo/NGS/velvet/part1 pwd tar xzf /home/trainee/denovo/data/velvet_1.2.10.tgz ls -R cd velvet_1.2.10 make ./velveth The standout displayed to screen when \u2019make\u2019 runs may contain an error message but it is ignored Take a look at the executables you have created. They will be displayed as green by the command: ls --color=always The switch \u2013color , instructs that files be coloured according to their type. This is often the default but we are just being explicit. By specifying the value always , we ensure that colouring is always applied, even from a script. Have a look of the output the command produces and you will see that MAXKMERLENGTH=31 and CATEGORIES=2 parameters were passed into the compiler. This indicates that the default compilation was set for de Bruijn graph k-mers of maximum size 31 and to allow a maximum of just 2 read categories. You can override these, and other, default configuration choices using command line parameters. Assume, you want to run velvet with a k-mer length of 41 using 3 categories, velvet needs to be recompiled to enable this functionality by typing: make clean make MAXKMERLENGTH=41 CATEGORIES=3 ./velveth Discuss with the persons next to you the following questions:\\ What are the consequences of the parameters you have given make for velvet? MAXKMERLENGTH: increase the max k-mer length from 31 to 41 CATEGORIES: paired-end data require to be put into separate categories. By increasing this parameter from 2 to 3 allows you to process 3 paired / mate-pair libraries and unpaired data. Why does Velvet use k-mer 31 and 2 categories as default? Possibly a number of reason:\\ - odd number to avoid palindromes\\ - The first reads were very short (20-40 bp) and there were hardly any paired-end data\\ around so there was no need to allow for longer k-mer lengths / more categories.\\ - For programmers: 31 bp get stored in 64 bits (using 2bit encoding) Should you get better results by using a longer k-mer length? If you can achieve a good k-mer coverage - yes. What effect would the following compile-time parameters have on velvet:\\ OPENMP=Y Turn on multithreading LONGSEQUENCES=Y Assembling reads / contigs longer than 32kb long BIGASSEMBLY=Y Using more than 2.2 billion reads VBIGASSEMBLY=Y Not documented yet SINGLE_COV_CAT=Y Merge all coverage statistics into a single variable - save memory For a further description of velvet compile and runtime parameters please see the velvet Manual: https://github.com/dzerbino/velvet/wiki/Manual","title":"De Novo"},{"location":"de-novo/handout/handout/#key-learning-outcomes","text":"After completing this practical the trainee should be able to: Compile velvet with appropriate compile-time parameters set for a specific analysis Be able to choose appropriate assembly parameters Assemble a set of single-ended reads Assemble a set of paired-end reads from a single insert-size library Be able to visualise an assembly in AMOS Hawkeye Understand the importance of using paired-end libraries in de novo genome assembly","title":"Key Learning Outcomes"},{"location":"de-novo/handout/handout/#resources-youll-be-using","text":"Although we have provided you with an environment which contains all the tools and data you will be using in this module, you may like to know where we have sourced those tools and data from.","title":"Resources You\u2019ll be Using"},{"location":"de-novo/handout/handout/#tools-used","text":"Velvet : \\ http://www.ebi.ac.uk/~zerbino/velvet/ AMOS Hawkeye : \\ http://apps.sourceforge.net/mediawiki/amos/index.php?title=Hawkeye gnx-tools : \\ https://github.com/mh11/gnx-tools FastQC : \\ http://www.bioinformatics.bbsrc.ac.uk/projects/fastqc/ R : \\ http://www.r-project.org/","title":"Tools Used"},{"location":"de-novo/handout/handout/#sources-of-data","text":"ftp://ftp.ensemblgenomes.org/pub/release-8/bacteria/fasta/Staphylococcus/s_aureus_mrsa252/dna/s_aureus_mrsa252.EB1_s_aureus_mrsa252.dna.chromosome.Chromosome.fa.gz http://www.ebi.ac.uk/ena/data/view/SRS004748 ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022825/SRR022825.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022823/SRR022823.fastq.gz http://www.ebi.ac.uk/ena/data/view/SRX008042 ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022852/SRR022852_1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022852/SRR022852_2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR023/SRR023408/SRR023408_1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR023/SRR023408/SRR023408_2.fastq.gz http://www.ebi.ac.uk/ena/data/view/SRX000181 ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR000/SRR000892/SRR000892.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR000/SRR000893/SRR000893.fastq.gz http://www.ebi.ac.uk/ena/data/view/SRX007709 ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022863/SRR022863_1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022863/SRR022863_2.fastq.gz","title":"Sources of Data"},{"location":"de-novo/handout/handout/#introduction","text":"The aim of this module is to become familiar with performing de novo genome assembly using Velvet, a de Bruijn graph based assembler, on a variety of sequence data.","title":"Introduction"},{"location":"de-novo/handout/handout/#prepare-the-environment","text":"The first exercise should get you a little more comfortable with the computer environment and the command line. First make sure that you are in the denovo working directory by typing: cd /home/trainee/denovo and making absolutely sure you\u2019re there by typing: pwd Now create sub-directories for this and the two other velvet practicals. All these directories will be made as sub-directories of a directory for the whole course called NGS. For this you can use the following commands: mkdir -p NGS/velvet/{part1,part2,part3} The -p tells mkdir (make directory) to make any parent directories if they don\u2019t already exist. You could have created the above directories one-at-a-time by doing this instead: mkdir NGS mkdir NGS/velvet mkdir NGS/velvet/part1 mkdir NGS/velvet/part2 mkdir NGS/velvet/part3 After creating the directories, examine the structure and move into the directory ready for the first velvet exercise by typing: ls -R NGS cd NGS/velvet/part1 pwd","title":"Prepare the Environment"},{"location":"de-novo/handout/handout/#downloading-and-compiling-velvet","text":"For the duration of this workshop, all the software you require has been set up for you already. This might not be the case when you return to \u201creal life\u201d. Many of the programs you will need, including velvet, are quite easy to set up, it might be instructive to try a couple. Although you will be using the preinstalled version of velvet, it is useful to know how to compile velvet as some of the parameters you might like to control can only be set at compile time. You can find the latest version of velvet at: http://www.ebi.ac.uk/~zerbino/velvet/ You could go to this URL and download the latest velvet version, or equivalently, you could type the following, which will download, unpack, inspect, compile and execute your locally compiled version of velvet: cd /home/trainee/denovo/NGS/velvet/part1 pwd tar xzf /home/trainee/denovo/data/velvet_1.2.10.tgz ls -R cd velvet_1.2.10 make ./velveth The standout displayed to screen when \u2019make\u2019 runs may contain an error message but it is ignored Take a look at the executables you have created. They will be displayed as green by the command: ls --color=always The switch \u2013color , instructs that files be coloured according to their type. This is often the default but we are just being explicit. By specifying the value always , we ensure that colouring is always applied, even from a script. Have a look of the output the command produces and you will see that MAXKMERLENGTH=31 and CATEGORIES=2 parameters were passed into the compiler. This indicates that the default compilation was set for de Bruijn graph k-mers of maximum size 31 and to allow a maximum of just 2 read categories. You can override these, and other, default configuration choices using command line parameters. Assume, you want to run velvet with a k-mer length of 41 using 3 categories, velvet needs to be recompiled to enable this functionality by typing: make clean make MAXKMERLENGTH=41 CATEGORIES=3 ./velveth Discuss with the persons next to you the following questions:\\ What are the consequences of the parameters you have given make for velvet? MAXKMERLENGTH: increase the max k-mer length from 31 to 41 CATEGORIES: paired-end data require to be put into separate categories. By increasing this parameter from 2 to 3 allows you to process 3 paired / mate-pair libraries and unpaired data. Why does Velvet use k-mer 31 and 2 categories as default? Possibly a number of reason:\\ - odd number to avoid palindromes\\ - The first reads were very short (20-40 bp) and there were hardly any paired-end data\\ around so there was no need to allow for longer k-mer lengths / more categories.\\ - For programmers: 31 bp get stored in 64 bits (using 2bit encoding) Should you get better results by using a longer k-mer length? If you can achieve a good k-mer coverage - yes. What effect would the following compile-time parameters have on velvet:\\ OPENMP=Y Turn on multithreading LONGSEQUENCES=Y Assembling reads / contigs longer than 32kb long BIGASSEMBLY=Y Using more than 2.2 billion reads VBIGASSEMBLY=Y Not documented yet SINGLE_COV_CAT=Y Merge all coverage statistics into a single variable - save memory For a further description of velvet compile and runtime parameters please see the velvet Manual: https://github.com/dzerbino/velvet/wiki/Manual","title":"Downloading and Compiling Velvet"},{"location":"post-workshop/","text":"btp-module-nectar Bioinformatics Training Platform (BTP) Module: Using the NeCTAR Research Cloud Topic How to access the Australian NeCTAR Research Cloud Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes How to access computational resources on the Australian NeCTAR Research Cloud How to instantiate VM\u2019s on the NeCTAR Research Cloud How to setup your own local VirtualBox VM How to connect to a VM using an NX client Time Required 1 hr License The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"btp-module-nectar"},{"location":"post-workshop/#btp-module-nectar","text":"Bioinformatics Training Platform (BTP) Module: Using the NeCTAR Research Cloud Topic How to access the Australian NeCTAR Research Cloud Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes How to access computational resources on the Australian NeCTAR Research Cloud How to instantiate VM\u2019s on the NeCTAR Research Cloud How to setup your own local VirtualBox VM How to connect to a VM using an NX client Time Required 1 hr","title":"btp-module-nectar"},{"location":"post-workshop/#license","text":"The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"License"},{"location":"preamble/preamble/","text":"\\newpage Providing Feedback ================== While we endeavour to deliver a workshop with quality content and documentation in a venue conducive to an exciting, well run hands-on workshop with a bunch of knowledgeable and likable trainers, we know there are things we could do better. Whilst we want to know what didn\u2019t quite hit the mark for you, what would be most helpful and least depressing, would be for you to provide ways to improve the workshop. i.e. constructive feedback. After all, if we knew something wasn\u2019t going to work, we wouldn\u2019t have done it or put it into the workshop in the first place! Remember, we\u2019re experts in the field of bioinformatics not experts in the field of biology! Clearly, we also want to know what we did well! This gives us that \u201cfeel good\u201d factor which will see us through those long days and nights in the lead up to such hands-on workshops! With that in mind, we\u2019ll provide three really high tech mechanism through which you can provide anonymous feedback during the workshop: A sheet of paper, from a flip-chart, sporting a \u201chappy\u201d face and a \u201cnot so happy\u201d face. Armed with a stack of colourful post-it notes, your mission is to see how many comments you can stick on the \u201chappy\u201d side! Some empty ruled pages at the back of this handout. Use them for your own personal notes or for write specific comments/feedback about the workshop as it progresses. An online post-workshop evaluation survey. We\u2019ll ask you to complete this before you leave. If you\u2019ve used the blank pages at the back of this handout to make feedback notes, you\u2019ll be able to provide more specific and helpful feedback with the least amount of brain-drain! Document Structure We have provided you with an electronic copy of the workshop\u2019s hands-on tutorial documents. We have done this for two reasons: 1) you will have something to take away with you at the end of the workshop, and 2) you can save time (mis)typing commands on the command line by using copy-and-paste. We advise you to use Acrobat Reader to view the PDF. This is because it properly supports some features we have implemented to ensure that copy-and-paste of commands works as expected. This includes the appropriate copy-and-paste of special characters like tilde and hyphens as well as skipping line numbers for easy copy-and-past of whole code blocks. While you could fly through the hands-on sessions doing copy-and-paste you will learn more if you take the time, saved from not having to type all those commands, to understand what each command is doing! The commands to enter at a terminal look something like this: tophat --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_2cells genome/ZV9 data/2cells_1.fastq data/2cells_2.fastq The following styled code is not to be entered at a terminal, it is simply to show you the syntax of the command. You must use your own judgement to substitute in the correct arguments, options, filenames etc ``` {style=\u201dcommand_syntax\u201d} tophat [options]* The following is an example how of R commands are styled : `` ` { style = \"R\" } R -- no - save library ( plotrix ) data <- read.table ( \"run_25/stats.txt\" , header = TRUE ) weighted.hist ( data $ short1_cov + data $ short2_cov , data $ lgth , breaks = 0 : 70 ) q () The following icons are used in the margin, throughout the documentation to help you navigate around the document more easily: \\hspace*{.2cm} Important\\ For reference\\ Follow these steps\\ Questions to answer\\ Warning - STOP and read\\ Bonus exercise for fast learners\\ Advanced exercise for super-fast learners\\ Resources Used We have provided you with an environment which contains all the tools and data you need for the duration of this workshop. However, we also provide details about the tools and data used by each module at the start of the respective module documentation.","title":"Workshop Information"},{"location":"preamble/preamble/#document-structure","text":"We have provided you with an electronic copy of the workshop\u2019s hands-on tutorial documents. We have done this for two reasons: 1) you will have something to take away with you at the end of the workshop, and 2) you can save time (mis)typing commands on the command line by using copy-and-paste. We advise you to use Acrobat Reader to view the PDF. This is because it properly supports some features we have implemented to ensure that copy-and-paste of commands works as expected. This includes the appropriate copy-and-paste of special characters like tilde and hyphens as well as skipping line numbers for easy copy-and-past of whole code blocks. While you could fly through the hands-on sessions doing copy-and-paste you will learn more if you take the time, saved from not having to type all those commands, to understand what each command is doing! The commands to enter at a terminal look something like this: tophat --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_2cells genome/ZV9 data/2cells_1.fastq data/2cells_2.fastq The following styled code is not to be entered at a terminal, it is simply to show you the syntax of the command. You must use your own judgement to substitute in the correct arguments, options, filenames etc ``` {style=\u201dcommand_syntax\u201d} tophat [options]* The following is an example how of R commands are styled : `` ` { style = \"R\" } R -- no - save library ( plotrix ) data <- read.table ( \"run_25/stats.txt\" , header = TRUE ) weighted.hist ( data $ short1_cov + data $ short2_cov , data $ lgth , breaks = 0 : 70 ) q () The following icons are used in the margin, throughout the documentation to help you navigate around the document more easily: \\hspace*{.2cm} Important\\ For reference\\ Follow these steps\\ Questions to answer\\ Warning - STOP and read\\ Bonus exercise for fast learners\\ Advanced exercise for super-fast learners\\","title":"Document Structure"},{"location":"preamble/preamble/#resources-used","text":"We have provided you with an environment which contains all the tools and data you need for the duration of this workshop. However, we also provide details about the tools and data used by each module at the start of the respective module documentation.","title":"Resources Used"},{"location":"qc/","text":"NGS Quality Control Bioinformatics Training Platform (BTP) Module: NGS Quality Control Topic NGS Quality Control Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes Assess the overall quality of NGS (FastQ format) sequence reads Visualise the quality, and other associated matrices, of reads to decide on filters and cutoffs for cleaning up data ready for downstream analysis Clean up adaptors and pre-process the sequence data for further analysis Time Required 2 hrs License The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"NGS Quality Control"},{"location":"qc/#ngs-quality-control","text":"Bioinformatics Training Platform (BTP) Module: NGS Quality Control Topic NGS Quality Control Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes Assess the overall quality of NGS (FastQ format) sequence reads Visualise the quality, and other associated matrices, of reads to decide on filters and cutoffs for cleaning up data ready for downstream analysis Clean up adaptors and pre-process the sequence data for further analysis Time Required 2 hrs","title":"NGS Quality Control"},{"location":"qc/#license","text":"The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"License"},{"location":"qc/handout/","text":"NGS Quality Control Acknowledgements This module was developed by Bioplatforms Australia, in partnership with CSIRO, and with support from the European Bioinformatics Institute (EBI), a member of the European Molecular Biology Laboratory (EMBL) in the UK. The module content has been maintained and updated by a network of dedicated [bioinformatics trainers] ( http://www.bioplatforms.com/bioinformatics-training/ ) from around Australia. The Bioinformatics Training Platform was developed in collaboration with the Monash Bioinformatics Platform. The development of this module is also supported by the NSW Office of Health and Medical Research and the Commonwealth Government National Collaborative Research Infrastructure Strategy (NCRIS).","title":"NGS Quality Control"},{"location":"qc/handout/#ngs-quality-control","text":"","title":"NGS Quality Control"},{"location":"qc/handout/#acknowledgements","text":"This module was developed by Bioplatforms Australia, in partnership with CSIRO, and with support from the European Bioinformatics Institute (EBI), a member of the European Molecular Biology Laboratory (EMBL) in the UK. The module content has been maintained and updated by a network of dedicated [bioinformatics trainers] ( http://www.bioplatforms.com/bioinformatics-training/ ) from around Australia. The Bioinformatics Training Platform was developed in collaboration with the Monash Bioinformatics Platform. The development of this module is also supported by the NSW Office of Health and Medical Research and the Commonwealth Government National Collaborative Research Infrastructure Strategy (NCRIS).","title":"Acknowledgements"},{"location":"qc/handout/handout/","text":"Key Learning Outcomes After completing this practical the trainee should be able to: Assess the overall quality of NGS (FastQ format) sequence reads Visualise the quality, and other associated matrices, of reads to decide on filters and cutoffs for cleaning up data ready for downstream analysis Clean up adaptors and pre-process the sequence data for further analysis Resources You\u2019ll be Using Tools Used FastQC: http://www.bioinformatics.babraham.ac.uk/projects/fastqc/ Skewer: http://sourceforge.net/projects/skewer/ FASTX-Toolkit: http://hannonlab.cshl.edu/fastx_toolkit/ Useful Links FASTQ Encoding: ( http://en.wikipedia.org/wiki/FASTQ_format#Encoding ) Author Information Primary Author(s): Sonika Tyagi: sonika.tyagi@monash.edu Contributor(s): Nandan Deshpande: n.deshpande@unsw.edu.au Introduction Going on a blind date with your read set? For a better understanding of the consequences please check the data quality! For the purpose of this tutorial we are focusing only on Illumina sequencing which uses \u2019sequence by synthesis\u2019 technology in a highly parallel fashion. Although Illumina high throughput sequencing provides highly accurate sequence data, several sequence artifacts, including base calling errors and small insertions/deletions, poor quality reads and primer/adapter contamination are quite common in the high throughput sequencing data. The primary errors are substitution errors. The error rates can vary from 0.5-2.0% with errors mainly rising in frequency at the 3\u2019 ends of reads. One way to investigate sequence data quality is to visualize the quality scores and other metrics in a compact manner to get an idea about the quality of a read data set. Read data sets can be improved by pre processing in different ways like trimming off low quality bases, cleaning up any sequencing adapters, removing PCR duplicates and screening for contamination. We can also look at other statistics such as, sequence length distribution, base composition, sequence complexity, presence of ambiguous bases etc. to assess the overall quality of the data set. Highly redundant coverage (>15X) of the genome can be used to correct sequencing errors in the reads before assembly. Various k-mer based error correction methods exist but are beyond the scope of this tutorial. Quality Value Encoding Schema Quality scoring calculates a set of predictors for each base call, and then uses the predictor values to look up the Q-score in a quality table. Quality tables are created to provide optimally accurate quality predictions for runs generated by a specific configuration of sequencing platform and version of chemistry ( www.illumina.com ). In order to use a single character to encode Phred qualities, ASCII characters are used. All ASCII characters have a decimal number associated with them but the first 32 characters are non-printable (e.g. backspace, shift, return, escape). Therefore, the first printable ASCII character is number 33, the exclamation mark ( ! ). In Phred+33 encoded quality values the exclamation mark takes the Phred quality score of zero. Early Solexa (now Illumina) sequencing needed to encode negative quality values. Because ASCII characters < 33 are non-printable, using the Phred+33 encoding was not possible. Therefore, they simply moved the offset from 33 to 64 thus inventing the Phred+64 encoded quality values. In this encoding a Phred quality of zero is denoted by the ASCII number 64 (the @ character). Since Illumina 1.8, quality values are now encoded using Phred+33. FASTQ does not provide a way to describe what quality encoding is used for the quality values. Therefore, you should find this out from your sequencing provider. Alternatively, you may be able to figure this out by determining what ASCII characters are present in the FASTQ file. E.g the presence of numbers in the quality strings, can only mean the quality values are Phred+33 encoded. However, due to the overlapping nature of the Phred+33 and Phred+64 encoding schema it is not always possible to identify what encoding is in use. For example, if the only characters seen in the quality string are ( @ABCDEFGHI ), then it is impossible to know if you have really good Phred+33 encoded qualities or really bad Phred+64 encoded qualities. For a graphical representation of the different ASCII characters used in the two encoding schema see: ( http://en.wikipedia.org/wiki/FASTQ_format#Encoding ). Q-score encoding implemented with the Novaseq platform In order to reduce the data footprints Illumina has come up with a new method to reduce quality score resolution and optimise data storae. The new Q-score encoding now follows an 8 level mapping of individual quality scores (0-40 or >40) [See Table 1 ]. With the new scoring scheme the original scores 20-24 may form one bin and the quality scores in that bin mapped to a new value of 22. This can be thought of as simply replacing all the occurrences of scores 20, 21, 23, 24 with a new score of 22 in the output sequence. Illumina claims that with the new Q-scoring system the reduction in the Illumina raw sequence format (.bcl) is typically > 50% and the resulting sorted BAM les are reduced by ~30%. Quality Score Bins Mapped quality scores N (no call) N (no call) 2-9 6 10-19 15 20-24 22 25-29 27 30-34 33 35-39 37 >=40 40 Table 1: Novaseq Q-score bins mapping Prepare the Environment To investigate sequence data quality we will demonstrate tools called FastQC and Skewer. FastQC will process and present the reports in a visual manner. Based on the results, the sequence data can be processed using the Skewer. We will use one data set in this practical, which can be found in the QC directory on your desktop. Open the Terminal and go to the directory where the data are stored: cd ls cd qc pwd At any time, help can be displayed for FastQC using the following command: fastqc -h Look at SYNOPSIS (Usage) and options after typing fastqc -h Quality Visualisation We have a file for a good quality and bad quality statistics. FastQC generates results in the form of a zipped and unzipped directory for each input file. Execute the following command on the two files: fastqc -f fastq qcdemo_R1.fastq.gz fastqc -f fastq qcdemo_R2.fastq.gz View the FastQC report file of the bad data using a web browser such as firefox. The & sign puts the job in the background. firefox qcdemo_R2_fastqc.html &amp; The report file will have a Basic Statistics table and various graphs and tables for different quality statistics e.g.: Property Value Filename qcdemo_R2.fastq.gz File type Conventional base calls Encoding Sanger / Illumina 1.9 Total Sequences 1000000 Filtered Sequences 0 Sequence length 150 %GC 37 Table 2: Summary statistics for bad_example_untrimmed Figure 1: bad_example_untrimmed_QC_plot A Phred quality score (or Q-score) expresses an error probability. In particular, it serves as a convenient and compact way to communicate very small error probabilities. The probability that base A is wrong (P(A)) is expressed by a quality score, Q(A), according to the relationship: Q(A) =-10 log10(P(A)) The relationship between the quality score and error probability is demonstrated with the following table: Quality score, Q(A) Error probability, P(A) Accuracy of base call 10 0.1 90% 20 0.01 99% 30 0.001 99.9% 40 0.0001 99.99% 50 0.00001 99.999% Table 3: Quality Error Probabilities Question How many sequences were there in your file? What is the read length? Answer 1,000,000. read length=150bp Question Does the quality score values vary throughout the read length? Hint Look at the \u2019per base sequence quality plot\u2019 Answer Yes. Quality scores are dropping towards the end of the reads. Question What is the quality score range you see? Answer 2-40 Question At around which position do the scores start falling below Q20 for the 25% quartile range (25%of reads below Q20)? Answer Around 30 bp position Question How can we trim the reads to filter out the low quality data? Answer By trimming off the bases after a fixed position of the read or by trimming off bases based on the quality score. Good Quality Data View the FastQC report files fastqc_report.html to see examples of a good quality data and compare the quality plot with that of the bad_example_fastqc . firefox qcdemo_R1_fastqc.html &amp; Sequencing errors can complicate the downstream analysis, which normally requires that reads be aligned to each other (for genome assembly) or to a reference genome (for detection of mutations). Sequence reads containing errors may lead to ambiguous paths in the assembly or improper gaps. In variant analysis projects sequence reads are aligned against the reference genome. The errors in the reads may lead to more mismatches than expected from mutations alone. But if these errors can be removed or corrected, the read alignments and hence the variant detection will improve. The assemblies will also improve after pre-processing the reads to remove errors. Read Trimming Read trimming can be done in a variety of different ways. Choose a method which best suits your data. Here we are giving examples of fixed-length trimming and quality-based trimming. Quality Based Trimming Base call quality scores can be used to dynamically determine the trim points for each read. A quality score threshold and minimum read length following trimming can be used to remove low quality data. The previous FastQC results show R1 is fine but R2 has low quality at the end. There is no adaptor contamination though. We will be using Skewer to perform the quality trimming. Run the following command to quality trim a set of paired end data. cd /home/trainee/qc skewer -t 4 -l 50 -q 30 -Q 25 -m pe -o qcdemo qcdemo_R1.fastq.gz qcdemo_R2.fastq.gz -t: number of threads to use -l: min length to keep after trimming -q: Quality threshold used for trimming at 3\u2019 end -Q: mean quality threshold for a read -m: pair-end mode Run FastQC on the quality trimmed file and visualise the quality scores. Look at the last files generated, are the file names same as the input ? ls -ltr Run Fastqc on the quality trimmed files: fastqc -f fastq qcdemo-trimmed-pair1.fastq fastqc -f fastq qcdemo-trimmed-pair2.fastq Visualise the fastqc results: firefox qcdemo-trimmed-pair1_fastqc.html &amp; firefox qcdemo-trimmed-pair2_fastqc.html &amp; Let\u2019s look at the quality from the second reads. The output should look like: Property Value Filename qcdemo-trimmed-pair2.fastq File type Conventional base calls Encoding Sanger / Illumina 1.9 Total Sequences 742262 Filtered Sequences 0 Sequence length 50-150 %GC 37 Table 4: Summary Statistics of QC_demo_R1_trimmed Figure 2: bad_example_quality_trimmed_plot Question Did the number of total reads in R1 and R2 change after trimming? Answer Quality trimming discarded >25000 reads. However, we retain a lot of maximal length reads which have good quality all the way to the ends. Question What reads lengths were obtained after quality based trimming? Answer 50-150 Reads <50 bp, following quality trimming, were discarded. Question Did you observe adapter sequences in the data? Answer No. (Hint: look at the overrepresented sequences) Question How can you use -a option with fastqc? (Hint: try fastqc -h). Answer Adaptors can be supplied in a file for screening. Adapter Clipping Sometimes sequence reads may end up getting the leftover of adapters and primers used in the sequencing process. It\u2019s good practice to screen your data for these possible contamination for more sensitive alignment and assembly based analysis. This is particularly important when read lengths can be longer than the molecules being sequenced. For example when sequencing miRNAs. Various QC tools are available to screen and/or clip these adapter/primer sequences from your data. Apart from skewer which will be using today the following two tools are also useful for trimming and removing adapter sequence: Cutadapt: http://code.google.com/p/cutadapt/ Trimmomatic: http://www.usadellab.org/cms/?page=trimmomatic Here we are demonstrating Skewer to trim a given adapter sequence. cd /home/trainee/qc fastqc -f fastq adaptorQC.fastq.gz firefox adaptorQC_fastqc.html skewer -x TGGAATTCTCGGGTGCCAAGGT -t 20 -l 10 -L 35 -q 30 adaptorQC.fastq.gz -x: adaptor sequence used -t: number of threads to use -l: min length to keep after trimming -L: Max length to keep after trimming, in this experiment we were expecting only small RNA fragments -Q: Quality threshold used for trimming at 3\u2019 end. Use -m option to control the end you want to trim Run FastQC on the adapter trimmed file and visualise the quality scores. Fastqc now shows adaptor free results. fastqc adaptorQC.fastq-trimmed.fastq firefox adaptorQC.fastq-trimmed_fastqc.html &amp; Fixed Length Trimming We will not cover fixed length trimming but provide the following for your information. Low quality read ends can be trimmed using a fixed-length trimming. We will use the fastx_trimmer from the FASTX-Toolkit. Usage message to find out various options you can use with this tool. Type fastx_trimmer -h at anytime to display help. We will now do fixed-length trimming of the bad_example.fastq file using the following command. You should still be in the qc directory, if not cd back in. cd /home/trainee/qc fastqc -f fastq bad_example.fastq fastx_trimmer -h fastx_trimmer -Q 33 -f 1 -l 80 -i bad_example.fastq -o bad_example_trimmed01.fastq We used the following options in the command above: -Q 33: Indicates the input quality scores are Phred+33 encoded -f: First base to be retained in the output -l: Last base to be retained in the output -i: Input FASTQ file name -o: Output file name Run FastQC on the trimmed file and visualise the quality scores of the trimmed file. fastqc -f fastq bad_example_trimmed01.fastq firefox bad_example_trimmed01_fastqc.html &amp; The output should look like: Property Value Filename bad_example_trimmed01.fastq File type Conventional base calls Encoding Sanger / Illumina 1.9 Total Sequences 40000 Filtered Sequences 0 Sequence length 80 %GC 48 Table 5: Summary Statistics of bad_example_trimmed summary Figure 3: bad_example_trimmed_plot Question What values would you use for -f if you wanted to trim off 10 bases at the 5\u2019 end of the reads? Answer -f 11","title":"Quality Control"},{"location":"qc/handout/handout/#key-learning-outcomes","text":"After completing this practical the trainee should be able to: Assess the overall quality of NGS (FastQ format) sequence reads Visualise the quality, and other associated matrices, of reads to decide on filters and cutoffs for cleaning up data ready for downstream analysis Clean up adaptors and pre-process the sequence data for further analysis","title":"Key Learning Outcomes"},{"location":"qc/handout/handout/#resources-youll-be-using","text":"","title":"Resources You\u2019ll be Using"},{"location":"qc/handout/handout/#tools-used","text":"FastQC: http://www.bioinformatics.babraham.ac.uk/projects/fastqc/ Skewer: http://sourceforge.net/projects/skewer/ FASTX-Toolkit: http://hannonlab.cshl.edu/fastx_toolkit/","title":"Tools Used"},{"location":"qc/handout/handout/#useful-links","text":"FASTQ Encoding: ( http://en.wikipedia.org/wiki/FASTQ_format#Encoding )","title":"Useful Links"},{"location":"qc/handout/handout/#author-information","text":"Primary Author(s): Sonika Tyagi: sonika.tyagi@monash.edu Contributor(s): Nandan Deshpande: n.deshpande@unsw.edu.au","title":"Author Information"},{"location":"qc/handout/handout/#introduction","text":"Going on a blind date with your read set? For a better understanding of the consequences please check the data quality! For the purpose of this tutorial we are focusing only on Illumina sequencing which uses \u2019sequence by synthesis\u2019 technology in a highly parallel fashion. Although Illumina high throughput sequencing provides highly accurate sequence data, several sequence artifacts, including base calling errors and small insertions/deletions, poor quality reads and primer/adapter contamination are quite common in the high throughput sequencing data. The primary errors are substitution errors. The error rates can vary from 0.5-2.0% with errors mainly rising in frequency at the 3\u2019 ends of reads. One way to investigate sequence data quality is to visualize the quality scores and other metrics in a compact manner to get an idea about the quality of a read data set. Read data sets can be improved by pre processing in different ways like trimming off low quality bases, cleaning up any sequencing adapters, removing PCR duplicates and screening for contamination. We can also look at other statistics such as, sequence length distribution, base composition, sequence complexity, presence of ambiguous bases etc. to assess the overall quality of the data set. Highly redundant coverage (>15X) of the genome can be used to correct sequencing errors in the reads before assembly. Various k-mer based error correction methods exist but are beyond the scope of this tutorial.","title":"Introduction"},{"location":"qc/handout/handout/#quality-value-encoding-schema","text":"Quality scoring calculates a set of predictors for each base call, and then uses the predictor values to look up the Q-score in a quality table. Quality tables are created to provide optimally accurate quality predictions for runs generated by a specific configuration of sequencing platform and version of chemistry ( www.illumina.com ). In order to use a single character to encode Phred qualities, ASCII characters are used. All ASCII characters have a decimal number associated with them but the first 32 characters are non-printable (e.g. backspace, shift, return, escape). Therefore, the first printable ASCII character is number 33, the exclamation mark ( ! ). In Phred+33 encoded quality values the exclamation mark takes the Phred quality score of zero. Early Solexa (now Illumina) sequencing needed to encode negative quality values. Because ASCII characters < 33 are non-printable, using the Phred+33 encoding was not possible. Therefore, they simply moved the offset from 33 to 64 thus inventing the Phred+64 encoded quality values. In this encoding a Phred quality of zero is denoted by the ASCII number 64 (the @ character). Since Illumina 1.8, quality values are now encoded using Phred+33. FASTQ does not provide a way to describe what quality encoding is used for the quality values. Therefore, you should find this out from your sequencing provider. Alternatively, you may be able to figure this out by determining what ASCII characters are present in the FASTQ file. E.g the presence of numbers in the quality strings, can only mean the quality values are Phred+33 encoded. However, due to the overlapping nature of the Phred+33 and Phred+64 encoding schema it is not always possible to identify what encoding is in use. For example, if the only characters seen in the quality string are ( @ABCDEFGHI ), then it is impossible to know if you have really good Phred+33 encoded qualities or really bad Phred+64 encoded qualities. For a graphical representation of the different ASCII characters used in the two encoding schema see: ( http://en.wikipedia.org/wiki/FASTQ_format#Encoding ).","title":"Quality Value Encoding Schema"},{"location":"qc/handout/handout/#q-score-encoding-implemented-with-the-novaseq-platform","text":"In order to reduce the data footprints Illumina has come up with a new method to reduce quality score resolution and optimise data storae. The new Q-score encoding now follows an 8 level mapping of individual quality scores (0-40 or >40) [See Table 1 ]. With the new scoring scheme the original scores 20-24 may form one bin and the quality scores in that bin mapped to a new value of 22. This can be thought of as simply replacing all the occurrences of scores 20, 21, 23, 24 with a new score of 22 in the output sequence. Illumina claims that with the new Q-scoring system the reduction in the Illumina raw sequence format (.bcl) is typically > 50% and the resulting sorted BAM les are reduced by ~30%. Quality Score Bins Mapped quality scores N (no call) N (no call) 2-9 6 10-19 15 20-24 22 25-29 27 30-34 33 35-39 37 >=40 40 Table 1: Novaseq Q-score bins mapping","title":"Q-score encoding implemented with the Novaseq platform"},{"location":"qc/handout/handout/#prepare-the-environment","text":"To investigate sequence data quality we will demonstrate tools called FastQC and Skewer. FastQC will process and present the reports in a visual manner. Based on the results, the sequence data can be processed using the Skewer. We will use one data set in this practical, which can be found in the QC directory on your desktop. Open the Terminal and go to the directory where the data are stored: cd ls cd qc pwd At any time, help can be displayed for FastQC using the following command: fastqc -h Look at SYNOPSIS (Usage) and options after typing fastqc -h","title":"Prepare the Environment"},{"location":"qc/handout/handout/#quality-visualisation","text":"We have a file for a good quality and bad quality statistics. FastQC generates results in the form of a zipped and unzipped directory for each input file. Execute the following command on the two files: fastqc -f fastq qcdemo_R1.fastq.gz fastqc -f fastq qcdemo_R2.fastq.gz View the FastQC report file of the bad data using a web browser such as firefox. The & sign puts the job in the background. firefox qcdemo_R2_fastqc.html &amp; The report file will have a Basic Statistics table and various graphs and tables for different quality statistics e.g.: Property Value Filename qcdemo_R2.fastq.gz File type Conventional base calls Encoding Sanger / Illumina 1.9 Total Sequences 1000000 Filtered Sequences 0 Sequence length 150 %GC 37 Table 2: Summary statistics for bad_example_untrimmed Figure 1: bad_example_untrimmed_QC_plot A Phred quality score (or Q-score) expresses an error probability. In particular, it serves as a convenient and compact way to communicate very small error probabilities. The probability that base A is wrong (P(A)) is expressed by a quality score, Q(A), according to the relationship: Q(A) =-10 log10(P(A)) The relationship between the quality score and error probability is demonstrated with the following table: Quality score, Q(A) Error probability, P(A) Accuracy of base call 10 0.1 90% 20 0.01 99% 30 0.001 99.9% 40 0.0001 99.99% 50 0.00001 99.999% Table 3: Quality Error Probabilities Question How many sequences were there in your file? What is the read length? Answer 1,000,000. read length=150bp Question Does the quality score values vary throughout the read length? Hint Look at the \u2019per base sequence quality plot\u2019 Answer Yes. Quality scores are dropping towards the end of the reads. Question What is the quality score range you see? Answer 2-40 Question At around which position do the scores start falling below Q20 for the 25% quartile range (25%of reads below Q20)? Answer Around 30 bp position Question How can we trim the reads to filter out the low quality data? Answer By trimming off the bases after a fixed position of the read or by trimming off bases based on the quality score.","title":"Quality Visualisation"},{"location":"qc/handout/handout/#good-quality-data","text":"View the FastQC report files fastqc_report.html to see examples of a good quality data and compare the quality plot with that of the bad_example_fastqc . firefox qcdemo_R1_fastqc.html &amp; Sequencing errors can complicate the downstream analysis, which normally requires that reads be aligned to each other (for genome assembly) or to a reference genome (for detection of mutations). Sequence reads containing errors may lead to ambiguous paths in the assembly or improper gaps. In variant analysis projects sequence reads are aligned against the reference genome. The errors in the reads may lead to more mismatches than expected from mutations alone. But if these errors can be removed or corrected, the read alignments and hence the variant detection will improve. The assemblies will also improve after pre-processing the reads to remove errors.","title":"Good Quality Data"},{"location":"qc/handout/handout/#read-trimming","text":"Read trimming can be done in a variety of different ways. Choose a method which best suits your data. Here we are giving examples of fixed-length trimming and quality-based trimming.","title":"Read Trimming"},{"location":"qc/handout/handout/#quality-based-trimming","text":"Base call quality scores can be used to dynamically determine the trim points for each read. A quality score threshold and minimum read length following trimming can be used to remove low quality data. The previous FastQC results show R1 is fine but R2 has low quality at the end. There is no adaptor contamination though. We will be using Skewer to perform the quality trimming. Run the following command to quality trim a set of paired end data. cd /home/trainee/qc skewer -t 4 -l 50 -q 30 -Q 25 -m pe -o qcdemo qcdemo_R1.fastq.gz qcdemo_R2.fastq.gz -t: number of threads to use -l: min length to keep after trimming -q: Quality threshold used for trimming at 3\u2019 end -Q: mean quality threshold for a read -m: pair-end mode Run FastQC on the quality trimmed file and visualise the quality scores. Look at the last files generated, are the file names same as the input ? ls -ltr Run Fastqc on the quality trimmed files: fastqc -f fastq qcdemo-trimmed-pair1.fastq fastqc -f fastq qcdemo-trimmed-pair2.fastq Visualise the fastqc results: firefox qcdemo-trimmed-pair1_fastqc.html &amp; firefox qcdemo-trimmed-pair2_fastqc.html &amp; Let\u2019s look at the quality from the second reads. The output should look like: Property Value Filename qcdemo-trimmed-pair2.fastq File type Conventional base calls Encoding Sanger / Illumina 1.9 Total Sequences 742262 Filtered Sequences 0 Sequence length 50-150 %GC 37 Table 4: Summary Statistics of QC_demo_R1_trimmed Figure 2: bad_example_quality_trimmed_plot Question Did the number of total reads in R1 and R2 change after trimming? Answer Quality trimming discarded >25000 reads. However, we retain a lot of maximal length reads which have good quality all the way to the ends. Question What reads lengths were obtained after quality based trimming? Answer 50-150 Reads <50 bp, following quality trimming, were discarded. Question Did you observe adapter sequences in the data? Answer No. (Hint: look at the overrepresented sequences) Question How can you use -a option with fastqc? (Hint: try fastqc -h). Answer Adaptors can be supplied in a file for screening.","title":"Quality Based Trimming"},{"location":"qc/handout/handout/#adapter-clipping","text":"Sometimes sequence reads may end up getting the leftover of adapters and primers used in the sequencing process. It\u2019s good practice to screen your data for these possible contamination for more sensitive alignment and assembly based analysis. This is particularly important when read lengths can be longer than the molecules being sequenced. For example when sequencing miRNAs. Various QC tools are available to screen and/or clip these adapter/primer sequences from your data. Apart from skewer which will be using today the following two tools are also useful for trimming and removing adapter sequence: Cutadapt: http://code.google.com/p/cutadapt/ Trimmomatic: http://www.usadellab.org/cms/?page=trimmomatic Here we are demonstrating Skewer to trim a given adapter sequence. cd /home/trainee/qc fastqc -f fastq adaptorQC.fastq.gz firefox adaptorQC_fastqc.html skewer -x TGGAATTCTCGGGTGCCAAGGT -t 20 -l 10 -L 35 -q 30 adaptorQC.fastq.gz -x: adaptor sequence used -t: number of threads to use -l: min length to keep after trimming -L: Max length to keep after trimming, in this experiment we were expecting only small RNA fragments -Q: Quality threshold used for trimming at 3\u2019 end. Use -m option to control the end you want to trim Run FastQC on the adapter trimmed file and visualise the quality scores. Fastqc now shows adaptor free results. fastqc adaptorQC.fastq-trimmed.fastq firefox adaptorQC.fastq-trimmed_fastqc.html &amp;","title":"Adapter Clipping"},{"location":"qc/handout/handout/#fixed-length-trimming","text":"We will not cover fixed length trimming but provide the following for your information. Low quality read ends can be trimmed using a fixed-length trimming. We will use the fastx_trimmer from the FASTX-Toolkit. Usage message to find out various options you can use with this tool. Type fastx_trimmer -h at anytime to display help. We will now do fixed-length trimming of the bad_example.fastq file using the following command. You should still be in the qc directory, if not cd back in. cd /home/trainee/qc fastqc -f fastq bad_example.fastq fastx_trimmer -h fastx_trimmer -Q 33 -f 1 -l 80 -i bad_example.fastq -o bad_example_trimmed01.fastq We used the following options in the command above: -Q 33: Indicates the input quality scores are Phred+33 encoded -f: First base to be retained in the output -l: Last base to be retained in the output -i: Input FASTQ file name -o: Output file name Run FastQC on the trimmed file and visualise the quality scores of the trimmed file. fastqc -f fastq bad_example_trimmed01.fastq firefox bad_example_trimmed01_fastqc.html &amp; The output should look like: Property Value Filename bad_example_trimmed01.fastq File type Conventional base calls Encoding Sanger / Illumina 1.9 Total Sequences 40000 Filtered Sequences 0 Sequence length 80 %GC 48 Table 5: Summary Statistics of bad_example_trimmed summary Figure 3: bad_example_trimmed_plot Question What values would you use for -f if you wanted to trim off 10 bases at the 5\u2019 end of the reads? Answer -f 11","title":"Fixed Length Trimming"},{"location":"qc/handout/license/","text":"This work is licensed under a Creative Commons Attribution 3.0 Unported License and the below text is a summary of the main terms of the full Legal Code (the full license) available at http://creativecommons.org/licenses/by/3.0/legalcode . You are free: to copy, distribute, display, and perform the work to make derivative works to make commercial use of the work Under the following conditions: Attribution - You must give the original author credit. With the understanding that: Waiver - Any of the above conditions can be waived if you get permission from the copyright holder. Public Domain - Where the work or any of its elements is in the public domain under applicable law, that status is in no way affected by the license. Other Rights - In no way are any of the following rights affected by the license: Your fair dealing or fair use rights, or other applicable copyright exceptions and limitations; The author\u2019s moral rights; Rights other persons may have either in the work itself or in how the work is used, such as publicity or privacy rights. Notice - For any reuse or distribution, you must make clear to others the license terms of this work.","title":"License"},{"location":"rna-seq/","text":"This repository is a template for developing your own Bioinformatics Training Platform (BTP) workshop module. Therefore, if you haven\u2019t already, you should head over to the btp-workshop-template repository and follow one of the workflows documented there. Table of Contents Overview General Design/Layout Overview The README.md files found in each subdirectory of this repository provide documentation on the structure, function, and usage of the btp-module-template in the context of the BTP framework. General Design/Layout These types of workshop modules are designed to be self contained. That is they contain all the required information to permit trainers to reuse and repurpose them quickly and easily. They contain the following 4 major elements: Handout - This often includes background information, step-by-step exercises, questions and answers as well as bonus exercises for those who progress rapidly. Data sets - This describes where to obtain the data sets, used in the handout exercises, and where on the computer system they should be located. Tools - This describes which tools are used in the handout exercises and how to install them. Presentations - This is where you will find presentations for introducing concepts that are explored in the handout exercises. ======= btp-module-rna-seq Bioinformatics Training Platform (BTP) Module: RNA-Seq Topic RNA-Seq Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes Understand and perform a simple RNA-Seq analysis workflow. Perform spliced alignments to an indexed reference genome using TopHat. Perform transcript assembly using Cufflinks. Visualize transcript alignments and annotation in a genome browser such as IGV. Be able to identify differential gene expression between two experimental conditions. Be familiar with R environment and be able to run R based RNA-seq packages. Time Required 6 hrs License The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"Home"},{"location":"rna-seq/#table-of-contents","text":"Overview General Design/Layout","title":"Table of Contents"},{"location":"rna-seq/#overview","text":"The README.md files found in each subdirectory of this repository provide documentation on the structure, function, and usage of the btp-module-template in the context of the BTP framework.","title":"Overview"},{"location":"rna-seq/#general-designlayout","text":"These types of workshop modules are designed to be self contained. That is they contain all the required information to permit trainers to reuse and repurpose them quickly and easily. They contain the following 4 major elements: Handout - This often includes background information, step-by-step exercises, questions and answers as well as bonus exercises for those who progress rapidly. Data sets - This describes where to obtain the data sets, used in the handout exercises, and where on the computer system they should be located. Tools - This describes which tools are used in the handout exercises and how to install them. Presentations - This is where you will find presentations for introducing concepts that are explored in the handout exercises. =======","title":"General Design/Layout"},{"location":"rna-seq/#btp-module-rna-seq","text":"Bioinformatics Training Platform (BTP) Module: RNA-Seq Topic RNA-Seq Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes Understand and perform a simple RNA-Seq analysis workflow. Perform spliced alignments to an indexed reference genome using TopHat. Perform transcript assembly using Cufflinks. Visualize transcript alignments and annotation in a genome browser such as IGV. Be able to identify differential gene expression between two experimental conditions. Be familiar with R environment and be able to run R based RNA-seq packages. Time Required 6 hrs","title":"btp-module-rna-seq"},{"location":"rna-seq/#license","text":"The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"License"},{"location":"rna-seq/datasets/","text":"Information about what datasets are required for a module\u2019s tutorial exercises are described in a data.yaml file. As well describing where to source the dataset files from, we also need to describe where on the filesystem the files should reside and who should own them.","title":"Home"},{"location":"rna-seq/handout/handout/","text":"Key Learning Outcomes After completing this practical the trainee should be able to: Understand and perform a simple RNA-Seq analysis workflow. Perform spliced alignments to an indexed reference genome using TopHat. Visualize spliced transcript alignments in a genome browser such as IGV. Be able to identify differential gene expression between two experimental conditions. Be familiar with R environment and be able to run R based RNA-seq packages. We also have bonus exercises where you can learn to: Perform transcript assembly using Cufflinks. Run cuffdiff, a Cufflinks utility for differential expression analysis. Visualize transcript alignments and annotation in a genome browser such as IGV. Resources You\u2019ll be Using Tools Used Tophat : \\ https://ccb.jhu.edu/software/tophat/index.shtml Cufflinks : \\ http://cole-trapnell-lab.github.io/cufflinks/ Samtools : \\ http://www.htslib.org/ BEDTools : \\ https://github.com/arq5x/bedtools2 UCSC tools : \\ http://hgdownload.cse.ucsc.edu/admin/exe/ IGV : \\ http://www.broadinstitute.org/igv/ FeatureCount : \\ http://subread.sourceforge.net/ edgeR pakcage : \\ https://bioconductor.org/packages/release/bioc/html/edgeR.html CummeRbund manual : \\ http://www.bioconductor.org/packages/release/bioc/vignettes/cummeRbund/inst/doc/cummeRbund-manual.pdf Sources of Data http://www.ebi.ac.uk/ena/data/view/ERR022484 \\ http://www.ebi.ac.uk/ena/data/view/ERR022485 \\ http://www.pnas.org/content/suppl/2008/12/16/0807121105.DCSupplemental Introduction The goal of this hands-on session is to perform some basic tasks in the downstream analysis of RNA-seq data.\\ First we will use RNA-seq data from zebrafish. You will align one set of reads to the zebrafish using Tophat2. You will then view the aligned reads using the IGV viewer. We will also demonstrate how gene counts can be derived from this data. You will go on to assembly a transcriptome from the read data using cufflinks. We will show you how this type of data may be analysed for differential expression. The second part of the tutorial will focus on RNA-seq data from a human experiment (cancer cell line versus normal cells). You will use the Bioconductor packages edgeR and voom (limma) to determine differential gene expression. The results from this analysis will then be used in the final session which introduces you to some of the tools used to gain biological insight into the results of a differential expression analysis Prepare the Environment We will use a dataset derived from sequencing of mRNA from Danio rerio embryos in two different developmental stages. Sequencing was performed on the Illumina platform and generated 76bp paired-end sequence data using polyA selected RNA. Due to the time constraints of the practical we will only use a subset of the reads. The data files are contained in the subdirectory called data and are the following: 2cells_1.fastq and 2cells_2.fastq : \\ These files are based on RNA-seq data of a 2-cell zebrafish embryo 6h_1.fastq and 6h_2.fastq : \\ These files are based on RNA-seq data of zebrafish embryos 6h post fertilization Open the Terminal and go to the rnaseq working directory: cd /home/trainee/rnaseq/ All commands entered into the terminal for this tutorial should be from within the /home/trainee/rnaseq directory. Check that the data directory contains the above-mentioned files by typing: ls data Alignment There are numerous tools for performing short read alignment and the choice of aligner should be carefully made according to the analysis goals/requirements. Here we will use Tophat2, a widely used ultrafast aligner that performs spliced alignments. Tophat2 is based on the Bowtie2 aligner and uses an indexed genome for the alignment to speed up the alignment and keep its memory footprint small. The the index for the Danio rerio genome has been created for you. The command to create an index is as follows. You DO NOT need to run this command yourself - we have done this for you. bowtie2-build genome/Danio_rerio.Zv9.66.dna.fa genome/ZV9 Tophat2 has a number of parameters in order to perform the alignment. To view them all type: tophat2 --help The general format of the tophat2 command is: tophat2 [options]* &lt;index_base&gt; &lt;reads_1&gt; &lt;reads_2&gt; Where the last two arguments are the .fastq files of the paired end reads, and the argument before is the basename of the indexed genome. The quality values in the FASTQ files used in this hands-on session are Phred+33 encoded. We explicitly tell tophat of this fact by using the command line argument \u2013solexa-quals . You can look at the first few reads in the file data/2cells_1.fastq with: head -n 20 data/2cells_1.fastq Some other parameters that we are going to use to run Tophat are listed below: -g : Maximum number of multihits allowed. Short reads are likely to map to more than one location in the genome even though these reads can have originated from only one of these regions. In RNA-seq we allow for a limited number of multihits, and in this case we ask Tophat to report only reads that map at most onto 2 different loci. \u2013library-type : Before performing any type of RNA-seq analysis you need to know a few things about the library preparation. Was it done using a strand-specific protocol or not? If yes, which strand? In our data the protocol was NOT strand specific. -j : Improve spliced alignment by providing Tophat with annotated splice junctions. Pre-existing genome annotation is an advantage when analysing RNA-seq data. This file contains the coordinates of annotated splice junctions from Ensembl. These are stored under the sub-directory annotation in a file called ZV9.spliceSites . -o : This specifies in which subdirectory Tophat should save the output files. Given that for every run the name of the output files is the same, we specify different directories for each run. It takes some time (approx. 20 min) to perform tophat spliced alignments, even for this subset of reads. Therefore, we have pre-aligned the 2cells data for you using the following command: You DO NOT need to run this command yourself - we have done this for you. tophat2 --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_2cells genome/ZV9 data/2cells_1.fastq data/2cells_2.fastq Align the 6h data yourself using the following command: # Takes approx. 20mins tophat2 --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_6h genome/ZV9 data/6h_1.fastq data/6h_2.fastq The 6h read alignment will take approx. 20 min to complete. Therefore, we\u2019ll take a look at some of the files, generated by tophat, for the pre-computed 2cells data. Tophat generates several files in the specified output directory. The most important files are listed below. accepted_hits.bam : This file contains the list of read alignments in BAM format. align_summary.txt : Provides a detailed summary of the read-alignments. unmapped.bam : This file contains the unmapped reads. The complete documentation can be found at: https://ccb.jhu.edu/software/tophat/manual.shtml Alignment Visualisation in IGV The Integrative Genomics Viewer (IGV) is able to provide a visualisation of read alignments given a reference sequence and a BAM file. We\u2019ll visualise the information contained in the accepted_hits.bam and junctions.bed files for the pre-computed 2cells data. The former, contains the tophat spliced alignments of the reads to the reference while the latter stores the coordinates of the splice junctions present in the data set. Open the rnaseq directory on your Desktop and double-click the tophat subdirectory and then the ZV9_2cells directory. Launch IGV by double-clicking the \u201cIGV 2.3.*\u201d icon on the Desktop (ignore any warnings that you may get as it opens). NOTE: IGV may take several minutes to load for the first time, please be patient. Choose \u201cZebrafish (Zv9)\u201d from the drop-down box in the top left of the IGV window. Else you can also load the genome fasta file. Load the accepted_hits.sorted.bam file by clicking the \u201cFile\u201d menu, selecting \u201cLoad from File\u201d and navigating to the Desktop/rnaseq/tophat/ZV9_2cells directory. Rename the track by right-clicking on its name and choosing \u201cRename Track\u201d. Give it a meaningful name like \u201c2cells BAM\u201d. Load the junctions.bed from the same directory and rename the track \u201c2cells Junctions BED\u201d. Load the Ensembl annotations file Danio_rerio.Zv9.66.gtf stored in the rnaseq/annotation directory. Navigate to a region on chromosome 12 by typing chr12 : 20 , 270 , 921 - 20 , 300 , 943 into the search box at the top of the IGV window. Keep zooming to view the bam file alignments Some useful IGV manuals can be found below http://www.broadinstitute.org/software/igv/interpreting_insert_size \\ http://www.broadinstitute.org/software/igv/alignmentdata Does the file \u2019align_summary.txt\u2019 look interesting? What information does it provide? As the name suggests, the file provides a details summary of the alignment statistics. One other important file is \u2019unmapped.bam\u2019. This file contains the unampped reads. Can you identify the splice junctions from the BAM file? Splice junctions can be identified in the alignment BAM files. These are the aligned RNA-Seq reads that have skipped-bases from the reference genome (most likely introns). Are the junctions annotated for CBY1 consistent with the annotation? Read alignment supports an extended length in exon 5 to the gene model (cby1-001) Once tophat finishes aligning the 6h data you will need to sort the alignments found in the BAM file and then index the sorted BAM file. samtools sort tophat/ZV9_6h/accepted_hits.bam tophat/ZV9_6h/accepted_hits.sorted samtools index tophat/ZV9_6h/accepted_hits.sorted.bam Load the sorted BAM file into IGV, as described previously, and rename the track appropriately. Generating Gene Counts In RNAseq experiments the digital gene expression is recorded as the gene counts or number of reads aligning to a known gene feature. If you have a well annotated genome, you can use the gene structure file in a standard gene annotation format (GTF or GFF)) along with the spliced alignment file to quantify the known genes. We will demonstrate a utility called FeatureCounts that comes with the Subread package. mkdir gene_counts featureCounts -a annotation/Danio_rerio.Zv9.66.gtf -t exon -g gene_id -o gene_counts/gene_counts.txt tophat/ZV9_6h/accepted_hits.sorted.bam tophat/ZV9_2cells/accepted_hits.sorted.bam Isoform Expression and Transcriptome Assembly For non-model organisms and genomes with draft assemblies and incomplete annotations, it is a common practice to take and assembly based approach to generate gene structures followed by the quantification step. There are a number of reference based transcript assemblers available that can be used for this purpose such as, cufflinks, stringy. These assemblers can give gene or isoform level assemblies that can be used to perform a gene/isoform level quantification. These assemblers require an alignment of reads with a reference genome or transcriptome as an input. The second optional input is a known gene structure in GTF or GFF format. There are a number of tools that perform reconstruction of the transcriptome and for this workshop we are going to use Cufflinks. Cufflinks can do transcriptome assembly either ab initio or using a reference annotation. It also quantifies the isoform expression in Fragments Per Kilobase of exon per Million fragments mapped (FPKM). Cufflinks has a number of parameters in order to perform transcriptome assembly and quantification. To view them all type: cufflinks --help We aim to reconstruct the transcriptome for both samples by using the Ensembl annotation both strictly and as a guide. In the first case Cufflinks will only report isoforms that are included in the annotation, while in the latter case it will report novel isoforms as well. The Ensembl annotation for Danio rerio is available in annotation/Danio_rerio.Zv9.66.gtf . The general format of the cufflinks command is: cufflinks [options]* &lt;aligned_reads.(sam|bam)&gt; Where the input is the aligned reads (either in SAM or BAM format). Some of the available parameters for Cufflinks that we are going to use to run Cufflinks are listed below: -o : Output directory. -G : Tells Cufflinks to use the supplied GTF annotations strictly in order to estimate isoform annotation. -b : Instructs Cufflinks to run a bias detection and correction algorithm which can significantly improve accuracy of transcript abundance estimates. To do this Cufflinks requires a multi-fasta file with the genomic sequences against which we have aligned the reads. -u : Tells Cufflinks to do an initial estimation procedure to more accurately weight reads mapping to multiple locations in the genome (multi-hits). \u2013library-type : Before performing any type of RNA-seq analysis you need to know a few things about the library preparation. Was it done using a strand-specific protocol or not? If yes, which strand? In our data the protocol was NOT strand specific. Perform transcriptome assembly, strictly using the supplied GTF annotations, for the 2cells and 6h data using cufflinks: # 2cells data (takes approx. 5mins): cufflinks -o cufflinks/ZV9_2cells_gtf -G annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_2cells/accepted_hits.bam # 6h data (takes approx. 5mins): cufflinks -o cufflinks/ZV9_6h_gtf -G annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_6h/accepted_hits.bam Cufflinks generates several files in the specified output directory. Here\u2019s a short description of these files: genes.fpkm_tracking : Contains the estimated gene-level expression values. isoforms.fpkm_tracking : Contains the estimated isoform-level expression values. skipped.gtf : Contains loci skipped as a result of exceeding the maximum number of fragments. transcripts.gtf : This GTF file contains Cufflinks\u2019 assembled isoforms. The complete documentation can be found at: http://cole-trapnell-lab.github.io/cufflinks/file_formats/#output-formats-used-in-the-cufflinks-suite So far we have forced cufflinks, by using the -G option, to strictly use the GTF annotations provided and thus novel transcripts will not be reported. We can get cufflinks to perform a GTF-guided transcriptome assembly by using the -g option instead. Thus, novel transcripts will be reported. GTF-guided transcriptome assembly is more computationally intensive than strictly using the GTF annotations. Therefore, we have pre-computed these GTF-guided assemblies for you and have placed the results under subdirectories: cufflinks/ZV9_2cells_gtf_guided and cufflinks/ZV9_6h_gft_guided . You DO NOT need to run these commands. We provide them so you know how we generated the the GTF-guided transcriptome assemblies: # 2cells guided transcriptome assembly (takes approx. 30mins): cufflinks -o cufflinks/ZV9_2cells_gtf_guided -g annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_2cells/accepted_hits.bam # 6h guided transcriptome assembly (takes approx. 30mins): cufflinks -o cufflinks/ZV9_6h_gtf_guided -g annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_6h/accepted_hits.bam Go back to IGV and load the pre-computed, GTF-guided transcriptome assembly for the 2cells data ( cufflinks/ZV9_2cells_gtf_guided/transcripts.gtf ). Rename the track as \u201c2cells GTF-Guided Transcripts\u201d. In the search box type ENSDART00000082297 in order for the browser to zoom in to the gene of interest. Do you observe any difference between the Ensembl GTF annotations and the GTF-guided transcripts assembled by cufflinks (the \u201c2cells GTF-Guided Transcripts\u201d track)? Yes. It appears that the Ensembl annotations may have truncated the last exon. However, our data also doesn\u2019t contain reads that span between the last two exons. Differential Gene Expression Analysis using edgeR Experiment Design The example we are working through today follows a case Study set out in the edgeR Users Guide (4.3 Androgen-treated prostate cancer cells (RNA-Seq, two groups) which is based on an experiment conducted by Li et al. (2008, Proc Natl Acad Sci USA, 105, 20179-84). The researches used a prostate cancer cell line (LNCaP cells). These cells are sensitive to stimulation by male hormones (androgens). Three replicate RNA samples were collected from LNCaP cells treated with an androgen hormone (DHT). Four replicates were collected from cells treated with an inactive compound. Each of the seven samples was run on a lane (7 lanes) of an Illumina flow cell to produce 35 bp reads. The experimental design was therefore: [H] rrr Lane & Treatment & Label \\ 1 & Control & Con1\\ 2 & Control & Con2\\ 3 & Control & Con3\\ 4 & Control & Con4\\ 5 & DHT & DHT1\\ 6 & DHT & DHT2\\ 7 & DHT & DHT3\\ [tab:experimental d esign] This workflow requires raw gene count files and these can be generated using a utility called featureCounts as demonstrated above. We are using a pre-computed gene counts data (stored in pnas_expression.txt ) for this exercise. Prepare the Environment Prepare the environment and load R: cd /home/trainee/rnaseq/edgeR R (press enter) Once on the R prompt. Load libraries: library(edgeR) library(biomaRt) library(gplots) library(\"limma\") library(\"RColorBrewer\") library(\"org.Hs.eg.db\") Read in Data Read in count table and experimental design: data &lt;- read.delim(\"pnas_expression.txt\", row.names=1, header=T) targets &lt;- read.delim(\"Targets.txt\", header=T) colnames(data) &lt;-targets$Label head(data, n=20) Add Gene annotation The data set only includes the Ensembl gene id and the counts. It is useful to have other annotations such as the gene symbol and entrez id. Next we will add in these annotations. We will use the BiomaRt package to do this. We start by using the useMart function of BiomaRt to access the human data base of ensemble gene ids. human&lt;-useMart(host=\"www.ensembl.org\", \"ENSEMBL_MART_ENSEMBL\", dataset=\"hsapiens_gene_ensembl\") attributes=c(\"ensembl_gene_id\", \"entrezgene\",\"hgnc_symbol\") We create a vector of our ensemble gene ids. ensembl_names&lt;-rownames(data) head(ensembl_names) We then use the function getBM to get the gene symbol data we want.This takes about a minute. genemap&lt;-getBM(attributes, filters=\"ensembl_gene_id\", values=ensembl_names, mart=human) Have a look at the start of the genemap dataframe. head(genemap) We then match the data we have retrieved to our dataset. idx &lt;-match(ensembl_names, genemap$ensembl_gene_id) data$entrezgene &lt;-genemap$entrezgene [ idx ] data$hgnc_symbol &lt;-genemap$hgnc_symbol [ idx ] Ann &lt;- cbind(rownames(data), data$hgnc_symbol, data$entrezgene) colnames(Ann)&lt;-c(\"Ensembl\", \"Symbol\", \"Entrez\") Ann&lt;-as.data.frame(Ann) Let\u2019s check and see that this additional information is there. head(data) Data checks Create DGEList object: treatment &lt;-factor(c(rep(\"Control\",4), rep(\"DHT\",3)), levels=c(\"Control\", \"DHT\")) y &lt;-DGEList(counts=data[,1:7], group=treatment, genes=Ann) Check the dimensions of the object: dim(y) We see we have 37435 rows (i.e. genes) and 7 columns (samples). Now we will filter out genes with low counts by only keeping those rows where the count per million (cpm) is at least 1 in at least three samples: keep &lt;-rowSums( cpm(y)&gt;1) &gt;=3 y &lt;- y[keep, ] How many rows (genes) are retained now dim(y) would give you 16494 How many genes were filtered out? do 37435-16494. As we have removed the lowly expressed genes the total number of counts per sample has not changed greatly. Let us check the total number of reads per sample in the original data (data) and now after filtering. Before: colSums(data[,1:7]) After filtering: colSums(y$counts) We will now perform normalization to take account of different library size: y&lt;-calcNormFactors(y) We will check the calculated normalization factors: y$samples Lets have a look at whether the samples cluster by condition. (You should produce a plot as shown in Figure 4): plotMDS(y, col=as.numeric(y$samples$group)) [H] [fig:MDS plot] Does the MDS plot indicate a difference in gene expression between the Controls and the DHT treated samples? The MDS plot shows us that the controls are separated from the DHT treated cells. This indicates that there is a difference in gene expression between the conditions. We will now estimate the dispersion. We start by estimating the common dispersion. The common dispersion estimates the overall Biological Coefficient of Variation (BCV) of the dataset averaged over all genes. By using verbose we get the Disp and BCV values printed on the screen y &lt;- estimateCommonDisp(y, verbose=T) What value to you see for BCV? We now estimate gene-specific dispersion. y &lt;- estimateTagwiseDisp(y) We will plot the tagwise dispersion and the common dispersion (You should obtain a plot as shown in the Figure 5): plotBCV(y) [H] [fig:BCV plot] We see here that the common dispersion estimates the overall Biological Coefficient of Variation (BCV) of the dataset averaged over all genes. The common dispersion is 0.02 and the BCV is the square root of the common dispersion (sqrt[0.02] = 0.14). A BCV of 14% is typical for cell line experiment. As you can see from the plot the BCV of some genes (generally those with low expression) can be much higher than the common dispersion. For example we see genes with a reasonable level of expression with tagwise dispersion of 0.4 indicating 40% variation between samples. If we used the common dispersion for these genes instead of the tagwise dispersion what effect would this have? If we simply used the common dispersion for these genes we would underestimate biological variability, which in turn affects whether these genes would be identified as being differentially expressed between conditions.It is recommended to use the tagwise dispersion, which takes account of gene-to-gene variability. Now that we have normalized our data and also calculated the variability of gene expression between samples we are in a position to perform differential expression testing.As this is a simple comparison between two conditions, androgen treatment and placebo treatment we can use the exact test for the negative binomial distribution (Robinson and Smyth, 2008). Testing for Differential Expression We now test for differentially expressed BCV genes: et &lt;- exactTest(y) Now we will use the topTags function to adjust for multiple testing. We will use the Benjimini Hochberg (\u201cBH\u201d) method and we will produce a table of results: res &lt;- topTags(et, n=nrow(y$counts), adjust.method=\"BH\")$table Let\u2019s have a look at the first rows of the table: head(res) To get a summary of the number of differentially expressed genes we can use the decideTestsDGE function. summary(de &lt;- decideTestsDGE(et)) This tells us that 2096 genes are downregulated and 2339 genes are upregulated at 5% FDR.We will now make subsets of the most significant upregulated and downregulated genes. alpha=0.05 lfc=1.5 edgeR_res_sig&lt;-res[res$FDR&lt;alpha,] edgeR_res_sig_lfc &lt;-edgeR_res_sig[abs(edgeR_res_sig$logFC) &gt;= lfc,]head(edgeR_res_sig, n=20)nrow(edgeR_res_sig)nrow(edgeR_res_sig_lfc) We can write out these results to our current directory. write.table(edgeR_res_sig , \"edgeR_res_sig.txt\", sep=\"\\t\", col.names=NA, quote=F) write.table(edgeR_res_sig_lfc , \"edgeR_res_sig_lfc.txt\", sep=\"\\t\", col.names=NA, quote=F) How many differentially expressed genes are there? 4435 How many upregulated genes and downregulated genes do we have? 2339 2096 Differential expression using the Voom function and the limma package We will now show an alternative approach to differential expression which uses the limma package.This is based on linear models. The first step is to create a design matrix. In this case we have a simple design where we have only one condition (treated vs non-treated). However, you may be dealing with more complex experimental designs, for example looking at treatment and other covariates, such as age, gender, batch. design &lt;-model.matrix(~treatment) check design print(design) We now use voom to transform the data into a form which is appropriate for linear modelling. v &lt;-voom(y, design) Next we will fit linear model to each gene in the dataset using the function lmFit. Following this we use the function eBayes to test each gene to find whether foldchange between the conditions being tested is statistically significant.We filter our results by using the same values of alpha (0.05) and log fold change (1.5) used previously. fit_v &lt;-lmFit(v, design) fit_v &lt;- eBayes(fit_v) voom_res&lt;-topTable(fit_v, coef=2,adjust.method=\"BH\", sort.by=\"P\", n=nrow(y$counts)) voom_res_sig &lt;-voom_res[voom_res$adj.P.Val &lt;alpha,] voom_res_sig_lfc &lt;-voom_res_sig[abs(voom_res_sig$logFC) &gt;= lfc,] How many differentially expressed genes are identified? nrow(voom_res_sig)nrow(voom_res_sig_lfc) We will write out these results. write.table(voom_res_sig, \"voom_res_sig.txt\", sep=\"\\t\", col.names=NA, quote=F) write.table(voom_res_sig_lfc, \"voom_res_sig_lfc.txt\", sep=\"\\t\", col.names=NA, quote=F) write.table(voom_res, \"voom_res.txt\", sep=\"\\t\", col.names=NA, quote=F) Data Visualisation Now let\u2019s visualize some of this data. First we will make a volcano plot using the volcanoplot function available in limma . This creates a plot which displays fold changes versus a measure of statistical significance of the change. volcanoplot(fit_v, coef=2, highlight=5) [H] [fig:Volcano plot] Next we will create a heatmap of the top differentially expressed genes. We use the heatmap.2 function available in the gplots package. select_top &lt;- p.adjust(fit_v$p.value[, 2]) &lt;1e-2 Exp_top &lt;- v$E [select_top, ] heatmap.2(Exp_top, scale=\"row\", density.info=\"none\", trace=\"none\", main=\"Top DEGs\", labRow=\"\", cexRow=0.4, cexCol=0.8) [H] [fig:Heatmap] You can now quit the R prompt q() You can save your workspace by typing Y on prompt. Please note that the output files you are creating are saved in your present working directory. If you are not sure where you are in the file system try typing pwd on your command prompt to find out. Differential Expression using cuffdiff This is optional exercise and will be run if time permits. One of the stand-alone tools that perform differential expression analysis is Cuffdiff. We use this tool to compare between two conditions; for example different conditions could be control and disease, or wild-type and mutant, or various developmental stages. In our case we want to identify genes that are differentially expressed between two developmental stages; a 2cells embryo and 6h post fertilization. The general format of the cuffdiff command is: cuffdiff [options]* &lt;transcripts.gtf&gt; &lt;sample1_replicate1.sam[,...,sample1_replicateM]&gt; &lt;sample2_replicate1.sam[,...,sample2_replicateM.sam]&gt; Where the input includes a transcripts.gtf file, which is an annotation file of the genome of interest or the cufflinks assembled transcripts, and the aligned reads (either in SAM or BAM format) for the conditions. Some of the Cufflinks options that we will use to run the program are: -o : Output directory. -L : Labels for the different conditions -T : Tells Cuffdiff that the reads are from a time series experiment. -b : Instructs Cufflinks to run a bias detection and correction algorithm which can significantly improve accuracy of transcript abundance estimates. To do this Cufflinks requires a multi-fasta file with the genomic sequences against which we have aligned the reads. -u : Tells Cufflinks to do an initial estimation procedure to more accurately weight reads mapping to multiple locations in the genome (multi-hits). \u2013library-type : Before performing any type of RNA-seq analysis you need to know a few things about the library preparation. Was it done using a strand-specific protocol or not? If yes, which strand? In our data the protocol was NOT strand specific. -C : Biological replicates and multiple group contrast can be defined here Run cuffdiff on the tophat generated BAM files for the 2cells vs. 6h data sets: cuffdiff -o cuffdiff/ -L ZV9_2cells,ZV9_6h -T -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded annotation/Danio_rerio.Zv9.66.gtf tophat/ZV9_2cells/accepted_hits.bam tophat/ZV9_6h/accepted_hits.bam We are interested in the differential expression at the gene level. The results are reported by Cuffdiff in the file cuffdiff/gene_exp.diff . Look at the first few lines of the file using the following command: head -n 20 cuffdiff/gene_exp.diff We would like to see which are the most significantly differentially expressed genes. Therefore we will sort the above file according to the q value (corrected p value for multiple testing). The result will be stored in a different file called gene_exp_qval.sorted.diff . sort -t$'\\t' -g -k 13 cuffdiff/gene_exp.diff &gt; cuffdiff/gene_exp_qval.sorted.diff Look again at the top 20 lines of the sorted file by typing: head -n 20 cuffdiff/gene_exp_qval.sorted.diff Copy an Ensembl transcript identifier from the first two columns for one of these genes (e.g. ENSDARG00000045067 ). Now go back to the IGV browser and paste it in the search box. What are the various outputs generated by cuffdiff? Hint: Please refer to the Cuffdiff output section of the cufflinks manual online. Do you see any difference in the read coverage between the 2cells and 6h conditions that might have given rise to this transcript being called as differentially expressed? The coverage on the Ensembl browser is based on raw reads and no normalisation has taken place contrary to the FPKM values. The read coverage of this transcript ( ENSDARG00000045067 ) in the 6h data set is much higher than in the 2cells data set. Visualising the CuffDiff expression analysis We will use an R-Bioconductor package called cummeRbund to visualise, manipulate and explore Cufflinks RNA-seq output. We will load an R environment and look at few quick tips to generate simple graphical output of the cufflinks analysis we have just run. CummeRbund takes the cuffdiff output and populates a SQLite database with various type of output generated by cuffdiff e.g, genes, transcripts, transcription start site, isoforms and CDS regions. The data from this database can be accessed and processed easily. This package comes with a number of in-built plotting functions that are commonly used for visualising the expression data. We strongly recommend reading through the bioconductor manual and user guide of CummeRbund to learn about functionality of the tool. The reference is provided in the resource section. Prepare the environment. Go to the cuffdiff output folder and copy the transcripts file there. cd /home/trainee/rnaseq/cuffdiff cp /home/trainee/rnaseq/annotation/Danio_rerio.Zv9.66.gtf /home/trainee/rnaseq/cuffdiff ls -l Load the R environment R (press enter) Load the require R package. library(cummeRbund) Read in the cuffdiff output cuff&lt;-readCufflinks(dir=\"/home/trainee/Desktop/rnaseq/cuffdiff\", \\ gtfFile='Danio_rerio.Zv9.66.gtf',genome=\"Zv9\", rebuild=T) Assess the distribution of FPKM scores across samples pdf(file = \"SCV.pdf\", height = 6, width = 6) dens&lt;-csDensity(genes(cuff)) dens dev.off() Box plots of the FPKM values for each samples pdf(file = \"BoxP.pdf\", height = 6, width = 6) b&lt;-csBoxplot(genes(cuff)) b dev.off() Accessing the data sigGeneIds&lt;-getSig(cuff,alpha=0.05,level=\"genes\") head(sigGeneIds) sigGenes&lt;-getGenes(cuff,sigGeneIds) sigGenes head(fpkm(sigGenes)) head(fpkm(isoforms(sigGenes))) Plotting a heatmap of the differentially expressed genes pdf(file = \"heatmap.pdf\", height = 6, width = 6) h&lt;-csHeatmap(sigGenes,cluster=\"both\") h dev.off() What options would you use to draw a density or boxplot for different replicates if available ? (Hint: look at the manual at Bioconductor website) densRep&lt;-csDensity(genes(cuff),replicates=T) brep&lt;-csBoxplot(genes(cuff),replicates=T) How many differentially expressed genes did you observe? type \u2019summary(sigGenes)\u2019 on the R prompt to see. References Trapnell, C., Pachter, L. & Salzberg, S. L. TopHat: discovering splice junctions with RNA-Seq. Bioinformatics 25, 1105-1111 (2009). Trapnell, C. et al. Transcript assembly and quantification by RNA-Seq reveals unannotated transcripts and isoform switching during cell differentiation. Nat. Biotechnol. 28, 511-515 (2010). Langmead, B., Trapnell, C., Pop, M. & Salzberg, S. L. Ultrafast and memory-efficient alignment of short DNA sequences to the human genome. Genome Biol. 10, R25 (2009). Roberts, A., Pimentel, H., Trapnell, C. & Pachter, L. Identification of novel transcripts in annotated genomes using RNA-Seq. Bioinformatics 27, 2325-2329 (2011). Roberts, A., Trapnell, C., Donaghey, J., Rinn, J. L. & Pachter, L. Improving RNA-Seq expression estimates by correcting for fragment bias. Genome Biol. 12, R22 (2011). Robinson MD, McCarthy DJ and Smyth GK. edgeR: a Bioconductor package for differential expression analysis of digital gene expression data. Bioinformatics, 26 (2010). Robinson MD and Smyth GK Moderated statistical tests for assessing differences in tag abundance. Bioinformatics, 23, pp. -6. Robinson MD and Smyth GK (2008). Small-sample estimation of negative binomial dispersion, with applications to SAGE data.\u201d Biostatistics, 9. McCarthy, J. D, Chen, Yunshun, Smyth and K. G (2012). Differential expression analysis of multifactor RNA-Seq experiments with respect to biological variation. Nucleic Acids Research, 40(10), pp. -9.","title":"RNA-Seq"},{"location":"rna-seq/handout/handout/#key-learning-outcomes","text":"After completing this practical the trainee should be able to: Understand and perform a simple RNA-Seq analysis workflow. Perform spliced alignments to an indexed reference genome using TopHat. Visualize spliced transcript alignments in a genome browser such as IGV. Be able to identify differential gene expression between two experimental conditions. Be familiar with R environment and be able to run R based RNA-seq packages. We also have bonus exercises where you can learn to: Perform transcript assembly using Cufflinks. Run cuffdiff, a Cufflinks utility for differential expression analysis. Visualize transcript alignments and annotation in a genome browser such as IGV.","title":"Key Learning Outcomes"},{"location":"rna-seq/handout/handout/#resources-youll-be-using","text":"","title":"Resources You\u2019ll be Using"},{"location":"rna-seq/handout/handout/#tools-used","text":"Tophat : \\ https://ccb.jhu.edu/software/tophat/index.shtml Cufflinks : \\ http://cole-trapnell-lab.github.io/cufflinks/ Samtools : \\ http://www.htslib.org/ BEDTools : \\ https://github.com/arq5x/bedtools2 UCSC tools : \\ http://hgdownload.cse.ucsc.edu/admin/exe/ IGV : \\ http://www.broadinstitute.org/igv/ FeatureCount : \\ http://subread.sourceforge.net/ edgeR pakcage : \\ https://bioconductor.org/packages/release/bioc/html/edgeR.html CummeRbund manual : \\ http://www.bioconductor.org/packages/release/bioc/vignettes/cummeRbund/inst/doc/cummeRbund-manual.pdf","title":"Tools Used"},{"location":"rna-seq/handout/handout/#sources-of-data","text":"http://www.ebi.ac.uk/ena/data/view/ERR022484 \\ http://www.ebi.ac.uk/ena/data/view/ERR022485 \\ http://www.pnas.org/content/suppl/2008/12/16/0807121105.DCSupplemental","title":"Sources of Data"},{"location":"rna-seq/handout/handout/#introduction","text":"The goal of this hands-on session is to perform some basic tasks in the downstream analysis of RNA-seq data.\\ First we will use RNA-seq data from zebrafish. You will align one set of reads to the zebrafish using Tophat2. You will then view the aligned reads using the IGV viewer. We will also demonstrate how gene counts can be derived from this data. You will go on to assembly a transcriptome from the read data using cufflinks. We will show you how this type of data may be analysed for differential expression. The second part of the tutorial will focus on RNA-seq data from a human experiment (cancer cell line versus normal cells). You will use the Bioconductor packages edgeR and voom (limma) to determine differential gene expression. The results from this analysis will then be used in the final session which introduces you to some of the tools used to gain biological insight into the results of a differential expression analysis","title":"Introduction"},{"location":"rna-seq/handout/handout/#prepare-the-environment","text":"We will use a dataset derived from sequencing of mRNA from Danio rerio embryos in two different developmental stages. Sequencing was performed on the Illumina platform and generated 76bp paired-end sequence data using polyA selected RNA. Due to the time constraints of the practical we will only use a subset of the reads. The data files are contained in the subdirectory called data and are the following: 2cells_1.fastq and 2cells_2.fastq : \\ These files are based on RNA-seq data of a 2-cell zebrafish embryo 6h_1.fastq and 6h_2.fastq : \\ These files are based on RNA-seq data of zebrafish embryos 6h post fertilization Open the Terminal and go to the rnaseq working directory: cd /home/trainee/rnaseq/ All commands entered into the terminal for this tutorial should be from within the /home/trainee/rnaseq directory. Check that the data directory contains the above-mentioned files by typing: ls data","title":"Prepare the Environment"},{"location":"rna-seq/handout/handout/#alignment","text":"There are numerous tools for performing short read alignment and the choice of aligner should be carefully made according to the analysis goals/requirements. Here we will use Tophat2, a widely used ultrafast aligner that performs spliced alignments. Tophat2 is based on the Bowtie2 aligner and uses an indexed genome for the alignment to speed up the alignment and keep its memory footprint small. The the index for the Danio rerio genome has been created for you. The command to create an index is as follows. You DO NOT need to run this command yourself - we have done this for you. bowtie2-build genome/Danio_rerio.Zv9.66.dna.fa genome/ZV9 Tophat2 has a number of parameters in order to perform the alignment. To view them all type: tophat2 --help The general format of the tophat2 command is: tophat2 [options]* &lt;index_base&gt; &lt;reads_1&gt; &lt;reads_2&gt; Where the last two arguments are the .fastq files of the paired end reads, and the argument before is the basename of the indexed genome. The quality values in the FASTQ files used in this hands-on session are Phred+33 encoded. We explicitly tell tophat of this fact by using the command line argument \u2013solexa-quals . You can look at the first few reads in the file data/2cells_1.fastq with: head -n 20 data/2cells_1.fastq Some other parameters that we are going to use to run Tophat are listed below: -g : Maximum number of multihits allowed. Short reads are likely to map to more than one location in the genome even though these reads can have originated from only one of these regions. In RNA-seq we allow for a limited number of multihits, and in this case we ask Tophat to report only reads that map at most onto 2 different loci. \u2013library-type : Before performing any type of RNA-seq analysis you need to know a few things about the library preparation. Was it done using a strand-specific protocol or not? If yes, which strand? In our data the protocol was NOT strand specific. -j : Improve spliced alignment by providing Tophat with annotated splice junctions. Pre-existing genome annotation is an advantage when analysing RNA-seq data. This file contains the coordinates of annotated splice junctions from Ensembl. These are stored under the sub-directory annotation in a file called ZV9.spliceSites . -o : This specifies in which subdirectory Tophat should save the output files. Given that for every run the name of the output files is the same, we specify different directories for each run. It takes some time (approx. 20 min) to perform tophat spliced alignments, even for this subset of reads. Therefore, we have pre-aligned the 2cells data for you using the following command: You DO NOT need to run this command yourself - we have done this for you. tophat2 --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_2cells genome/ZV9 data/2cells_1.fastq data/2cells_2.fastq Align the 6h data yourself using the following command: # Takes approx. 20mins tophat2 --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_6h genome/ZV9 data/6h_1.fastq data/6h_2.fastq The 6h read alignment will take approx. 20 min to complete. Therefore, we\u2019ll take a look at some of the files, generated by tophat, for the pre-computed 2cells data. Tophat generates several files in the specified output directory. The most important files are listed below. accepted_hits.bam : This file contains the list of read alignments in BAM format. align_summary.txt : Provides a detailed summary of the read-alignments. unmapped.bam : This file contains the unmapped reads. The complete documentation can be found at: https://ccb.jhu.edu/software/tophat/manual.shtml","title":"Alignment"},{"location":"rna-seq/handout/handout/#alignment-visualisation-in-igv","text":"The Integrative Genomics Viewer (IGV) is able to provide a visualisation of read alignments given a reference sequence and a BAM file. We\u2019ll visualise the information contained in the accepted_hits.bam and junctions.bed files for the pre-computed 2cells data. The former, contains the tophat spliced alignments of the reads to the reference while the latter stores the coordinates of the splice junctions present in the data set. Open the rnaseq directory on your Desktop and double-click the tophat subdirectory and then the ZV9_2cells directory. Launch IGV by double-clicking the \u201cIGV 2.3.*\u201d icon on the Desktop (ignore any warnings that you may get as it opens). NOTE: IGV may take several minutes to load for the first time, please be patient. Choose \u201cZebrafish (Zv9)\u201d from the drop-down box in the top left of the IGV window. Else you can also load the genome fasta file. Load the accepted_hits.sorted.bam file by clicking the \u201cFile\u201d menu, selecting \u201cLoad from File\u201d and navigating to the Desktop/rnaseq/tophat/ZV9_2cells directory. Rename the track by right-clicking on its name and choosing \u201cRename Track\u201d. Give it a meaningful name like \u201c2cells BAM\u201d. Load the junctions.bed from the same directory and rename the track \u201c2cells Junctions BED\u201d. Load the Ensembl annotations file Danio_rerio.Zv9.66.gtf stored in the rnaseq/annotation directory. Navigate to a region on chromosome 12 by typing chr12 : 20 , 270 , 921 - 20 , 300 , 943 into the search box at the top of the IGV window. Keep zooming to view the bam file alignments Some useful IGV manuals can be found below http://www.broadinstitute.org/software/igv/interpreting_insert_size \\ http://www.broadinstitute.org/software/igv/alignmentdata Does the file \u2019align_summary.txt\u2019 look interesting? What information does it provide? As the name suggests, the file provides a details summary of the alignment statistics. One other important file is \u2019unmapped.bam\u2019. This file contains the unampped reads. Can you identify the splice junctions from the BAM file? Splice junctions can be identified in the alignment BAM files. These are the aligned RNA-Seq reads that have skipped-bases from the reference genome (most likely introns). Are the junctions annotated for CBY1 consistent with the annotation? Read alignment supports an extended length in exon 5 to the gene model (cby1-001) Once tophat finishes aligning the 6h data you will need to sort the alignments found in the BAM file and then index the sorted BAM file. samtools sort tophat/ZV9_6h/accepted_hits.bam tophat/ZV9_6h/accepted_hits.sorted samtools index tophat/ZV9_6h/accepted_hits.sorted.bam Load the sorted BAM file into IGV, as described previously, and rename the track appropriately.","title":"Alignment Visualisation in IGV"},{"location":"rna-seq/handout/handout/#generating-gene-counts","text":"In RNAseq experiments the digital gene expression is recorded as the gene counts or number of reads aligning to a known gene feature. If you have a well annotated genome, you can use the gene structure file in a standard gene annotation format (GTF or GFF)) along with the spliced alignment file to quantify the known genes. We will demonstrate a utility called FeatureCounts that comes with the Subread package. mkdir gene_counts featureCounts -a annotation/Danio_rerio.Zv9.66.gtf -t exon -g gene_id -o gene_counts/gene_counts.txt tophat/ZV9_6h/accepted_hits.sorted.bam tophat/ZV9_2cells/accepted_hits.sorted.bam","title":"Generating Gene Counts"},{"location":"rna-seq/handout/handout/#isoform-expression-and-transcriptome-assembly","text":"For non-model organisms and genomes with draft assemblies and incomplete annotations, it is a common practice to take and assembly based approach to generate gene structures followed by the quantification step. There are a number of reference based transcript assemblers available that can be used for this purpose such as, cufflinks, stringy. These assemblers can give gene or isoform level assemblies that can be used to perform a gene/isoform level quantification. These assemblers require an alignment of reads with a reference genome or transcriptome as an input. The second optional input is a known gene structure in GTF or GFF format. There are a number of tools that perform reconstruction of the transcriptome and for this workshop we are going to use Cufflinks. Cufflinks can do transcriptome assembly either ab initio or using a reference annotation. It also quantifies the isoform expression in Fragments Per Kilobase of exon per Million fragments mapped (FPKM). Cufflinks has a number of parameters in order to perform transcriptome assembly and quantification. To view them all type: cufflinks --help We aim to reconstruct the transcriptome for both samples by using the Ensembl annotation both strictly and as a guide. In the first case Cufflinks will only report isoforms that are included in the annotation, while in the latter case it will report novel isoforms as well. The Ensembl annotation for Danio rerio is available in annotation/Danio_rerio.Zv9.66.gtf . The general format of the cufflinks command is: cufflinks [options]* &lt;aligned_reads.(sam|bam)&gt; Where the input is the aligned reads (either in SAM or BAM format). Some of the available parameters for Cufflinks that we are going to use to run Cufflinks are listed below: -o : Output directory. -G : Tells Cufflinks to use the supplied GTF annotations strictly in order to estimate isoform annotation. -b : Instructs Cufflinks to run a bias detection and correction algorithm which can significantly improve accuracy of transcript abundance estimates. To do this Cufflinks requires a multi-fasta file with the genomic sequences against which we have aligned the reads. -u : Tells Cufflinks to do an initial estimation procedure to more accurately weight reads mapping to multiple locations in the genome (multi-hits). \u2013library-type : Before performing any type of RNA-seq analysis you need to know a few things about the library preparation. Was it done using a strand-specific protocol or not? If yes, which strand? In our data the protocol was NOT strand specific. Perform transcriptome assembly, strictly using the supplied GTF annotations, for the 2cells and 6h data using cufflinks: # 2cells data (takes approx. 5mins): cufflinks -o cufflinks/ZV9_2cells_gtf -G annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_2cells/accepted_hits.bam # 6h data (takes approx. 5mins): cufflinks -o cufflinks/ZV9_6h_gtf -G annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_6h/accepted_hits.bam Cufflinks generates several files in the specified output directory. Here\u2019s a short description of these files: genes.fpkm_tracking : Contains the estimated gene-level expression values. isoforms.fpkm_tracking : Contains the estimated isoform-level expression values. skipped.gtf : Contains loci skipped as a result of exceeding the maximum number of fragments. transcripts.gtf : This GTF file contains Cufflinks\u2019 assembled isoforms. The complete documentation can be found at: http://cole-trapnell-lab.github.io/cufflinks/file_formats/#output-formats-used-in-the-cufflinks-suite So far we have forced cufflinks, by using the -G option, to strictly use the GTF annotations provided and thus novel transcripts will not be reported. We can get cufflinks to perform a GTF-guided transcriptome assembly by using the -g option instead. Thus, novel transcripts will be reported. GTF-guided transcriptome assembly is more computationally intensive than strictly using the GTF annotations. Therefore, we have pre-computed these GTF-guided assemblies for you and have placed the results under subdirectories: cufflinks/ZV9_2cells_gtf_guided and cufflinks/ZV9_6h_gft_guided . You DO NOT need to run these commands. We provide them so you know how we generated the the GTF-guided transcriptome assemblies: # 2cells guided transcriptome assembly (takes approx. 30mins): cufflinks -o cufflinks/ZV9_2cells_gtf_guided -g annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_2cells/accepted_hits.bam # 6h guided transcriptome assembly (takes approx. 30mins): cufflinks -o cufflinks/ZV9_6h_gtf_guided -g annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_6h/accepted_hits.bam Go back to IGV and load the pre-computed, GTF-guided transcriptome assembly for the 2cells data ( cufflinks/ZV9_2cells_gtf_guided/transcripts.gtf ). Rename the track as \u201c2cells GTF-Guided Transcripts\u201d. In the search box type ENSDART00000082297 in order for the browser to zoom in to the gene of interest. Do you observe any difference between the Ensembl GTF annotations and the GTF-guided transcripts assembled by cufflinks (the \u201c2cells GTF-Guided Transcripts\u201d track)? Yes. It appears that the Ensembl annotations may have truncated the last exon. However, our data also doesn\u2019t contain reads that span between the last two exons.","title":"Isoform Expression and Transcriptome Assembly"},{"location":"rna-seq/handout/handout/#differential-gene-expression-analysis-using-edger","text":"","title":"Differential Gene Expression Analysis using edgeR"},{"location":"rna-seq/handout/handout/#experiment-design","text":"The example we are working through today follows a case Study set out in the edgeR Users Guide (4.3 Androgen-treated prostate cancer cells (RNA-Seq, two groups) which is based on an experiment conducted by Li et al. (2008, Proc Natl Acad Sci USA, 105, 20179-84). The researches used a prostate cancer cell line (LNCaP cells). These cells are sensitive to stimulation by male hormones (androgens). Three replicate RNA samples were collected from LNCaP cells treated with an androgen hormone (DHT). Four replicates were collected from cells treated with an inactive compound. Each of the seven samples was run on a lane (7 lanes) of an Illumina flow cell to produce 35 bp reads. The experimental design was therefore: [H] rrr Lane & Treatment & Label \\ 1 & Control & Con1\\ 2 & Control & Con2\\ 3 & Control & Con3\\ 4 & Control & Con4\\ 5 & DHT & DHT1\\ 6 & DHT & DHT2\\ 7 & DHT & DHT3\\ [tab:experimental d esign] This workflow requires raw gene count files and these can be generated using a utility called featureCounts as demonstrated above. We are using a pre-computed gene counts data (stored in pnas_expression.txt ) for this exercise.","title":"Experiment Design"},{"location":"rna-seq/handout/handout/#prepare-the-environment_1","text":"Prepare the environment and load R: cd /home/trainee/rnaseq/edgeR R (press enter) Once on the R prompt. Load libraries: library(edgeR) library(biomaRt) library(gplots) library(\"limma\") library(\"RColorBrewer\") library(\"org.Hs.eg.db\")","title":"Prepare the Environment"},{"location":"rna-seq/handout/handout/#read-in-data","text":"Read in count table and experimental design: data &lt;- read.delim(\"pnas_expression.txt\", row.names=1, header=T) targets &lt;- read.delim(\"Targets.txt\", header=T) colnames(data) &lt;-targets$Label head(data, n=20)","title":"Read in Data"},{"location":"rna-seq/handout/handout/#add-gene-annotation","text":"The data set only includes the Ensembl gene id and the counts. It is useful to have other annotations such as the gene symbol and entrez id. Next we will add in these annotations. We will use the BiomaRt package to do this. We start by using the useMart function of BiomaRt to access the human data base of ensemble gene ids. human&lt;-useMart(host=\"www.ensembl.org\", \"ENSEMBL_MART_ENSEMBL\", dataset=\"hsapiens_gene_ensembl\") attributes=c(\"ensembl_gene_id\", \"entrezgene\",\"hgnc_symbol\") We create a vector of our ensemble gene ids. ensembl_names&lt;-rownames(data) head(ensembl_names) We then use the function getBM to get the gene symbol data we want.This takes about a minute. genemap&lt;-getBM(attributes, filters=\"ensembl_gene_id\", values=ensembl_names, mart=human) Have a look at the start of the genemap dataframe. head(genemap) We then match the data we have retrieved to our dataset. idx &lt;-match(ensembl_names, genemap$ensembl_gene_id) data$entrezgene &lt;-genemap$entrezgene [ idx ] data$hgnc_symbol &lt;-genemap$hgnc_symbol [ idx ] Ann &lt;- cbind(rownames(data), data$hgnc_symbol, data$entrezgene) colnames(Ann)&lt;-c(\"Ensembl\", \"Symbol\", \"Entrez\") Ann&lt;-as.data.frame(Ann) Let\u2019s check and see that this additional information is there. head(data)","title":"Add Gene annotation"},{"location":"rna-seq/handout/handout/#data-checks","text":"Create DGEList object: treatment &lt;-factor(c(rep(\"Control\",4), rep(\"DHT\",3)), levels=c(\"Control\", \"DHT\")) y &lt;-DGEList(counts=data[,1:7], group=treatment, genes=Ann) Check the dimensions of the object: dim(y) We see we have 37435 rows (i.e. genes) and 7 columns (samples). Now we will filter out genes with low counts by only keeping those rows where the count per million (cpm) is at least 1 in at least three samples: keep &lt;-rowSums( cpm(y)&gt;1) &gt;=3 y &lt;- y[keep, ] How many rows (genes) are retained now dim(y) would give you 16494 How many genes were filtered out? do 37435-16494. As we have removed the lowly expressed genes the total number of counts per sample has not changed greatly. Let us check the total number of reads per sample in the original data (data) and now after filtering. Before: colSums(data[,1:7]) After filtering: colSums(y$counts) We will now perform normalization to take account of different library size: y&lt;-calcNormFactors(y) We will check the calculated normalization factors: y$samples Lets have a look at whether the samples cluster by condition. (You should produce a plot as shown in Figure 4): plotMDS(y, col=as.numeric(y$samples$group)) [H] [fig:MDS plot] Does the MDS plot indicate a difference in gene expression between the Controls and the DHT treated samples? The MDS plot shows us that the controls are separated from the DHT treated cells. This indicates that there is a difference in gene expression between the conditions. We will now estimate the dispersion. We start by estimating the common dispersion. The common dispersion estimates the overall Biological Coefficient of Variation (BCV) of the dataset averaged over all genes. By using verbose we get the Disp and BCV values printed on the screen y &lt;- estimateCommonDisp(y, verbose=T) What value to you see for BCV? We now estimate gene-specific dispersion. y &lt;- estimateTagwiseDisp(y) We will plot the tagwise dispersion and the common dispersion (You should obtain a plot as shown in the Figure 5): plotBCV(y) [H] [fig:BCV plot] We see here that the common dispersion estimates the overall Biological Coefficient of Variation (BCV) of the dataset averaged over all genes. The common dispersion is 0.02 and the BCV is the square root of the common dispersion (sqrt[0.02] = 0.14). A BCV of 14% is typical for cell line experiment. As you can see from the plot the BCV of some genes (generally those with low expression) can be much higher than the common dispersion. For example we see genes with a reasonable level of expression with tagwise dispersion of 0.4 indicating 40% variation between samples. If we used the common dispersion for these genes instead of the tagwise dispersion what effect would this have? If we simply used the common dispersion for these genes we would underestimate biological variability, which in turn affects whether these genes would be identified as being differentially expressed between conditions.It is recommended to use the tagwise dispersion, which takes account of gene-to-gene variability. Now that we have normalized our data and also calculated the variability of gene expression between samples we are in a position to perform differential expression testing.As this is a simple comparison between two conditions, androgen treatment and placebo treatment we can use the exact test for the negative binomial distribution (Robinson and Smyth, 2008).","title":"Data checks"},{"location":"rna-seq/handout/handout/#testing-for-differential-expression","text":"We now test for differentially expressed BCV genes: et &lt;- exactTest(y) Now we will use the topTags function to adjust for multiple testing. We will use the Benjimini Hochberg (\u201cBH\u201d) method and we will produce a table of results: res &lt;- topTags(et, n=nrow(y$counts), adjust.method=\"BH\")$table Let\u2019s have a look at the first rows of the table: head(res) To get a summary of the number of differentially expressed genes we can use the decideTestsDGE function. summary(de &lt;- decideTestsDGE(et)) This tells us that 2096 genes are downregulated and 2339 genes are upregulated at 5% FDR.We will now make subsets of the most significant upregulated and downregulated genes. alpha=0.05 lfc=1.5 edgeR_res_sig&lt;-res[res$FDR&lt;alpha,] edgeR_res_sig_lfc &lt;-edgeR_res_sig[abs(edgeR_res_sig$logFC) &gt;= lfc,]head(edgeR_res_sig, n=20)nrow(edgeR_res_sig)nrow(edgeR_res_sig_lfc) We can write out these results to our current directory. write.table(edgeR_res_sig , \"edgeR_res_sig.txt\", sep=\"\\t\", col.names=NA, quote=F) write.table(edgeR_res_sig_lfc , \"edgeR_res_sig_lfc.txt\", sep=\"\\t\", col.names=NA, quote=F) How many differentially expressed genes are there? 4435 How many upregulated genes and downregulated genes do we have? 2339 2096","title":"Testing for Differential Expression"},{"location":"rna-seq/handout/handout/#differential-expression-using-the-voom-function-and-the-limma-package","text":"We will now show an alternative approach to differential expression which uses the limma package.This is based on linear models. The first step is to create a design matrix. In this case we have a simple design where we have only one condition (treated vs non-treated). However, you may be dealing with more complex experimental designs, for example looking at treatment and other covariates, such as age, gender, batch. design &lt;-model.matrix(~treatment) check design print(design) We now use voom to transform the data into a form which is appropriate for linear modelling. v &lt;-voom(y, design) Next we will fit linear model to each gene in the dataset using the function lmFit. Following this we use the function eBayes to test each gene to find whether foldchange between the conditions being tested is statistically significant.We filter our results by using the same values of alpha (0.05) and log fold change (1.5) used previously. fit_v &lt;-lmFit(v, design) fit_v &lt;- eBayes(fit_v) voom_res&lt;-topTable(fit_v, coef=2,adjust.method=\"BH\", sort.by=\"P\", n=nrow(y$counts)) voom_res_sig &lt;-voom_res[voom_res$adj.P.Val &lt;alpha,] voom_res_sig_lfc &lt;-voom_res_sig[abs(voom_res_sig$logFC) &gt;= lfc,] How many differentially expressed genes are identified? nrow(voom_res_sig)nrow(voom_res_sig_lfc) We will write out these results. write.table(voom_res_sig, \"voom_res_sig.txt\", sep=\"\\t\", col.names=NA, quote=F) write.table(voom_res_sig_lfc, \"voom_res_sig_lfc.txt\", sep=\"\\t\", col.names=NA, quote=F) write.table(voom_res, \"voom_res.txt\", sep=\"\\t\", col.names=NA, quote=F)","title":"Differential expression using the Voom function and the limma package"},{"location":"rna-seq/handout/handout/#data-visualisation","text":"Now let\u2019s visualize some of this data. First we will make a volcano plot using the volcanoplot function available in limma . This creates a plot which displays fold changes versus a measure of statistical significance of the change. volcanoplot(fit_v, coef=2, highlight=5) [H] [fig:Volcano plot] Next we will create a heatmap of the top differentially expressed genes. We use the heatmap.2 function available in the gplots package. select_top &lt;- p.adjust(fit_v$p.value[, 2]) &lt;1e-2 Exp_top &lt;- v$E [select_top, ] heatmap.2(Exp_top, scale=\"row\", density.info=\"none\", trace=\"none\", main=\"Top DEGs\", labRow=\"\", cexRow=0.4, cexCol=0.8) [H] [fig:Heatmap] You can now quit the R prompt q() You can save your workspace by typing Y on prompt. Please note that the output files you are creating are saved in your present working directory. If you are not sure where you are in the file system try typing pwd on your command prompt to find out.","title":"Data Visualisation"},{"location":"rna-seq/handout/handout/#differential-expression-using-cuffdiff","text":"This is optional exercise and will be run if time permits. One of the stand-alone tools that perform differential expression analysis is Cuffdiff. We use this tool to compare between two conditions; for example different conditions could be control and disease, or wild-type and mutant, or various developmental stages. In our case we want to identify genes that are differentially expressed between two developmental stages; a 2cells embryo and 6h post fertilization. The general format of the cuffdiff command is: cuffdiff [options]* &lt;transcripts.gtf&gt; &lt;sample1_replicate1.sam[,...,sample1_replicateM]&gt; &lt;sample2_replicate1.sam[,...,sample2_replicateM.sam]&gt; Where the input includes a transcripts.gtf file, which is an annotation file of the genome of interest or the cufflinks assembled transcripts, and the aligned reads (either in SAM or BAM format) for the conditions. Some of the Cufflinks options that we will use to run the program are: -o : Output directory. -L : Labels for the different conditions -T : Tells Cuffdiff that the reads are from a time series experiment. -b : Instructs Cufflinks to run a bias detection and correction algorithm which can significantly improve accuracy of transcript abundance estimates. To do this Cufflinks requires a multi-fasta file with the genomic sequences against which we have aligned the reads. -u : Tells Cufflinks to do an initial estimation procedure to more accurately weight reads mapping to multiple locations in the genome (multi-hits). \u2013library-type : Before performing any type of RNA-seq analysis you need to know a few things about the library preparation. Was it done using a strand-specific protocol or not? If yes, which strand? In our data the protocol was NOT strand specific. -C : Biological replicates and multiple group contrast can be defined here Run cuffdiff on the tophat generated BAM files for the 2cells vs. 6h data sets: cuffdiff -o cuffdiff/ -L ZV9_2cells,ZV9_6h -T -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded annotation/Danio_rerio.Zv9.66.gtf tophat/ZV9_2cells/accepted_hits.bam tophat/ZV9_6h/accepted_hits.bam We are interested in the differential expression at the gene level. The results are reported by Cuffdiff in the file cuffdiff/gene_exp.diff . Look at the first few lines of the file using the following command: head -n 20 cuffdiff/gene_exp.diff We would like to see which are the most significantly differentially expressed genes. Therefore we will sort the above file according to the q value (corrected p value for multiple testing). The result will be stored in a different file called gene_exp_qval.sorted.diff . sort -t$'\\t' -g -k 13 cuffdiff/gene_exp.diff &gt; cuffdiff/gene_exp_qval.sorted.diff Look again at the top 20 lines of the sorted file by typing: head -n 20 cuffdiff/gene_exp_qval.sorted.diff Copy an Ensembl transcript identifier from the first two columns for one of these genes (e.g. ENSDARG00000045067 ). Now go back to the IGV browser and paste it in the search box. What are the various outputs generated by cuffdiff? Hint: Please refer to the Cuffdiff output section of the cufflinks manual online. Do you see any difference in the read coverage between the 2cells and 6h conditions that might have given rise to this transcript being called as differentially expressed? The coverage on the Ensembl browser is based on raw reads and no normalisation has taken place contrary to the FPKM values. The read coverage of this transcript ( ENSDARG00000045067 ) in the 6h data set is much higher than in the 2cells data set.","title":"Differential Expression using cuffdiff"},{"location":"rna-seq/handout/handout/#visualising-the-cuffdiff-expression-analysis","text":"We will use an R-Bioconductor package called cummeRbund to visualise, manipulate and explore Cufflinks RNA-seq output. We will load an R environment and look at few quick tips to generate simple graphical output of the cufflinks analysis we have just run. CummeRbund takes the cuffdiff output and populates a SQLite database with various type of output generated by cuffdiff e.g, genes, transcripts, transcription start site, isoforms and CDS regions. The data from this database can be accessed and processed easily. This package comes with a number of in-built plotting functions that are commonly used for visualising the expression data. We strongly recommend reading through the bioconductor manual and user guide of CummeRbund to learn about functionality of the tool. The reference is provided in the resource section. Prepare the environment. Go to the cuffdiff output folder and copy the transcripts file there. cd /home/trainee/rnaseq/cuffdiff cp /home/trainee/rnaseq/annotation/Danio_rerio.Zv9.66.gtf /home/trainee/rnaseq/cuffdiff ls -l Load the R environment R (press enter) Load the require R package. library(cummeRbund) Read in the cuffdiff output cuff&lt;-readCufflinks(dir=\"/home/trainee/Desktop/rnaseq/cuffdiff\", \\ gtfFile='Danio_rerio.Zv9.66.gtf',genome=\"Zv9\", rebuild=T) Assess the distribution of FPKM scores across samples pdf(file = \"SCV.pdf\", height = 6, width = 6) dens&lt;-csDensity(genes(cuff)) dens dev.off() Box plots of the FPKM values for each samples pdf(file = \"BoxP.pdf\", height = 6, width = 6) b&lt;-csBoxplot(genes(cuff)) b dev.off() Accessing the data sigGeneIds&lt;-getSig(cuff,alpha=0.05,level=\"genes\") head(sigGeneIds) sigGenes&lt;-getGenes(cuff,sigGeneIds) sigGenes head(fpkm(sigGenes)) head(fpkm(isoforms(sigGenes))) Plotting a heatmap of the differentially expressed genes pdf(file = \"heatmap.pdf\", height = 6, width = 6) h&lt;-csHeatmap(sigGenes,cluster=\"both\") h dev.off() What options would you use to draw a density or boxplot for different replicates if available ? (Hint: look at the manual at Bioconductor website) densRep&lt;-csDensity(genes(cuff),replicates=T) brep&lt;-csBoxplot(genes(cuff),replicates=T) How many differentially expressed genes did you observe? type \u2019summary(sigGenes)\u2019 on the R prompt to see.","title":"Visualising the CuffDiff expression analysis"},{"location":"rna-seq/handout/handout/#references","text":"Trapnell, C., Pachter, L. & Salzberg, S. L. TopHat: discovering splice junctions with RNA-Seq. Bioinformatics 25, 1105-1111 (2009). Trapnell, C. et al. Transcript assembly and quantification by RNA-Seq reveals unannotated transcripts and isoform switching during cell differentiation. Nat. Biotechnol. 28, 511-515 (2010). Langmead, B., Trapnell, C., Pop, M. & Salzberg, S. L. Ultrafast and memory-efficient alignment of short DNA sequences to the human genome. Genome Biol. 10, R25 (2009). Roberts, A., Pimentel, H., Trapnell, C. & Pachter, L. Identification of novel transcripts in annotated genomes using RNA-Seq. Bioinformatics 27, 2325-2329 (2011). Roberts, A., Trapnell, C., Donaghey, J., Rinn, J. L. & Pachter, L. Improving RNA-Seq expression estimates by correcting for fragment bias. Genome Biol. 12, R22 (2011). Robinson MD, McCarthy DJ and Smyth GK. edgeR: a Bioconductor package for differential expression analysis of digital gene expression data. Bioinformatics, 26 (2010). Robinson MD and Smyth GK Moderated statistical tests for assessing differences in tag abundance. Bioinformatics, 23, pp. -6. Robinson MD and Smyth GK (2008). Small-sample estimation of negative binomial dispersion, with applications to SAGE data.\u201d Biostatistics, 9. McCarthy, J. D, Chen, Yunshun, Smyth and K. G (2012). Differential expression analysis of multifactor RNA-Seq experiments with respect to biological variation. Nucleic Acids Research, 40(10), pp. -9.","title":"References"},{"location":"rna-seq/presentations/","text":"This directory is intended to store presentation files used to provide context to the hands-on aspect of the module.","title":"Home"},{"location":"rna-seq/tools/","text":"The tools used in the module\u2019s tutorial exercises are captured in a tools.yaml file. This file can then be used by a workshop deployment script to download and install the tools required by the module.","title":"Home"},{"location":"trainers/trainers/","text":"The Trainers \\newlength{\\trainerIconWidth} \\setlength{\\trainerIconWidth}{2.0cm} +:----------------------------------+:----------------------------------+ | {width=\u201d | Dr. Zhiliang Chen | | \\trainerIconWidth\u201d} | Postdoctoral Research Associate | | | The University of New South Wales | | | (UNSW), NSW | +-----------------------------------+-----------------------------------+ | {width | Dr. Susan Corley Postdoctoral | | =\u201d\\trainerIconWidth\u201d} | Research Associate The University | | | of New South Wales (UNSW), NSW | +-----------------------------------+-----------------------------------+ | {wi | Dr. Nandan Deshpande | | dth=\u201d\\trainerIconWidth\u201d} | Postdoctoral Research Associate | | | The University of New South Wales | | | (UNSW), NSW | +-----------------------------------+-----------------------------------+ | {widt | Dr. Konsta Duesing Research | | h=\u201d\\trainerIconWidth\u201d} | Team Leader - Statistics & | | | Bioinformatics CSIRO Animal, Food | | | and Health Science, NSW | +-----------------------------------+-----------------------------------+ | {width= | Dr. Matthew Field | | \u201c\\trainerIconWidth\u201d} | Computational Biologist The John | | | Curtin School of Medical Research | | | ANU College of Medicine, Biology | | | & Environment, ACT | +-----------------------------------+-----------------------------------+ | {width=\u201d\\t | Dr. Xi (Sean) Li | | rainerIconWidth\u201d} | Bioinformatics Analyst | | | Bioinformatics Core, CSIRO | | | Mathematics, Informatics and | | | Statistics, ACT | +-----------------------------------+-----------------------------------+ | {widt | Dr. Annette McGrath | | h=\u201d\\trainerIconWidth\u201d} | Bioinformatics Core Leader at | | | CSIRO Bioinformatics Core, CSIRO | | | Mathematics, Informatics and | | | Statistics, ACT | +-----------------------------------+-----------------------------------+ | {wi | Mr. Sean McWilliam | | dth=\u201d\\trainerIconWidth\u201d} | Bioinformatics Analyst CSIRO | | | Animal, Food and Health Sciences, | | | QLD | +-----------------------------------+-----------------------------------+ | \\centering | Dr. Paula Moolhuijzen | | \\arraybackslash | Bioinformatics Analyst Centre for | | \\pagebreak | Crop Disease Management, Curtin | | { | University, WA | | width=\u201d\\trainerIconWidth\u201d} | | +-----------------------------------+-----------------------------------+ | {width= | Dr. Sonika Tyagi | | \u201c\\trainerIconWidth\u201d} | Bioinformatics Supervisor | | | Australian Genome Research | | | Facility Ltd, The Walter and | | | Eliza Hall Institute, VIC | +-----------------------------------+-----------------------------------+ | | Dr. Nathan S. Watson-Haigh | | {width=\u201d\\trainerIconWidth\u201d} | Research Fellow in Bioinformatics | | | The Australian Centre for Plant | | | Functional Genomics (ACPFG), SA | +-----------------------------------+-----------------------------------+","title":"The Trainers"},{"location":"trainers/trainers/#the-trainers","text":"\\newlength{\\trainerIconWidth} \\setlength{\\trainerIconWidth}{2.0cm} +:----------------------------------+:----------------------------------+ | {width=\u201d | Dr. Zhiliang Chen | | \\trainerIconWidth\u201d} | Postdoctoral Research Associate | | | The University of New South Wales | | | (UNSW), NSW | +-----------------------------------+-----------------------------------+ | {width | Dr. Susan Corley Postdoctoral | | =\u201d\\trainerIconWidth\u201d} | Research Associate The University | | | of New South Wales (UNSW), NSW | +-----------------------------------+-----------------------------------+ | {wi | Dr. Nandan Deshpande | | dth=\u201d\\trainerIconWidth\u201d} | Postdoctoral Research Associate | | | The University of New South Wales | | | (UNSW), NSW | +-----------------------------------+-----------------------------------+ | {widt | Dr. Konsta Duesing Research | | h=\u201d\\trainerIconWidth\u201d} | Team Leader - Statistics & | | | Bioinformatics CSIRO Animal, Food | | | and Health Science, NSW | +-----------------------------------+-----------------------------------+ | {width= | Dr. Matthew Field | | \u201c\\trainerIconWidth\u201d} | Computational Biologist The John | | | Curtin School of Medical Research | | | ANU College of Medicine, Biology | | | & Environment, ACT | +-----------------------------------+-----------------------------------+ | {width=\u201d\\t | Dr. Xi (Sean) Li | | rainerIconWidth\u201d} | Bioinformatics Analyst | | | Bioinformatics Core, CSIRO | | | Mathematics, Informatics and | | | Statistics, ACT | +-----------------------------------+-----------------------------------+ | {widt | Dr. Annette McGrath | | h=\u201d\\trainerIconWidth\u201d} | Bioinformatics Core Leader at | | | CSIRO Bioinformatics Core, CSIRO | | | Mathematics, Informatics and | | | Statistics, ACT | +-----------------------------------+-----------------------------------+ | {wi | Mr. Sean McWilliam | | dth=\u201d\\trainerIconWidth\u201d} | Bioinformatics Analyst CSIRO | | | Animal, Food and Health Sciences, | | | QLD | +-----------------------------------+-----------------------------------+ | \\centering | Dr. Paula Moolhuijzen | | \\arraybackslash | Bioinformatics Analyst Centre for | | \\pagebreak | Crop Disease Management, Curtin | | { | University, WA | | width=\u201d\\trainerIconWidth\u201d} | | +-----------------------------------+-----------------------------------+ | {width= | Dr. Sonika Tyagi | | \u201c\\trainerIconWidth\u201d} | Bioinformatics Supervisor | | | Australian Genome Research | | | Facility Ltd, The Walter and | | | Eliza Hall Institute, VIC | +-----------------------------------+-----------------------------------+ | | Dr. Nathan S. Watson-Haigh | | {width=\u201d\\trainerIconWidth\u201d} | Research Fellow in Bioinformatics | | | The Australian Centre for Plant | | | Functional Genomics (ACPFG), SA | +-----------------------------------+-----------------------------------+","title":"The Trainers"}]}