{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Introduction to NGS Workshop (5-7 Nov 2018) The Introduction to NGS Data Analysis is a three-day, hands-on workshop that offers attendees a basic understanding of NGS data analysis workflows. The workshop provides hands-on computational experience in analysis of NGS data using common analytical approaches for ChIP-Seq (optional), RNA-Seq data and de novo genome assembly. Time Table Time Subject notes Day1: 5 Nov 2018 9-9:15 am Housekeeping + Icebreaker Slides + group activity 9:15-10:45 am Intro to CLI Lecture + hands on 10:45-11:00 am Break 11:00-11:30 pm Introduction to HTS Lecture 11:30-11:50 pm Intro to QC Lecture 11:50-12:30 pm Assess quality of sequence data Hands on exercise 12:30-12:45 pm Recap + QA Group discussion Day2: 6 Nov 2018 9:00-9:45 am Intro to denovo assembly Lecture 9:45-10:45 am Compile and explore Velvet + read data Practice 10:45-11:00 am Break 11:00-11:30 am Assembly activity I group activity 11:30-12:30 pm Perform assembly using different settings Practice 12:30-1:30 pm Break 1:30-2:00 pm Assembly activity II Group discussion 2:00-3:00 pm Assembly visualization and exploration Practice 3:00-3:15 pm Break 3:15-4:00 pm Assembly Stats summary Practice 4:00-5:00 pm Assembly talk part II Lecture Day3: 7 Nov 2018 9:00-10:00 am Run eukaryotic assembly Practice 10:00-10:45 am Assembly statistics Practice 10:45-11:00 am Break 10:50-11:20 am Scaffolding Practice/ discussion 11:20-12:30 pm Long read assembly Lecture + Practice 12:30-1:30 pm Break ----- -------- ----- 1:30-1:45 pm Intro to Alignment Lecture 1:45-2:15 pm Alignment and processing bams Practice 2:15-3pm RNAseq: align reads from two conditions Practice 3-3:15 pm break 3:15-3:30 pm Quick refresher on RNAseq experiment Lecture 3:30-4 pm Align Reads. Visualise and explore bam files Practice 4-4:20 pm RNAseq: transcript assembly Practice 4:20 - 4:45 pm RNAseq: DE using edgeR Practice 4:45-5 pm Wrap up Workshop Content Topics covered by this workshop include: An introduction to the command line interface and NGS file formats Assessment of the quality of NGS sequence reads Sequence alignment algorithms Basic RNA-Seq analysis Short and long read de novo genome assembly This workshop will be delivered using a mixture of lectures, hands-on practical sessions, and open discussions. Acknowledgements This workshop was developed by Bioplatforms Australia, in partnership with CSIRO, and with support from the European Bioinformatics Institute (EBI), a member of the European Molecular Biology Laboratory (EMBL) in the UK. The workshop content has been maintained and updated by a network of dedicated [bioinformatics trainers] ( http://www.bioplatforms.com/bioinformatics-training/ ) from around Australia. The Bioinformatics Training Platform was developed in collaboration with the Monash Bioinformatics Platform and this workshop has been hosted on Interset infrastructure. This workshop is possible thanks to funding support from the NSW Office of Health and Medical Research and the Commonwealth Government National Collaborative Research Infrastructure Strategy (NCRIS).","title":"Home"},{"location":"#welcome-to-the-introduction-to-ngs-workshop-5-7-nov-2018","text":"The Introduction to NGS Data Analysis is a three-day, hands-on workshop that offers attendees a basic understanding of NGS data analysis workflows. The workshop provides hands-on computational experience in analysis of NGS data using common analytical approaches for ChIP-Seq (optional), RNA-Seq data and de novo genome assembly.","title":"Welcome to the Introduction to NGS Workshop (5-7 Nov 2018)"},{"location":"#time-table","text":"Time Subject notes Day1: 5 Nov 2018 9-9:15 am Housekeeping + Icebreaker Slides + group activity 9:15-10:45 am Intro to CLI Lecture + hands on 10:45-11:00 am Break 11:00-11:30 pm Introduction to HTS Lecture 11:30-11:50 pm Intro to QC Lecture 11:50-12:30 pm Assess quality of sequence data Hands on exercise 12:30-12:45 pm Recap + QA Group discussion Day2: 6 Nov 2018 9:00-9:45 am Intro to denovo assembly Lecture 9:45-10:45 am Compile and explore Velvet + read data Practice 10:45-11:00 am Break 11:00-11:30 am Assembly activity I group activity 11:30-12:30 pm Perform assembly using different settings Practice 12:30-1:30 pm Break 1:30-2:00 pm Assembly activity II Group discussion 2:00-3:00 pm Assembly visualization and exploration Practice 3:00-3:15 pm Break 3:15-4:00 pm Assembly Stats summary Practice 4:00-5:00 pm Assembly talk part II Lecture Day3: 7 Nov 2018 9:00-10:00 am Run eukaryotic assembly Practice 10:00-10:45 am Assembly statistics Practice 10:45-11:00 am Break 10:50-11:20 am Scaffolding Practice/ discussion 11:20-12:30 pm Long read assembly Lecture + Practice 12:30-1:30 pm Break ----- -------- ----- 1:30-1:45 pm Intro to Alignment Lecture 1:45-2:15 pm Alignment and processing bams Practice 2:15-3pm RNAseq: align reads from two conditions Practice 3-3:15 pm break 3:15-3:30 pm Quick refresher on RNAseq experiment Lecture 3:30-4 pm Align Reads. Visualise and explore bam files Practice 4-4:20 pm RNAseq: transcript assembly Practice 4:20 - 4:45 pm RNAseq: DE using edgeR Practice 4:45-5 pm Wrap up","title":"Time Table"},{"location":"#workshop-content","text":"Topics covered by this workshop include: An introduction to the command line interface and NGS file formats Assessment of the quality of NGS sequence reads Sequence alignment algorithms Basic RNA-Seq analysis Short and long read de novo genome assembly This workshop will be delivered using a mixture of lectures, hands-on practical sessions, and open discussions.","title":"Workshop Content"},{"location":"#acknowledgements","text":"This workshop was developed by Bioplatforms Australia, in partnership with CSIRO, and with support from the European Bioinformatics Institute (EBI), a member of the European Molecular Biology Laboratory (EMBL) in the UK. The workshop content has been maintained and updated by a network of dedicated [bioinformatics trainers] ( http://www.bioplatforms.com/bioinformatics-training/ ) from around Australia. The Bioinformatics Training Platform was developed in collaboration with the Monash Bioinformatics Platform and this workshop has been hosted on Interset infrastructure. This workshop is possible thanks to funding support from the NSW Office of Health and Medical Research and the Commonwealth Government National Collaborative Research Infrastructure Strategy (NCRIS).","title":"Acknowledgements"},{"location":"license/","text":"This work is licensed under a Creative Commons Attribution 3.0 Unported License and the below text is a summary of the main terms of the full Legal Code (the full license) available at http://creativecommons.org/licenses/by/3.0/legalcode . You are free: to copy, distribute, display, and perform the work to make derivative works to make commercial use of the work Under the following conditions: Attribution - You must give the original author credit. With the understanding that: Waiver - Any of the above conditions can be waived if you get permission from the copyright holder. Public Domain - Where the work or any of its elements is in the public domain under applicable law, that status is in no way affected by the license. Other Rights - In no way are any of the following rights affected by the license: Your fair dealing or fair use rights, or other applicable copyright exceptions and limitations; The author\u2019s moral rights; Rights other persons may have either in the work itself or in how the work is used, such as publicity or privacy rights. Notice - For any reuse or distribution, you must make clear to others the license terms of this work.","title":"License"},{"location":"alignment/","text":"NGS Read Mapping Bioinformatics Training Platform (BTP) Module: NGS Read Mapping with Bowtie2 Topic NGS Read Mapping with Bowtie2 Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes Perform the simple NGS data alignment task against one interested reference data Interpret and manipulate the mapping output using SAMtools Visualise the alignment via a standard genome browser, e.g. IGV browser Time Required 2 hrs License The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"NGS Read Mapping"},{"location":"alignment/#ngs-read-mapping","text":"Bioinformatics Training Platform (BTP) Module: NGS Read Mapping with Bowtie2 Topic NGS Read Mapping with Bowtie2 Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes Perform the simple NGS data alignment task against one interested reference data Interpret and manipulate the mapping output using SAMtools Visualise the alignment via a standard genome browser, e.g. IGV browser Time Required 2 hrs","title":"NGS Read Mapping"},{"location":"alignment/#license","text":"The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"License"},{"location":"alignment/handout/","text":"NGS Read Mapping Acknowledgements This module was developed by Bioplatforms Australia, in partnership with CSIRO, and with support from the European Bioinformatics Institute (EBI), a member of the European Molecular Biology Laboratory (EMBL) in the UK. The module content has been maintained and updated by a network of dedicated [bioinformatics trainers] ( http://www.bioplatforms.com/bioinformatics-training/ ) from around Australia. The Bioinformatics Training Platform was developed in collaboration with the Monash Bioinformatics Platform. The development of this module is also supported by the NSW Office of Health and Medical Research and the Commonwealth Government National Collaborative Research Infrastructure Strategy (NCRIS).","title":"NGS Read Mapping"},{"location":"alignment/handout/#ngs-read-mapping","text":"","title":"NGS Read Mapping"},{"location":"alignment/handout/#acknowledgements","text":"This module was developed by Bioplatforms Australia, in partnership with CSIRO, and with support from the European Bioinformatics Institute (EBI), a member of the European Molecular Biology Laboratory (EMBL) in the UK. The module content has been maintained and updated by a network of dedicated [bioinformatics trainers] ( http://www.bioplatforms.com/bioinformatics-training/ ) from around Australia. The Bioinformatics Training Platform was developed in collaboration with the Monash Bioinformatics Platform. The development of this module is also supported by the NSW Office of Health and Medical Research and the Commonwealth Government National Collaborative Research Infrastructure Strategy (NCRIS).","title":"Acknowledgements"},{"location":"alignment/handout/handout/","text":"Key Learning Outcomes After completing this practical the trainee should be able to: Perform a simple NGS data alignment task, with Bowtie2, against one interested reference data Interpret and manipulate the mapping output using SAMtools Visualise the alignment via a standard genome browser, e.g. IGV browser Resources You\u2019ll be Using Tools Used Bowtie2: http://bowtie-bio.sourceforge.net/bowtie2/index.shtml Samtools: http://broadinstitute.github.io/picard BEDTools: http://code.google.com/p/bedtools/ UCSC tools: http://hgdownload.cse.ucsc.edu/admin/exe/ IGV genome browser: http://www.broadinstitute.org/igv/ Useful Links SAM Specification: http://samtools.sourceforge.net/SAM1.pdf Explain SAM Flags: https://broadinstitute.github.io/picard/explain-flags.html Sources of Data http://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-11431 Author Information Primary Author(s): Myrto Kostadima kostadim@ebi.ac.uk Contributor(s): Xi (Sean) Li sean.li@csiro.au Introduction The goal of this hands-on session is to perform an unspliced alignment for a small subset of raw reads. We will align raw sequencing data to the mouse genome using Bowtie2 and then we will manipulate the SAM output in order to visualize the alignment on the IGV browser. Prepare the Environment We will use one data set in this practical, which can be found in the ChIP-seq directory on your desktop. Open the Terminal. First, go to the right folder, where the data are stored. cd /home/trainee/chipseq The .fastq file that we will align is called Oct4.fastq . This file is based on Oct4 ChIP-seq data published by Chen et al. (2008). For the sake of time, we will align these reads to a single mouse chromosome. Alignment You already know that there are a number of competing tools for short read alignment, each with its own set of strengths, weaknesses, and caveats. Here we will try Bowtie2, a widely used ultrafast, memory efficient short read aligner. Bowtie2 has a number of parameters in order to perform the alignment. To view them all type bowtie2 --help Bowtie2 uses indexed genome for the alignment in order to keep its memory footprint small. Because of time constraints we will build the index only for one chromosome of the mouse genome. For this we need the chromosome sequence in FASTA format. This is stored in a file named mm10 , under the subdirectory bowtie_index . STOP DO NOT run this command. This has already been run for you. ** bowtie2-build bowtie_index/mm10.fa bowtie_index/mm10 ** This command will output 6 files that constitute the index. These files that have the prefix mm10 are stored in the bowtie_index subdirectory. To view if they files have been successfully created type: ls -l bowtie_index Now that the genome is indexed we can move on to the actual alignment. The first argument for bowtie2 is the basename of the index for the genome to be searched; in our case this is mm10 . We also want to make sure that the output is in SAM format using the -S parameter. The last argument is the name of the FASTQ file. Align the Oct4 reads using Bowtie2: bowtie2 -x bowtie_index/mm10 -q Oct4.fastq &gt; Oct4.sam The above command outputs the alignment in SAM format and stores them in the file Oct4.sam . In general before you run Bowtie2, you have to know what quality encoding your FASTQ files are in. The available FASTQ encodings for bowtie are: \u2013phred33-quals : Input qualities are Phred+33 (default). \u2013phred64-quals : Input qualities are Phred+64 (same as \u2013solexa1.3-quals ). \u2013solexa-quals : Input qualities are from GA Pipeline ver. < 1.3. \u2013solexa1.3-quals : Input qualities are from GA Pipeline ver. >= 1.3. \u2013integer-quals : Qualities are given as space-separated integers (not ASCII). The FASTQ files we are working with are Sanger encoded (Phred+33), which is the default for Bowtie2. Bowtie2 will take 2-3 minutes to align the file. This is fast compared to other aligners which sacrifice some speed to obtain higher sensitivity. Look at the top 10 lines of the SAM file using head (record lines are wrapped). Then try the second command, note use arrow navigation and to exit type \u2019q\u2019. head Oct4.sam less -S Oct4.sam Question Can you distinguish between the header of the SAM format and the actual alignments? Answer Answer The header line starts with the letter \u2018@\u2019, i.e.: @HD VN:1.0 SO:unsorted @SQ SN:chr1 LN:195471971 @PG ID:Bowtie2 PN:bowtie2 VN:2.2.4 CL:\u201c/tools/bowtie2/bowtie2-default/bowtie2-align-s \u2013wrapper basic-0 -x bowtie_index/mm10 -q Oct4.fastq\u201d While, the actual alignments start with read id, i.e.: SRR002012.45 0 etc SRR002012.48 16 chr1 etc Question What kind of information does the header provide? Answer Answer @HD: Header line; VN: Format version; SO: the sort order of alignments. @SQ: Reference sequence information; SN: reference sequence name; LN: reference sequence length. @PG: Read group information; ID: Read group identifier; VN: Program version; CL: the command line that produces the alignment. Question To which chromosome are the reads mapped? Answer Answer Chromosome 1. Manipulate SAM output SAM files are rather big and when dealing with a high volume of NGS data, storage space can become an issue. As we have already seen, we can convert SAM to BAM files (their binary equivalent that are not human readable) that occupy much less space. Convert SAM to BAM using samtools view and store the output in the file Oct4.bam . You have to instruct samtools view that the input is in SAM format ( -S ), the output should be in BAM format ( -b ) and that you want the output to be stored in the file specified by the -o option: samtools view -bSo Oct4.bam Oct4.sam Compute summary stats for the Flag values associated with the alignments using: samtools flagstat Oct4.bam Visualize alignments in IGV IGV is a stand-alone genome browser. Please check their website ( http://www.broadinstitute.org/igv/ ) for all the formats that IGV can display. For our visualization purposes we will use the BAM and bigWig formats. When uploading a BAM file into the genome browser, the browser will look for the index of the BAM file in the same folder where the BAM files is. The index file should have the same name as the BAM file and the suffix .bai . Finally, to create the index of a BAM file you need to make sure that the file is sorted according to chromosomal coordinates. Sort alignments according to chromosomal position and store the result in the file with the prefix Oct4.sorted : samtools sort Oct4.bam -o Oct4.sorted.bam Index the sorted file. samtools index Oct4.sorted.bam The indexing will create a file called Oct4.sorted.bam.bai . Note that you don\u2019t have to specify the name of the index file when running samtools index , it simply appends a .bai suffix to the input BAM file. Another way to visualize the alignments is to convert the BAM file into a bigWig file. The bigWig format is for display of dense, continuous data and the data will be displayed as a graph. The resulting bigWig files are in an indexed binary format. The BAM to bigWig conversion takes place in two steps. Firstly, we convert the BAM file into a bedgraph, called Oct4.bedgraph , using the tool genomeCoverageBed from BEDTools. Then we convert the bedgraph into a bigWig binary file called Oct4.bw , using bedGraphToBigWig from the UCSC tools: genomeCoverageBed -bg -ibam Oct4.sorted.bam -g bowtie_index/mouse.mm10.genome &gt; Oct4.bedgraph bedGraphToBigWig Oct4.bedgraph bowtie_index/mouse.mm10.genome Oct4.bw Both of the commands above take as input a file called mouse.mm10.genome that is stored under the subdirectory bowtie_index . These genome files are tab-delimited and describe the size of the chromosomes for the organism of interest. When using the UCSC Genome Browser, Ensembl, or Galaxy, you typically indicate which species/genome build you are working with. The way you do this for BEDTools is to create a \u201cgenome\u201d file, which simply lists the names of the chromosomes (or scaffolds, etc.) and their size (in basepairs). BEDTools includes pre-defined genome files for human and mouse in the genomes subdirectory included in the BEDTools distribution. Now we will load the data into the IGV browser for visualization. In order to launch IGV double click on the IGV 2.3 icon on your Desktop. Ignore any warnings and when it opens you have to load the genome of interest. On the top left of your screen choose from the drop down menu Mouse (mm10) . If it doesn\u2019t appear in list, click More .. , type mm10 in the Filter section, choose the mouse genome and press OK. Then in order to load the desire files go to: File &gt; Load from File On the pop up window navigate to Desktop -> chipseq folder and select the file Oct4.sorted.bam . Repeat these steps in order to load Oct4.bw as well. Select chr1 from the drop down menu on the top left. Right click on the name of Oct4.bw and choose Maximum under the Windowing Function. Right click again and select Autoscale. In order to see the aligned reads of the BAM file, you need to zoom in to a specific region. For example, look for gene Lemd1 in the search box. Question What is the main difference between the visualization of BAM and bigWig files? Answer Answer The actual alignment of reads that stack to a particular region can be displayed using the information stored in a BAM format. The bigWig format is for display of dense, continuous data that will be displayed in the Genome Browser as a graph. Using the + button on the top right, zoom in to see more of the details of the alignments. Question What do you think the different colors mean? Answer Answer The different color represents four nucleotides, e.g. blue is Cytidine (C), red is Thymidine (T). Practice Makes Perfect! In the chipseq folder you will find the file gfp.fastq . Follow the above described analysis, from the bowtie2 alignment step, for this dataset as well. You will need these files for the ChIP-Seq module.","title":"Read Alignment"},{"location":"alignment/handout/handout/#key-learning-outcomes","text":"After completing this practical the trainee should be able to: Perform a simple NGS data alignment task, with Bowtie2, against one interested reference data Interpret and manipulate the mapping output using SAMtools Visualise the alignment via a standard genome browser, e.g. IGV browser","title":"Key Learning Outcomes"},{"location":"alignment/handout/handout/#resources-youll-be-using","text":"","title":"Resources You\u2019ll be Using"},{"location":"alignment/handout/handout/#tools-used","text":"Bowtie2: http://bowtie-bio.sourceforge.net/bowtie2/index.shtml Samtools: http://broadinstitute.github.io/picard BEDTools: http://code.google.com/p/bedtools/ UCSC tools: http://hgdownload.cse.ucsc.edu/admin/exe/ IGV genome browser: http://www.broadinstitute.org/igv/","title":"Tools Used"},{"location":"alignment/handout/handout/#useful-links","text":"SAM Specification: http://samtools.sourceforge.net/SAM1.pdf Explain SAM Flags: https://broadinstitute.github.io/picard/explain-flags.html","title":"Useful Links"},{"location":"alignment/handout/handout/#sources-of-data","text":"http://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-11431","title":"Sources of Data"},{"location":"alignment/handout/handout/#author-information","text":"Primary Author(s): Myrto Kostadima kostadim@ebi.ac.uk Contributor(s): Xi (Sean) Li sean.li@csiro.au","title":"Author Information"},{"location":"alignment/handout/handout/#introduction","text":"The goal of this hands-on session is to perform an unspliced alignment for a small subset of raw reads. We will align raw sequencing data to the mouse genome using Bowtie2 and then we will manipulate the SAM output in order to visualize the alignment on the IGV browser.","title":"Introduction"},{"location":"alignment/handout/handout/#prepare-the-environment","text":"We will use one data set in this practical, which can be found in the ChIP-seq directory on your desktop. Open the Terminal. First, go to the right folder, where the data are stored. cd /home/trainee/chipseq The .fastq file that we will align is called Oct4.fastq . This file is based on Oct4 ChIP-seq data published by Chen et al. (2008). For the sake of time, we will align these reads to a single mouse chromosome.","title":"Prepare the Environment"},{"location":"alignment/handout/handout/#alignment","text":"You already know that there are a number of competing tools for short read alignment, each with its own set of strengths, weaknesses, and caveats. Here we will try Bowtie2, a widely used ultrafast, memory efficient short read aligner. Bowtie2 has a number of parameters in order to perform the alignment. To view them all type bowtie2 --help Bowtie2 uses indexed genome for the alignment in order to keep its memory footprint small. Because of time constraints we will build the index only for one chromosome of the mouse genome. For this we need the chromosome sequence in FASTA format. This is stored in a file named mm10 , under the subdirectory bowtie_index . STOP DO NOT run this command. This has already been run for you. ** bowtie2-build bowtie_index/mm10.fa bowtie_index/mm10 ** This command will output 6 files that constitute the index. These files that have the prefix mm10 are stored in the bowtie_index subdirectory. To view if they files have been successfully created type: ls -l bowtie_index Now that the genome is indexed we can move on to the actual alignment. The first argument for bowtie2 is the basename of the index for the genome to be searched; in our case this is mm10 . We also want to make sure that the output is in SAM format using the -S parameter. The last argument is the name of the FASTQ file. Align the Oct4 reads using Bowtie2: bowtie2 -x bowtie_index/mm10 -q Oct4.fastq &gt; Oct4.sam The above command outputs the alignment in SAM format and stores them in the file Oct4.sam . In general before you run Bowtie2, you have to know what quality encoding your FASTQ files are in. The available FASTQ encodings for bowtie are: \u2013phred33-quals : Input qualities are Phred+33 (default). \u2013phred64-quals : Input qualities are Phred+64 (same as \u2013solexa1.3-quals ). \u2013solexa-quals : Input qualities are from GA Pipeline ver. < 1.3. \u2013solexa1.3-quals : Input qualities are from GA Pipeline ver. >= 1.3. \u2013integer-quals : Qualities are given as space-separated integers (not ASCII). The FASTQ files we are working with are Sanger encoded (Phred+33), which is the default for Bowtie2. Bowtie2 will take 2-3 minutes to align the file. This is fast compared to other aligners which sacrifice some speed to obtain higher sensitivity. Look at the top 10 lines of the SAM file using head (record lines are wrapped). Then try the second command, note use arrow navigation and to exit type \u2019q\u2019. head Oct4.sam less -S Oct4.sam Question Can you distinguish between the header of the SAM format and the actual alignments? Answer Answer The header line starts with the letter \u2018@\u2019, i.e.: @HD VN:1.0 SO:unsorted @SQ SN:chr1 LN:195471971 @PG ID:Bowtie2 PN:bowtie2 VN:2.2.4 CL:\u201c/tools/bowtie2/bowtie2-default/bowtie2-align-s \u2013wrapper basic-0 -x bowtie_index/mm10 -q Oct4.fastq\u201d While, the actual alignments start with read id, i.e.: SRR002012.45 0 etc SRR002012.48 16 chr1 etc Question What kind of information does the header provide? Answer Answer @HD: Header line; VN: Format version; SO: the sort order of alignments. @SQ: Reference sequence information; SN: reference sequence name; LN: reference sequence length. @PG: Read group information; ID: Read group identifier; VN: Program version; CL: the command line that produces the alignment. Question To which chromosome are the reads mapped? Answer Answer Chromosome 1.","title":"Alignment"},{"location":"alignment/handout/handout/#manipulate-sam-output","text":"SAM files are rather big and when dealing with a high volume of NGS data, storage space can become an issue. As we have already seen, we can convert SAM to BAM files (their binary equivalent that are not human readable) that occupy much less space. Convert SAM to BAM using samtools view and store the output in the file Oct4.bam . You have to instruct samtools view that the input is in SAM format ( -S ), the output should be in BAM format ( -b ) and that you want the output to be stored in the file specified by the -o option: samtools view -bSo Oct4.bam Oct4.sam Compute summary stats for the Flag values associated with the alignments using: samtools flagstat Oct4.bam","title":"Manipulate SAM output"},{"location":"alignment/handout/handout/#visualize-alignments-in-igv","text":"IGV is a stand-alone genome browser. Please check their website ( http://www.broadinstitute.org/igv/ ) for all the formats that IGV can display. For our visualization purposes we will use the BAM and bigWig formats. When uploading a BAM file into the genome browser, the browser will look for the index of the BAM file in the same folder where the BAM files is. The index file should have the same name as the BAM file and the suffix .bai . Finally, to create the index of a BAM file you need to make sure that the file is sorted according to chromosomal coordinates. Sort alignments according to chromosomal position and store the result in the file with the prefix Oct4.sorted : samtools sort Oct4.bam -o Oct4.sorted.bam Index the sorted file. samtools index Oct4.sorted.bam The indexing will create a file called Oct4.sorted.bam.bai . Note that you don\u2019t have to specify the name of the index file when running samtools index , it simply appends a .bai suffix to the input BAM file. Another way to visualize the alignments is to convert the BAM file into a bigWig file. The bigWig format is for display of dense, continuous data and the data will be displayed as a graph. The resulting bigWig files are in an indexed binary format. The BAM to bigWig conversion takes place in two steps. Firstly, we convert the BAM file into a bedgraph, called Oct4.bedgraph , using the tool genomeCoverageBed from BEDTools. Then we convert the bedgraph into a bigWig binary file called Oct4.bw , using bedGraphToBigWig from the UCSC tools: genomeCoverageBed -bg -ibam Oct4.sorted.bam -g bowtie_index/mouse.mm10.genome &gt; Oct4.bedgraph bedGraphToBigWig Oct4.bedgraph bowtie_index/mouse.mm10.genome Oct4.bw Both of the commands above take as input a file called mouse.mm10.genome that is stored under the subdirectory bowtie_index . These genome files are tab-delimited and describe the size of the chromosomes for the organism of interest. When using the UCSC Genome Browser, Ensembl, or Galaxy, you typically indicate which species/genome build you are working with. The way you do this for BEDTools is to create a \u201cgenome\u201d file, which simply lists the names of the chromosomes (or scaffolds, etc.) and their size (in basepairs). BEDTools includes pre-defined genome files for human and mouse in the genomes subdirectory included in the BEDTools distribution. Now we will load the data into the IGV browser for visualization. In order to launch IGV double click on the IGV 2.3 icon on your Desktop. Ignore any warnings and when it opens you have to load the genome of interest. On the top left of your screen choose from the drop down menu Mouse (mm10) . If it doesn\u2019t appear in list, click More .. , type mm10 in the Filter section, choose the mouse genome and press OK. Then in order to load the desire files go to: File &gt; Load from File On the pop up window navigate to Desktop -> chipseq folder and select the file Oct4.sorted.bam . Repeat these steps in order to load Oct4.bw as well. Select chr1 from the drop down menu on the top left. Right click on the name of Oct4.bw and choose Maximum under the Windowing Function. Right click again and select Autoscale. In order to see the aligned reads of the BAM file, you need to zoom in to a specific region. For example, look for gene Lemd1 in the search box. Question What is the main difference between the visualization of BAM and bigWig files? Answer Answer The actual alignment of reads that stack to a particular region can be displayed using the information stored in a BAM format. The bigWig format is for display of dense, continuous data that will be displayed in the Genome Browser as a graph. Using the + button on the top right, zoom in to see more of the details of the alignments. Question What do you think the different colors mean? Answer Answer The different color represents four nucleotides, e.g. blue is Cytidine (C), red is Thymidine (T).","title":"Visualize alignments in IGV"},{"location":"alignment/handout/handout/#practice-makes-perfect","text":"In the chipseq folder you will find the file gfp.fastq . Follow the above described analysis, from the bowtie2 alignment step, for this dataset as well. You will need these files for the ChIP-Seq module.","title":"Practice Makes Perfect!"},{"location":"alignment/handout/license/","text":"This work is licensed under a Creative Commons Attribution 3.0 Unported License and the below text is a summary of the main terms of the full Legal Code (the full license) available at http://creativecommons.org/licenses/by/3.0/legalcode . You are free: to copy, distribute, display, and perform the work to make derivative works to make commercial use of the work Under the following conditions: Attribution - You must give the original author credit. With the understanding that: Waiver - Any of the above conditions can be waived if you get permission from the copyright holder. Public Domain - Where the work or any of its elements is in the public domain under applicable law, that status is in no way affected by the license. Other Rights - In no way are any of the following rights affected by the license: Your fair dealing or fair use rights, or other applicable copyright exceptions and limitations; The author\u2019s moral rights; Rights other persons may have either in the work itself or in how the work is used, such as publicity or privacy rights. Notice - For any reuse or distribution, you must make clear to others the license terms of this work.","title":"License"},{"location":"cli/","text":"Introduction to Command Line Bioinformatics Training Platform (BTP) Module: Introduction to Command Line Topic Introduction to Command Line Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes Familiarise yourself with the command line environment on a Linux operating system. Run some basic linux system and file operation commands Navigation of biological data files structure and manipulation Time Required 1 hr License The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"Introduction to Command Line"},{"location":"cli/#introduction-to-command-line","text":"Bioinformatics Training Platform (BTP) Module: Introduction to Command Line Topic Introduction to Command Line Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes Familiarise yourself with the command line environment on a Linux operating system. Run some basic linux system and file operation commands Navigation of biological data files structure and manipulation Time Required 1 hr","title":"Introduction to Command Line"},{"location":"cli/#license","text":"The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"License"},{"location":"cli/datasets/","text":"Information about what datasets are required for a module\u2019s tutorial exercises are described in a data.yaml file. As well describing where to source the dataset files from, we also need to describe where on the filesystem the files should reside and who should own them.","title":"Home"},{"location":"cli/handout/","text":"Introduction to Command Line Acknowledgements This module was developed by Bioplatforms Australia, in partnership with CSIRO, and with support from the European Bioinformatics Institute (EBI), a member of the European Molecular Biology Laboratory (EMBL) in the UK. The module content has been maintained and updated by a network of dedicated [bioinformatics trainers] ( http://www.bioplatforms.com/bioinformatics-training/ ) from around Australia. The Bioinformatics Training Platform was developed in collaboration with the Monash Bioinformatics Platform. The development of this module is also supported by the NSW Office of Health and Medical Research and the Commonwealth Government National Collaborative Research Infrastructure Strategy (NCRIS).","title":"Introduction to Command Line"},{"location":"cli/handout/#introduction-to-command-line","text":"","title":"Introduction to Command Line"},{"location":"cli/handout/#acknowledgements","text":"This module was developed by Bioplatforms Australia, in partnership with CSIRO, and with support from the European Bioinformatics Institute (EBI), a member of the European Molecular Biology Laboratory (EMBL) in the UK. The module content has been maintained and updated by a network of dedicated [bioinformatics trainers] ( http://www.bioplatforms.com/bioinformatics-training/ ) from around Australia. The Bioinformatics Training Platform was developed in collaboration with the Monash Bioinformatics Platform. The development of this module is also supported by the NSW Office of Health and Medical Research and the Commonwealth Government National Collaborative Research Infrastructure Strategy (NCRIS).","title":"Acknowledgements"},{"location":"cli/handout/handout/","text":"Key Learning Outcomes After completing this practical the trainee should be able to: Familiarise yourself with the command line environment on a Linux operating system. Run some basic linux system and file operation commands Navigation of biological data files structure and manipulation Resources Tools Basic Linux system commands on an Ubuntu OS. Basic file operation commands Links Software Carpentry Example 1000Genome Project data Author Information Primary Author(s): Matt Field matt.field@anu.edu.au Shell Exercise Let\u2019s try out your new shell skills on some real data. The file 1000gp.vcf is a small sample (1%) of a very large text file containing human genetics data. Specifically, it describes genetic variation in three African individuals sequenced as part of the 1000 Genomes Project . The \u2019vcf\u2019 extension lets us know that it\u2019s in a specific text format, namely \u2019Variant Call Format\u2019. The file starts with a bunch of comment lines (they start with \u2019#\u2019 or \u2019##\u2019), and then a large number of data lines. This VCF file lists the differences between the three African individuals and a standard \u2019individual\u2019 called the reference (actually based upon a few different people). Each line in the file corresponds to a difference. The line tells us the position of the difference (chromosome and position), the genetic sequence in the reference, and the corresponding sequence in each of the three Africans. Before we start processing the file, let\u2019s get a high-level view of the file that we\u2019re about to work with. Open the Terminal and go to the directory where the data are stored: cd /home/trainee/cli ls pwd ls -lh 1000gp.vcf wc -l 1000gp.vcf Question What is the file size (in kilo-bytes), and how many lines are in the file?. Hint man ls , man wc Answer 3.6M 45034 lines Because this file is so large, you\u2019re going to almost always want to pipe (\u2018|\u2019) the result of any command to less (a simple text viewer, type q to exit) or head (to print the first 10 lines) so that you don\u2019t accidentally print 45,000 lines to the screen. Let\u2019s start by printing the first 5 lines to see what it looks like. head -5 1000gp.vcf That isn\u2019t very interesting; it\u2019s just a bunch of the comments at the beginning of the file (they all start with # )! Print the first 20 lines to see more of the file. head -20 1000gp.vcf Okay, so now we can see the basic structure of the file. A few comment lines that start with \u2019#\u2019 or \u2019##\u2019 and then a bunch of lines of data that contain all the data and are pretty hard to understand. Each line of data contains the same number of fields, and all fields are separated with TABs. These fields are: the chromosome (which volume the difference is in) the position (which character in the volume the difference starts at) the ID of the difference the sequence in the reference human(s) The rest of the columns tell us, in a rather complex way, a bunch of additional information about that position, including: the predicted sequence for each of the three Africans and how confident the scientists are that these sequences are correct. To start analyzing the actual data, we have to remove the header. Question How can we print the first 10 non-header lines (those that don\u2019t start with a \u2019#\u2019)? Hint man grep (remember to use pipes \u2018|\u2019) Answer grep -v \"^#\" 1000gp.vcf | head This is an advanced section. Question How many lines of data are in the file (rather than counting the number of header lines and subtracting, try just counting the number of data lines)? Answer grep -v \"^#\" 1000gp.vcf | wc -l (should print 45024) Where these differences are located can be important. If all the differences between two encyclopedias were in just the first volume, that would be interesting. The first field of each data line is the name of the chromosome that the difference occurs on (which volume we\u2019re on). Question Print the first 10 chromosomes, one per line. Hint man cut (remember to remove header lines first) Answer grep -v \"^#\" 1000gp.vcf | cut -f 1 | head As you should have observed, the first 10 lines are on numbered chromosomes. Every normal cell in your body has 23 pairs of chromosomes, 22 pairs of \u2018autosomal\u2019 chromosomes (these are numbered 1-22) and a pair of sex chromosomes (two Xs if you\u2019re female, an X and a Y if you\u2019re male). Let\u2019s look at which chromosomes these variations are on. Question Print a list of the chromosomes that are in the file (each chromosome name should only be printed once, so you should only print 23 lines). Hint Remove all duplicates from your previous answer ( man sort ) Answer grep -v \"^#\" 1000gp.vcf | cut -f 1 | sort -u Rather than using sort to print unique results, a common pipeline is to first sort and then pipe to another UNIX command, uniq . The uniq command takes sorted input and prints only unique lines, but it provides more flexibility than just using sort by itself. Keep in mind, if the input isn\u2019t sorted, uniq won\u2019t work properly. Question Using sort and uniq , print the number of times each chromosome occurs in the file. Hint man uniq Answer grep -v \"^#\" 1000gp.vcf | cut -f 1 | sort | uniq -c Question Add to your previous solution to list the chromosomes from most frequently observed to least frequently observed. Hint Make sure you\u2019re sorting in descending order. By default, sort sorts in ascending order. Answer grep -v \"^#\" 1000gp.vcf | cut -f 1 | sort | uniq -c | sort -n -r This is great, but biologists might also like to see the chromosomes ordered by their number (not dictionary order), since different chromosomes have different attributes and this ordering allows them to find a specific chromosome more easily. Question Sort the previous output by chromosome number Hint A lot of the power of sort comes from the fact that you can specify which fields to sort on, and the order in which to sort them. In this case you only need to sort on one field. Answer grep -v \"^#\" 1000gp.vcf | cut -f 1 | sort | uniq -c | sort -k 2n","title":"Introduction to Command Line"},{"location":"cli/handout/handout/#key-learning-outcomes","text":"After completing this practical the trainee should be able to: Familiarise yourself with the command line environment on a Linux operating system. Run some basic linux system and file operation commands Navigation of biological data files structure and manipulation","title":"Key Learning Outcomes"},{"location":"cli/handout/handout/#resources","text":"","title":"Resources"},{"location":"cli/handout/handout/#tools","text":"Basic Linux system commands on an Ubuntu OS. Basic file operation commands","title":"Tools"},{"location":"cli/handout/handout/#links","text":"Software Carpentry Example 1000Genome Project data","title":"Links"},{"location":"cli/handout/handout/#author-information","text":"Primary Author(s): Matt Field matt.field@anu.edu.au","title":"Author Information"},{"location":"cli/handout/handout/#shell-exercise","text":"Let\u2019s try out your new shell skills on some real data. The file 1000gp.vcf is a small sample (1%) of a very large text file containing human genetics data. Specifically, it describes genetic variation in three African individuals sequenced as part of the 1000 Genomes Project . The \u2019vcf\u2019 extension lets us know that it\u2019s in a specific text format, namely \u2019Variant Call Format\u2019. The file starts with a bunch of comment lines (they start with \u2019#\u2019 or \u2019##\u2019), and then a large number of data lines. This VCF file lists the differences between the three African individuals and a standard \u2019individual\u2019 called the reference (actually based upon a few different people). Each line in the file corresponds to a difference. The line tells us the position of the difference (chromosome and position), the genetic sequence in the reference, and the corresponding sequence in each of the three Africans. Before we start processing the file, let\u2019s get a high-level view of the file that we\u2019re about to work with. Open the Terminal and go to the directory where the data are stored: cd /home/trainee/cli ls pwd ls -lh 1000gp.vcf wc -l 1000gp.vcf Question What is the file size (in kilo-bytes), and how many lines are in the file?. Hint man ls , man wc Answer 3.6M 45034 lines Because this file is so large, you\u2019re going to almost always want to pipe (\u2018|\u2019) the result of any command to less (a simple text viewer, type q to exit) or head (to print the first 10 lines) so that you don\u2019t accidentally print 45,000 lines to the screen. Let\u2019s start by printing the first 5 lines to see what it looks like. head -5 1000gp.vcf That isn\u2019t very interesting; it\u2019s just a bunch of the comments at the beginning of the file (they all start with # )! Print the first 20 lines to see more of the file. head -20 1000gp.vcf Okay, so now we can see the basic structure of the file. A few comment lines that start with \u2019#\u2019 or \u2019##\u2019 and then a bunch of lines of data that contain all the data and are pretty hard to understand. Each line of data contains the same number of fields, and all fields are separated with TABs. These fields are: the chromosome (which volume the difference is in) the position (which character in the volume the difference starts at) the ID of the difference the sequence in the reference human(s) The rest of the columns tell us, in a rather complex way, a bunch of additional information about that position, including: the predicted sequence for each of the three Africans and how confident the scientists are that these sequences are correct. To start analyzing the actual data, we have to remove the header. Question How can we print the first 10 non-header lines (those that don\u2019t start with a \u2019#\u2019)? Hint man grep (remember to use pipes \u2018|\u2019) Answer grep -v \"^#\" 1000gp.vcf | head This is an advanced section. Question How many lines of data are in the file (rather than counting the number of header lines and subtracting, try just counting the number of data lines)? Answer grep -v \"^#\" 1000gp.vcf | wc -l (should print 45024) Where these differences are located can be important. If all the differences between two encyclopedias were in just the first volume, that would be interesting. The first field of each data line is the name of the chromosome that the difference occurs on (which volume we\u2019re on). Question Print the first 10 chromosomes, one per line. Hint man cut (remember to remove header lines first) Answer grep -v \"^#\" 1000gp.vcf | cut -f 1 | head As you should have observed, the first 10 lines are on numbered chromosomes. Every normal cell in your body has 23 pairs of chromosomes, 22 pairs of \u2018autosomal\u2019 chromosomes (these are numbered 1-22) and a pair of sex chromosomes (two Xs if you\u2019re female, an X and a Y if you\u2019re male). Let\u2019s look at which chromosomes these variations are on. Question Print a list of the chromosomes that are in the file (each chromosome name should only be printed once, so you should only print 23 lines). Hint Remove all duplicates from your previous answer ( man sort ) Answer grep -v \"^#\" 1000gp.vcf | cut -f 1 | sort -u Rather than using sort to print unique results, a common pipeline is to first sort and then pipe to another UNIX command, uniq . The uniq command takes sorted input and prints only unique lines, but it provides more flexibility than just using sort by itself. Keep in mind, if the input isn\u2019t sorted, uniq won\u2019t work properly. Question Using sort and uniq , print the number of times each chromosome occurs in the file. Hint man uniq Answer grep -v \"^#\" 1000gp.vcf | cut -f 1 | sort | uniq -c Question Add to your previous solution to list the chromosomes from most frequently observed to least frequently observed. Hint Make sure you\u2019re sorting in descending order. By default, sort sorts in ascending order. Answer grep -v \"^#\" 1000gp.vcf | cut -f 1 | sort | uniq -c | sort -n -r This is great, but biologists might also like to see the chromosomes ordered by their number (not dictionary order), since different chromosomes have different attributes and this ordering allows them to find a specific chromosome more easily. Question Sort the previous output by chromosome number Hint A lot of the power of sort comes from the fact that you can specify which fields to sort on, and the order in which to sort them. In this case you only need to sort on one field. Answer grep -v \"^#\" 1000gp.vcf | cut -f 1 | sort | uniq -c | sort -k 2n","title":"Shell Exercise"},{"location":"cli/handout/license/","text":"This work is licensed under a Creative Commons Attribution 3.0 Unported License and the below text is a summary of the main terms of the full Legal Code (the full license) available at http://creativecommons.org/licenses/by/3.0/legalcode . You are free: to copy, distribute, display, and perform the work to make derivative works to make commercial use of the work Under the following conditions: Attribution - You must give the original author credit. With the understanding that: Waiver - Any of the above conditions can be waived if you get permission from the copyright holder. Public Domain - Where the work or any of its elements is in the public domain under applicable law, that status is in no way affected by the license. Other Rights - In no way are any of the following rights affected by the license: Your fair dealing or fair use rights, or other applicable copyright exceptions and limitations; The author\u2019s moral rights; Rights other persons may have either in the work itself or in how the work is used, such as publicity or privacy rights. Notice - For any reuse or distribution, you must make clear to others the license terms of this work.","title":"License"},{"location":"cli/presentations/","text":"This directory is intended to store presentation files used to provide context to the hands-on aspect of the module.","title":"Home"},{"location":"cli/tools/","text":"The tools used in the module\u2019s tutorial exercises are captured in a tools.yaml file. This file can then be used by a workshop deployment script to download and install the tools required by the module.","title":"Home"},{"location":"denovo/","text":"btp-module-velvet Bioinformatics Training Platform (BTP) Module: Velvet for de novo assembly Topic de novo genome assembly with Velvet Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes Compile velvet with appropriate compile-time parameters set for a specific analysis Be able to choose appropriate assembly parameters Assemble a set of single-ended reads Assemble a set of paired-end reads from a single insert-size library Be able to visualise an assembly in AMOS Hawkeye Understand the importance of using paired-end libraries in de novo genome assembly Time Required 1 day License The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"btp-module-velvet"},{"location":"denovo/#btp-module-velvet","text":"Bioinformatics Training Platform (BTP) Module: Velvet for de novo assembly Topic de novo genome assembly with Velvet Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes Compile velvet with appropriate compile-time parameters set for a specific analysis Be able to choose appropriate assembly parameters Assemble a set of single-ended reads Assemble a set of paired-end reads from a single insert-size library Be able to visualise an assembly in AMOS Hawkeye Understand the importance of using paired-end libraries in de novo genome assembly Time Required 1 day","title":"btp-module-velvet"},{"location":"denovo/#license","text":"The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"License"},{"location":"denovo/handout/handout/","text":"Key Learning Outcomes After completing this practical the trainee should be able to: Compile velvet with appropriate compile-time parameters set for a specific analysis Be able to choose appropriate assembly parameters Assemble a set of paired-end reads from a single insert-size library Be able to visualise an assembly in AMOS Hawkeye Resources You\u2019ll be Using Although we have provided you with an environment which contains all the tools and data you will be using in this module, you may like to know where we have sourced those tools and data from. Tools Used Velvet: http://www.ebi.ac.uk/~zerbino/velvet/ AMOS Hawkeye: http://apps.sourceforge.net/mediawiki/amos/index.php?title=Hawkeye gnx-tools: https://github.com/mh11/gnx-tools FastQC: http://www.bioinformatics.bbsrc.ac.uk/projects/fastqc/ R: http://www.r-project.org/ Sources of Data ftp://ftp.ensemblgenomes.org/pub/release-8/bacteria/fasta/Staphylococcus/s_aureus_mrsa252/dna/s_aureus_mrsa252.EB1_s_aureus_mrsa252.dna.chromosome.Chromosome.fa.gz http://www.ebi.ac.uk/ena/data/view/SRX008042 ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022852/SRR022852_1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022852/SRR022852_2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR023/SRR023408/SRR023408_1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR023/SRR023408/SRR023408_2.fastq.gz Introduction The aim of this module is to become familiar with performing de novo genome assembly using Velvet, a de Bruijn graph based assembler, on a variety of sequence data. Prepare the Environment The first exercise should get you a little more comfortable with the computer environment and the command line. First make sure that you are in the denovo working directory by typing: cd /home/trainee/denovo and making absolutely sure you\u2019re there by typing: pwd Now create sub-directories for this and the two other velvet practicals. All these directories will be made as sub-directories of a directory for the whole course called NGS. For this you can use the following commands: mkdir -p NGS/velvet/{part1,part2} The -p tells mkdir (make directory) to make any parent directories if they don\u2019t already exist. You could have created the above directories one-at-a-time by doing this instead: mkdir NGS mkdir NGS/velvet mkdir NGS/velvet/part1 mkdir NGS/velvet/part2 After creating the directories, examine the structure and move into the directory ready for the first velvet exercise by typing: ls -R NGS cd NGS/velvet/part1 pwd Downloading and Compiling Velvet For the duration of this workshop, all the software you require has been set up for you already. This might not be the case when you return to \u201creal life\u201d. Many of the programs you will need, including velvet, are quite easy to set up, it might be instructive to try a couple. Although you will be using the preinstalled version of velvet, it is useful to know how to compile velvet as some of the parameters you might like to control can only be set at compile time. You can find the latest version of velvet at: http://www.ebi.ac.uk/~zerbino/velvet/ You could go to this URL and download the latest velvet version, or equivalently, you could type the following, which will download, unpack, inspect, compile and execute your locally compiled version of velvet: cd /home/trainee/denovo/NGS/velvet/part1 pwd tar xzf /home/trainee/denovo/data/velvet_1.2.10.tgz ls -R cd velvet_1.2.10 make ./velveth The standout displayed to screen when \u2019make\u2019 runs may contain an error message but it is ignored Take a look at the executables you have created. They will be displayed as green by the command: ls --color=always The switch \u2013color , instructs that files be coloured according to their type. This is often the default but we are just being explicit. By specifying the value always , we ensure that colouring is always applied, even from a script. Have a look of the output the command produces and you will see that MAXKMERLENGTH=31 and CATEGORIES=2 parameters were passed into the compiler. This indicates that the default compilation was set for de Bruijn graph k-mers of maximum size 31 and to allow a maximum of just 2 read categories. You can override these, and other, default configuration choices using command line parameters. Assume, you want to run velvet with a k-mer length of 41 using 3 categories, velvet needs to be recompiled to enable this functionality by typing: make clean make MAXKMERLENGTH=41 CATEGORIES=3 ./velveth Discuss with the persons next to you the following questions: Question What are the consequences of the parameters you have given make for velvet? Answer MAXKMERLENGTH: increase the max k-mer length from 31 to 41 CATEGORIES: paired-end data require to be put into separate categories. By increasing this parameter from 2 to 3 allows you to process 3 paired / mate-pair libraries and unpaired data. Question Why does Velvet use k-mer 31 and 2 categories as default? Answer Possibly a number of reason: 1) odd number to avoid palindromes 2) The first reads were very short (20-40 bp) and there were hardly any paired-end data around so there was no need to allow for longer k-mer lengths / more categories. 3) For programmers: 31 bp get stored in 64 bits (using 2bit encoding) Question Should you get better results by using a longer k-mer length? Answer If you can achieve a good k-mer coverage - yes. Question What effect would the following compile-time parameters have on velvet: OPENMP=Y Answer Turn on multithreading Question LONGSEQUENCES=Y Answer Assembling reads / contigs longer than 32kb long Question BIGASSEMBLY=Y Answer Using more than 2.2 billion reads Question SINGLE_COV_CAT=Y Answer Merge all coverage statistics into a single variable - save memory For a further description of velvet compile and runtime parameters please see the velvet Manual: https://github.com/dzerbino/velvet/wiki/Manual Assembling Paired-end Reads using Velvet The use of paired-end data in de novo genome assembly results in better quality assemblies, particularly for larger, more complex genomes. In addition, paired-end constraint violation (expected distance and orientation of paired reads) can be used to identify misassemblies. If you are doing de novo assembly, pay the extra and get paired-ends: they\u2019re worth it! The data you will examine in this exercise is again from Staphylococcus aureus which has a genome of around 3MBases. The reads are Illumina paired end with an insert size of ~350 bp. The required data can be downloaded from the SRA. Specifically, the run data (SRR022852) from the SRA Sample SRS004748. The following exercise focuses on preparing the paired-end FASTQ files ready for Velvet, using Velvet in paired-end mode and comparing results with Velvet\u2019s \u2019auto\u2019 option. First move to the directory you made for this exercise and make a suitable named directory for the exercise: cd /home/trainee/denovo/NGS/velvet/part2 mkdir SRS004748 cd SRS004748 There is no need to download the read files, as they are already stored locally. You will simply create a symlink to this pre-downloaded data using the following commands: ln -s /home/trainee/denovo/data/SRR022852_?.fastq.gz ./ It is interesting to monitor the computer\u2019s resource utilisation, particularly memory. A simple way to do this is to open a second terminal and in it type: top top is a program that continually monitors all the processes running on your computer, showing the resources used by each. Leave this running and refer to it periodically throughout your Velvet analyses. Particularly if they are taking a long time or whenever your curiosity gets the better of you. You should find that as this practical progresses, memory usage will increase significantly. Now, back to the first terminal, you are ready to run velveth and velvetg . The reads are -shortPaired and for the first run you should not use any parameters for velvetg . From this point on, where it will be informative to time your runs. This is very easy to do, just prefix the command to run the program with the command time . This will cause UNIX to report how long the program took to complete its task. Set the two stages of velvet running, whilst you watch the memory usage as reported by top . Time the velvetg stage: velveth run_25 25 -fmtAuto -create_binary -shortPaired -separate SRR022852_1.fastq.gz SRR022852_2.fastq.gz time velvetg run_25 Question What does -fmtAuto and -create_binary do? (see help menu) Answer -fmtAuto tries to detect the correct format of the input files e.g. FASTA, FASTQ and whether they are compressed or not. -create_binary outputs sequences as a binary file. That means that velvetg can read the sequences from the binary file more quickly that from the original sequence files. Question Comment on the use of memory and CPU for velveth and velvetg ? Answer velveth uses only one CPU while velvetg uses all possible CPUs for some parts of the calculation. Question How long did velvetg take? Answer My own measurements are: real 1m8.877s; user 4m15.324s; sys 0m4.716s Next, after saving your contigs.fa file from being overwritten, set the cut-off parameters that you investigated in the previous exercise and rerun velvetg . time and monitor the use of resources as previously. Start with -cov_cutoff 16 thus: mv run_25/contigs.fa run_25/contigs.fa.0 time velvetg run_25 -cov_cutoff 16 Up until now, velvetg has ignored the paired-end information. Now try running velvetg with both -cov_cutoff 16 and -exp_cov 26 , but first save your contigs.fa file. By using -cov_cutoff and -exp_cov , velvetg tries to estimate the insert length, which you will see in the velvetg output. The command is, of course: mv run_25/contigs.fa run_25/contigs.fa.1 time velvetg run_25 -cov_cutoff 16 -exp_cov 26 Question Comment on the time required, use of memory and CPU for velvetg ? Answer Runtime is lower when velvet can reuse previously calculated data. By using -exp_cov , the memory usage increases. Question Which insert length does Velvet estimate? Answer Paired-end library 1 has length: 228, sample standard deviation: 26 Next try running velvetg in \u2018paired-end mode\u2018. This entails running velvetg specifying the insert length with the parameter -ins_length set to 350. Even though velvet estimates the insert length it is always advisable to check / provide the insert length manually as velvet can get the statistics wrong due to noise. Just in case, save your last version of contigs.fa . The commands are: mv run_25/contigs.fa run_25/contigs.fa.2 time velvetg run_25 -cov_cutoff 16 -exp_cov 26 -ins_length 350 mv run_25/contigs.fa run_25/contigs.fa.3 Question How fast was this run? Answer My own measurements are: real 0m29.792s; user 1m4.372s; sys 0m3.880s Take a look into the Log file. Question What is the N50 value for the velvetg runs using the switches Answer Base run: 19,510 bp -cov_cutoff 16 : 24,739 bp -cov_cutoff 16 -exp_cov 26 : 61,793 bp -cov_cutoff 16 -exp_cov 26 -ins_length 350 : n50 of 62,740 bp; max 194,649 bp; total 2,871,093 bp Try giving the -cov_cutoff and/or -exp_cov parameters the value auto . See the velvetg help to show you how. The information Velvet prints during running includes information about the values used (coverage cut-off or insert length) when using the auto option. Question What coverage values does Velvet choose (hint: look at the output that Velvet produces while running)? Answer Median coverage depth = 26.021837 Removing contigs with coverage < 13.010918 \u2026 Question How does the N50 value change? Answer n50 of 68,843 bp; max 194,645 bp; total 2,872,678 bp Run gnx on all the contig.fa files you have generated in the course of this exercise. The command will be: gnx -min 100 -nx 25,50,75 run_25/contigs.fa* Question For which runs are there Ns in the contigs.fa file and why? Answer contigs.fa.2, contigs.fa.3, contigs.fa Velvet tries to use the provided (or infers) the insert length and fills ambiguous regions with Ns. Comment on the number of contigs and total length generated for each run. Filename No. contigs Total length No. Ns Contigs.fa.0 631 2,830,659 0 Contigs.fa.1 580 2,832,670 0 Contigs.fa.2 166 2,849,919 4,847 Contigs.fa.3 166 2,856,795 11,713 Contigs.fa 163 2,857,439 11,526 AMOS Hawkeye We will now output the assembly in the AMOS massage format and visualise the assembly using AMOS Hawkeye. Run velvetg with appropriate arguments and output the AMOS message file, then convert it to an AMOS bank and open it in Hawkeye: time velvetg run_25 -cov_cutoff 16 -exp_cov 26 -ins_length 350 -amos_file yes -read_trkg yes time bank-transact -c -b run_25/velvet_asm.bnk -m run_25/velvet_asm.afg hawkeye run_25/velvet_asm.bnk Looking at the scaffold view of a contig, comment on the proportion of \u201chappy mates\u201d to \u201ccompressed mates\u201d and \u201cstretched mates\u201d. Nearly all mates are compressed with no stretched mates and very few happy mates. Question What is the mean and standard deviation of the insert size reported under the Libraries tab? Answer Mean: 350 bp SD: 35 bp Question Look at the actual distribution of insert sizes for this library. Can you explain where there is a difference between the mean and SD reported in those two places? Answer We specified -ins_length 350 to the velvetg command. Velvet uses this value, in the AMOS message file that it outputs, rather than its own estimate. You can get AMOS to re-estimate the mean and SD of insert sizes using intra-contig pairs. First, close Hawkeye and then run the following commands before reopening the AMOS bank to see what has changed. asmQC -b run_25/velvet_asm.bnk -scaff -recompute -update -numsd 2 hawkeye run_25/velvet_asm.bnk Looking at the scaffold view of a contig, comment on the proportion of \u201chappy mates\u201d to \u201ccompressed mates\u201d and \u201cstretched mates\u201d. There are only a few compressed and stretched mates compared to happy mates. There are similar numbers of stretched and compressed mates. Question What is the mean and standard deviation of the insert size reported under the Libraries tab? Answer Mean: 226 bp SD: 25 bp Question Look at the actual distribution of insert sizes for this library. Does the mean and SD reported in both places now match? Answer Yes Question Can you find a region with an unusually high proportion of stretched, compressed, incorrectly orientated or linking mates? What might this situation indicate? Answer This would indicate a possible misassembly and worthy of further investigation. Look at the largest scaffold, there are stacks of stretched pairs which span contig boundaries. This indicates that the gap size has been underestimated during the scaffolding phase. Velvet and Data Quality So far we have used the raw read data without performing any quality control or read trimming prior to doing our velvet assemblies. Velvet does not use quality information present in FASTQ files. For this reason, it is vitally important to perform read QC and quality trimming. In doing so, we remove errors/noise from the dataset which in turn means velvet will run faster, will use less memory and will produce a better assembly. Assuming we haven\u2019t compromised too much on coverage. To investigate the effect of data quality, we will use the run data (SRR023408) from the SRA experiment SRX008042. The reads are Illumina paired end with an insert size of 92 bp. Go back to the main directory for this exercise and create and enter a new directory dedicated to this phase of the exercise. The commands are: cd /home/trainee/denovo/NGS/velvet/part2 mkdir SRX008042 cd SRX008042 Create symlinks to the read data files that we downloaded for you from the SRA: ln -s /home/trainee/denovo/data/SRR023408_?.fastq.gz ./ We will use FastQC, a tool you should be familiar with, to visualise the quality of our data. We will use FastQC in the Graphical User Interface (GUI) mode. Start FastQC and set the process running in the background, by using a trailing & , so we get control of our terminal back for entering more commands: fastqc &amp; Open the two compressed FASTQ files (File -> Open) by selecting them both and clicking OK). Look at tabs for both files: Question Are the quality scores the same for both files? Answer Overall yes Question Which value varies? Answer Per sequence quality scores Question Take a look at the Per base sequence quality for both files. Did you note that it is not good for either file? Answer The quality score of both files drop very fast. Qualities of the REV strand drop faster than the FWD strand. This is because the template has been sat around while the FWD strand was sequenced. Question At which positions would you cut the reads if we did \u201cfixed length trimming\u201d? Answer Looking at the \u201cPer base quality\u201d and \u201cPer base sequence content\u201d, I would choose around 27 Question Why does the quality deteriorate towards the end of the read? Answer Errors more likely for later cycles Question Does it make sense to trim the 5\u2019 start of reads? Answer Looking at the \u201cPer base sequence content\u201d, yes - there is a clear signal at the beginning. Have a look at the other options that FastQC offers. Question Which other statistics could you use to support your trimming strategy? Answer \u201cPer base sequence content\u201d, \u201cPer base GC content\u201d, \u201cKmer content\u201d, \u201cPer base sequence quality\u201d Once you have decided what your trim points will be, close FastQC. We will use fastx_trimmer from the FASTX-Toolkit to perform fixed-length trimming. For usage information see the help: fastx_trimmer -h fastx_trimmer is not able to read compressed FASTQ files, so we first need to decompress the files ready for input. The suggestion (hopefully not far from your own thoughts?) is that you trim your reads as follows: gunzip &lt; SRR023408_1.fastq.gz &gt; SRR023408_1.fastq gunzip &lt; SRR023408_2.fastq.gz &gt; SRR023408_2.fastq fastx_trimmer -Q 33 -f 1 -l 32 -i SRR023408_1.fastq -o SRR023408_trim1.fastq fastx_trimmer -Q 33 -f 1 -l 27 -i SRR023408_2.fastq -o SRR023408_trim2.fastq Many NGS read files are large. This means that simply reading and writing files can become the bottleneck, also known as I/O bound. Therefore, it is often good practice to avoid unnecessary disk read/write. Tips We could do what is called pipelining to send a stream of data from one command to another, using the pipe ( | ) character, without the need for intermediary files. The following command would achieve this: gunzip \u2013to-stdout < SRR023408_1.fastq.gz | fastx_trimmer -Q 33 -f 4 -l 32 -o SRR023408_trim1.fastq gunzip \u2013to-stdout < SRR023408_2.fastq.gz | fastx_trimmer -Q 33 -f 3 -l 29 -o SRR023408_trim2.fastq Now run velveth with a k-mer value of 21 for both the untrimmed and trimmed read files in -shortPaired mode. Separate the output of the two executions of velveth into suitably named directories, followed by velvetg : # untrimmed reads velveth run_21 21 -fmtAuto -create_binary -shortPaired -separate SRR023408_1.fastq SRR023408_2.fastq time velvetg run_21 # trimmed reads velveth run_21trim 21 -fmtAuto -create_binary -shortPaired -separate SRR023408_trim1.fastq SRR023408_trim2.fastq time velvetg run_21trim Question How long did the two velvetg runs take? Answer run_25: real 3m16.132s; user 8m18.261s; sys 0m7.317s run_25trim: real 1m18.611s; user 3m53.140s; sys 0m4.962s Question What N50 scores did you achieve? Answer Untrimmed: 11 Trimmed: 15 Question What were the overall effects of trimming? Answer Time saving, increased N50, reduced coverage The evidence is that trimming improved the assembly. The thing to do surely, is to run velvetg with the -cov_cutoff and -exp_cov . In order to use -cov_cutoff and -exp_cov sensibly, you need to investigate with R, as you did in the previous exercise, what parameter values to use. Start up R and produce the weighted histograms: R -- no - save library ( plotrix ) data <- read.table ( \"run_21/stats.txt\" , header = TRUE ) data2 <- read.table ( \"run_21trim/stats.txt\" , header = TRUE ) par ( mfrow = c ( 1 , 2 )) weighted.hist ( data $ short1_cov , data $ lgth , breaks = 0 : 50 ) weighted.hist ( data2 $ short1_cov , data2 $ lgth , breaks = 0 : 50 ) Weighted k-mer coverage histograms of the paired-end reads pre-trimmed (left) and post-trimmed (right). For the untrimmed read histogram (left) there is an expected coverage of around 13 with a coverage cut-off of around 7. For the trimmed read histogram (right) there is an expected coverage of around 9 with a coverage cut-off of around 5. If you disagree, feel free to try different settings, but first quit R before running velvetg : q() time velvetg run_21 -cov_cutoff 7 -exp_cov 13 -ins_length 92 time velvetg run_21trim -cov_cutoff 5 -exp_cov 9 -ins_length 92 Question How good does it look now? Answer Still not great Runtime: Reduced runtime Memory: Lower memory usage Question K-mer choice (Can you use k-mer 31 for a read of length 30 bp?) Answer K-mer has to be lower than the read length and the K-mer coverage should be sufficient to produce results. Question Does less data mean \u201cworse\u201d results? Answer Not necessarily. If you have lots of data you can safely remove poor data without too much impact on overall coverage. Compare the results, produced during the last exercises, with each other, Metric SRR023408 SRR023408.trimmed Overall Quality (1-5) 5 4 bp Coverage 95x (37bp; 7761796) 82x (32bp; 7761796) k-mer Coverage 43x (21); 33x (25) 30x (21); 20.5x (25) N50 (k-mer used) 2,803 (21) 2,914 (21) Question What would you consider as the \u201cbest\u201d assembly? Answer SRR023408.trimmed Question If you found a candidate, why do you consider it as \u201cbest\u201d assembly? Answer Overall data quality and coverage Question How else might you assess the the quality of an assembly? Hint Hawkeye Answer By trying to identify paired-end constraint violations using AMOS Hawkeye.","title":"De Novo Genome Assembly"},{"location":"denovo/handout/handout/#key-learning-outcomes","text":"After completing this practical the trainee should be able to: Compile velvet with appropriate compile-time parameters set for a specific analysis Be able to choose appropriate assembly parameters Assemble a set of paired-end reads from a single insert-size library Be able to visualise an assembly in AMOS Hawkeye","title":"Key Learning Outcomes"},{"location":"denovo/handout/handout/#resources-youll-be-using","text":"Although we have provided you with an environment which contains all the tools and data you will be using in this module, you may like to know where we have sourced those tools and data from.","title":"Resources You\u2019ll be Using"},{"location":"denovo/handout/handout/#tools-used","text":"Velvet: http://www.ebi.ac.uk/~zerbino/velvet/ AMOS Hawkeye: http://apps.sourceforge.net/mediawiki/amos/index.php?title=Hawkeye gnx-tools: https://github.com/mh11/gnx-tools FastQC: http://www.bioinformatics.bbsrc.ac.uk/projects/fastqc/ R: http://www.r-project.org/","title":"Tools Used"},{"location":"denovo/handout/handout/#sources-of-data","text":"ftp://ftp.ensemblgenomes.org/pub/release-8/bacteria/fasta/Staphylococcus/s_aureus_mrsa252/dna/s_aureus_mrsa252.EB1_s_aureus_mrsa252.dna.chromosome.Chromosome.fa.gz http://www.ebi.ac.uk/ena/data/view/SRX008042 ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022852/SRR022852_1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022852/SRR022852_2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR023/SRR023408/SRR023408_1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR023/SRR023408/SRR023408_2.fastq.gz","title":"Sources of Data"},{"location":"denovo/handout/handout/#introduction","text":"The aim of this module is to become familiar with performing de novo genome assembly using Velvet, a de Bruijn graph based assembler, on a variety of sequence data.","title":"Introduction"},{"location":"denovo/handout/handout/#prepare-the-environment","text":"The first exercise should get you a little more comfortable with the computer environment and the command line. First make sure that you are in the denovo working directory by typing: cd /home/trainee/denovo and making absolutely sure you\u2019re there by typing: pwd Now create sub-directories for this and the two other velvet practicals. All these directories will be made as sub-directories of a directory for the whole course called NGS. For this you can use the following commands: mkdir -p NGS/velvet/{part1,part2} The -p tells mkdir (make directory) to make any parent directories if they don\u2019t already exist. You could have created the above directories one-at-a-time by doing this instead: mkdir NGS mkdir NGS/velvet mkdir NGS/velvet/part1 mkdir NGS/velvet/part2 After creating the directories, examine the structure and move into the directory ready for the first velvet exercise by typing: ls -R NGS cd NGS/velvet/part1 pwd","title":"Prepare the Environment"},{"location":"denovo/handout/handout/#downloading-and-compiling-velvet","text":"For the duration of this workshop, all the software you require has been set up for you already. This might not be the case when you return to \u201creal life\u201d. Many of the programs you will need, including velvet, are quite easy to set up, it might be instructive to try a couple. Although you will be using the preinstalled version of velvet, it is useful to know how to compile velvet as some of the parameters you might like to control can only be set at compile time. You can find the latest version of velvet at: http://www.ebi.ac.uk/~zerbino/velvet/ You could go to this URL and download the latest velvet version, or equivalently, you could type the following, which will download, unpack, inspect, compile and execute your locally compiled version of velvet: cd /home/trainee/denovo/NGS/velvet/part1 pwd tar xzf /home/trainee/denovo/data/velvet_1.2.10.tgz ls -R cd velvet_1.2.10 make ./velveth The standout displayed to screen when \u2019make\u2019 runs may contain an error message but it is ignored Take a look at the executables you have created. They will be displayed as green by the command: ls --color=always The switch \u2013color , instructs that files be coloured according to their type. This is often the default but we are just being explicit. By specifying the value always , we ensure that colouring is always applied, even from a script. Have a look of the output the command produces and you will see that MAXKMERLENGTH=31 and CATEGORIES=2 parameters were passed into the compiler. This indicates that the default compilation was set for de Bruijn graph k-mers of maximum size 31 and to allow a maximum of just 2 read categories. You can override these, and other, default configuration choices using command line parameters. Assume, you want to run velvet with a k-mer length of 41 using 3 categories, velvet needs to be recompiled to enable this functionality by typing: make clean make MAXKMERLENGTH=41 CATEGORIES=3 ./velveth Discuss with the persons next to you the following questions: Question What are the consequences of the parameters you have given make for velvet? Answer MAXKMERLENGTH: increase the max k-mer length from 31 to 41 CATEGORIES: paired-end data require to be put into separate categories. By increasing this parameter from 2 to 3 allows you to process 3 paired / mate-pair libraries and unpaired data. Question Why does Velvet use k-mer 31 and 2 categories as default? Answer Possibly a number of reason: 1) odd number to avoid palindromes 2) The first reads were very short (20-40 bp) and there were hardly any paired-end data around so there was no need to allow for longer k-mer lengths / more categories. 3) For programmers: 31 bp get stored in 64 bits (using 2bit encoding) Question Should you get better results by using a longer k-mer length? Answer If you can achieve a good k-mer coverage - yes. Question What effect would the following compile-time parameters have on velvet: OPENMP=Y Answer Turn on multithreading Question LONGSEQUENCES=Y Answer Assembling reads / contigs longer than 32kb long Question BIGASSEMBLY=Y Answer Using more than 2.2 billion reads Question SINGLE_COV_CAT=Y Answer Merge all coverage statistics into a single variable - save memory For a further description of velvet compile and runtime parameters please see the velvet Manual: https://github.com/dzerbino/velvet/wiki/Manual","title":"Downloading and Compiling Velvet"},{"location":"denovo/handout/handout/#assembling-paired-end-reads-using-velvet","text":"The use of paired-end data in de novo genome assembly results in better quality assemblies, particularly for larger, more complex genomes. In addition, paired-end constraint violation (expected distance and orientation of paired reads) can be used to identify misassemblies. If you are doing de novo assembly, pay the extra and get paired-ends: they\u2019re worth it! The data you will examine in this exercise is again from Staphylococcus aureus which has a genome of around 3MBases. The reads are Illumina paired end with an insert size of ~350 bp. The required data can be downloaded from the SRA. Specifically, the run data (SRR022852) from the SRA Sample SRS004748. The following exercise focuses on preparing the paired-end FASTQ files ready for Velvet, using Velvet in paired-end mode and comparing results with Velvet\u2019s \u2019auto\u2019 option. First move to the directory you made for this exercise and make a suitable named directory for the exercise: cd /home/trainee/denovo/NGS/velvet/part2 mkdir SRS004748 cd SRS004748 There is no need to download the read files, as they are already stored locally. You will simply create a symlink to this pre-downloaded data using the following commands: ln -s /home/trainee/denovo/data/SRR022852_?.fastq.gz ./ It is interesting to monitor the computer\u2019s resource utilisation, particularly memory. A simple way to do this is to open a second terminal and in it type: top top is a program that continually monitors all the processes running on your computer, showing the resources used by each. Leave this running and refer to it periodically throughout your Velvet analyses. Particularly if they are taking a long time or whenever your curiosity gets the better of you. You should find that as this practical progresses, memory usage will increase significantly. Now, back to the first terminal, you are ready to run velveth and velvetg . The reads are -shortPaired and for the first run you should not use any parameters for velvetg . From this point on, where it will be informative to time your runs. This is very easy to do, just prefix the command to run the program with the command time . This will cause UNIX to report how long the program took to complete its task. Set the two stages of velvet running, whilst you watch the memory usage as reported by top . Time the velvetg stage: velveth run_25 25 -fmtAuto -create_binary -shortPaired -separate SRR022852_1.fastq.gz SRR022852_2.fastq.gz time velvetg run_25 Question What does -fmtAuto and -create_binary do? (see help menu) Answer -fmtAuto tries to detect the correct format of the input files e.g. FASTA, FASTQ and whether they are compressed or not. -create_binary outputs sequences as a binary file. That means that velvetg can read the sequences from the binary file more quickly that from the original sequence files. Question Comment on the use of memory and CPU for velveth and velvetg ? Answer velveth uses only one CPU while velvetg uses all possible CPUs for some parts of the calculation. Question How long did velvetg take? Answer My own measurements are: real 1m8.877s; user 4m15.324s; sys 0m4.716s Next, after saving your contigs.fa file from being overwritten, set the cut-off parameters that you investigated in the previous exercise and rerun velvetg . time and monitor the use of resources as previously. Start with -cov_cutoff 16 thus: mv run_25/contigs.fa run_25/contigs.fa.0 time velvetg run_25 -cov_cutoff 16 Up until now, velvetg has ignored the paired-end information. Now try running velvetg with both -cov_cutoff 16 and -exp_cov 26 , but first save your contigs.fa file. By using -cov_cutoff and -exp_cov , velvetg tries to estimate the insert length, which you will see in the velvetg output. The command is, of course: mv run_25/contigs.fa run_25/contigs.fa.1 time velvetg run_25 -cov_cutoff 16 -exp_cov 26 Question Comment on the time required, use of memory and CPU for velvetg ? Answer Runtime is lower when velvet can reuse previously calculated data. By using -exp_cov , the memory usage increases. Question Which insert length does Velvet estimate? Answer Paired-end library 1 has length: 228, sample standard deviation: 26 Next try running velvetg in \u2018paired-end mode\u2018. This entails running velvetg specifying the insert length with the parameter -ins_length set to 350. Even though velvet estimates the insert length it is always advisable to check / provide the insert length manually as velvet can get the statistics wrong due to noise. Just in case, save your last version of contigs.fa . The commands are: mv run_25/contigs.fa run_25/contigs.fa.2 time velvetg run_25 -cov_cutoff 16 -exp_cov 26 -ins_length 350 mv run_25/contigs.fa run_25/contigs.fa.3 Question How fast was this run? Answer My own measurements are: real 0m29.792s; user 1m4.372s; sys 0m3.880s Take a look into the Log file. Question What is the N50 value for the velvetg runs using the switches Answer Base run: 19,510 bp -cov_cutoff 16 : 24,739 bp -cov_cutoff 16 -exp_cov 26 : 61,793 bp -cov_cutoff 16 -exp_cov 26 -ins_length 350 : n50 of 62,740 bp; max 194,649 bp; total 2,871,093 bp Try giving the -cov_cutoff and/or -exp_cov parameters the value auto . See the velvetg help to show you how. The information Velvet prints during running includes information about the values used (coverage cut-off or insert length) when using the auto option. Question What coverage values does Velvet choose (hint: look at the output that Velvet produces while running)? Answer Median coverage depth = 26.021837 Removing contigs with coverage < 13.010918 \u2026 Question How does the N50 value change? Answer n50 of 68,843 bp; max 194,645 bp; total 2,872,678 bp Run gnx on all the contig.fa files you have generated in the course of this exercise. The command will be: gnx -min 100 -nx 25,50,75 run_25/contigs.fa* Question For which runs are there Ns in the contigs.fa file and why? Answer contigs.fa.2, contigs.fa.3, contigs.fa Velvet tries to use the provided (or infers) the insert length and fills ambiguous regions with Ns. Comment on the number of contigs and total length generated for each run. Filename No. contigs Total length No. Ns Contigs.fa.0 631 2,830,659 0 Contigs.fa.1 580 2,832,670 0 Contigs.fa.2 166 2,849,919 4,847 Contigs.fa.3 166 2,856,795 11,713 Contigs.fa 163 2,857,439 11,526","title":"Assembling Paired-end Reads using Velvet"},{"location":"denovo/handout/handout/#amos-hawkeye","text":"We will now output the assembly in the AMOS massage format and visualise the assembly using AMOS Hawkeye. Run velvetg with appropriate arguments and output the AMOS message file, then convert it to an AMOS bank and open it in Hawkeye: time velvetg run_25 -cov_cutoff 16 -exp_cov 26 -ins_length 350 -amos_file yes -read_trkg yes time bank-transact -c -b run_25/velvet_asm.bnk -m run_25/velvet_asm.afg hawkeye run_25/velvet_asm.bnk Looking at the scaffold view of a contig, comment on the proportion of \u201chappy mates\u201d to \u201ccompressed mates\u201d and \u201cstretched mates\u201d. Nearly all mates are compressed with no stretched mates and very few happy mates. Question What is the mean and standard deviation of the insert size reported under the Libraries tab? Answer Mean: 350 bp SD: 35 bp Question Look at the actual distribution of insert sizes for this library. Can you explain where there is a difference between the mean and SD reported in those two places? Answer We specified -ins_length 350 to the velvetg command. Velvet uses this value, in the AMOS message file that it outputs, rather than its own estimate. You can get AMOS to re-estimate the mean and SD of insert sizes using intra-contig pairs. First, close Hawkeye and then run the following commands before reopening the AMOS bank to see what has changed. asmQC -b run_25/velvet_asm.bnk -scaff -recompute -update -numsd 2 hawkeye run_25/velvet_asm.bnk Looking at the scaffold view of a contig, comment on the proportion of \u201chappy mates\u201d to \u201ccompressed mates\u201d and \u201cstretched mates\u201d. There are only a few compressed and stretched mates compared to happy mates. There are similar numbers of stretched and compressed mates. Question What is the mean and standard deviation of the insert size reported under the Libraries tab? Answer Mean: 226 bp SD: 25 bp Question Look at the actual distribution of insert sizes for this library. Does the mean and SD reported in both places now match? Answer Yes Question Can you find a region with an unusually high proportion of stretched, compressed, incorrectly orientated or linking mates? What might this situation indicate? Answer This would indicate a possible misassembly and worthy of further investigation. Look at the largest scaffold, there are stacks of stretched pairs which span contig boundaries. This indicates that the gap size has been underestimated during the scaffolding phase.","title":"AMOS Hawkeye"},{"location":"denovo/handout/handout/#velvet-and-data-quality","text":"So far we have used the raw read data without performing any quality control or read trimming prior to doing our velvet assemblies. Velvet does not use quality information present in FASTQ files. For this reason, it is vitally important to perform read QC and quality trimming. In doing so, we remove errors/noise from the dataset which in turn means velvet will run faster, will use less memory and will produce a better assembly. Assuming we haven\u2019t compromised too much on coverage. To investigate the effect of data quality, we will use the run data (SRR023408) from the SRA experiment SRX008042. The reads are Illumina paired end with an insert size of 92 bp. Go back to the main directory for this exercise and create and enter a new directory dedicated to this phase of the exercise. The commands are: cd /home/trainee/denovo/NGS/velvet/part2 mkdir SRX008042 cd SRX008042 Create symlinks to the read data files that we downloaded for you from the SRA: ln -s /home/trainee/denovo/data/SRR023408_?.fastq.gz ./ We will use FastQC, a tool you should be familiar with, to visualise the quality of our data. We will use FastQC in the Graphical User Interface (GUI) mode. Start FastQC and set the process running in the background, by using a trailing & , so we get control of our terminal back for entering more commands: fastqc &amp; Open the two compressed FASTQ files (File -> Open) by selecting them both and clicking OK). Look at tabs for both files: Question Are the quality scores the same for both files? Answer Overall yes Question Which value varies? Answer Per sequence quality scores Question Take a look at the Per base sequence quality for both files. Did you note that it is not good for either file? Answer The quality score of both files drop very fast. Qualities of the REV strand drop faster than the FWD strand. This is because the template has been sat around while the FWD strand was sequenced. Question At which positions would you cut the reads if we did \u201cfixed length trimming\u201d? Answer Looking at the \u201cPer base quality\u201d and \u201cPer base sequence content\u201d, I would choose around 27 Question Why does the quality deteriorate towards the end of the read? Answer Errors more likely for later cycles Question Does it make sense to trim the 5\u2019 start of reads? Answer Looking at the \u201cPer base sequence content\u201d, yes - there is a clear signal at the beginning. Have a look at the other options that FastQC offers. Question Which other statistics could you use to support your trimming strategy? Answer \u201cPer base sequence content\u201d, \u201cPer base GC content\u201d, \u201cKmer content\u201d, \u201cPer base sequence quality\u201d Once you have decided what your trim points will be, close FastQC. We will use fastx_trimmer from the FASTX-Toolkit to perform fixed-length trimming. For usage information see the help: fastx_trimmer -h fastx_trimmer is not able to read compressed FASTQ files, so we first need to decompress the files ready for input. The suggestion (hopefully not far from your own thoughts?) is that you trim your reads as follows: gunzip &lt; SRR023408_1.fastq.gz &gt; SRR023408_1.fastq gunzip &lt; SRR023408_2.fastq.gz &gt; SRR023408_2.fastq fastx_trimmer -Q 33 -f 1 -l 32 -i SRR023408_1.fastq -o SRR023408_trim1.fastq fastx_trimmer -Q 33 -f 1 -l 27 -i SRR023408_2.fastq -o SRR023408_trim2.fastq Many NGS read files are large. This means that simply reading and writing files can become the bottleneck, also known as I/O bound. Therefore, it is often good practice to avoid unnecessary disk read/write. Tips We could do what is called pipelining to send a stream of data from one command to another, using the pipe ( | ) character, without the need for intermediary files. The following command would achieve this: gunzip \u2013to-stdout < SRR023408_1.fastq.gz | fastx_trimmer -Q 33 -f 4 -l 32 -o SRR023408_trim1.fastq gunzip \u2013to-stdout < SRR023408_2.fastq.gz | fastx_trimmer -Q 33 -f 3 -l 29 -o SRR023408_trim2.fastq Now run velveth with a k-mer value of 21 for both the untrimmed and trimmed read files in -shortPaired mode. Separate the output of the two executions of velveth into suitably named directories, followed by velvetg : # untrimmed reads velveth run_21 21 -fmtAuto -create_binary -shortPaired -separate SRR023408_1.fastq SRR023408_2.fastq time velvetg run_21 # trimmed reads velveth run_21trim 21 -fmtAuto -create_binary -shortPaired -separate SRR023408_trim1.fastq SRR023408_trim2.fastq time velvetg run_21trim Question How long did the two velvetg runs take? Answer run_25: real 3m16.132s; user 8m18.261s; sys 0m7.317s run_25trim: real 1m18.611s; user 3m53.140s; sys 0m4.962s Question What N50 scores did you achieve? Answer Untrimmed: 11 Trimmed: 15 Question What were the overall effects of trimming? Answer Time saving, increased N50, reduced coverage The evidence is that trimming improved the assembly. The thing to do surely, is to run velvetg with the -cov_cutoff and -exp_cov . In order to use -cov_cutoff and -exp_cov sensibly, you need to investigate with R, as you did in the previous exercise, what parameter values to use. Start up R and produce the weighted histograms: R -- no - save library ( plotrix ) data <- read.table ( \"run_21/stats.txt\" , header = TRUE ) data2 <- read.table ( \"run_21trim/stats.txt\" , header = TRUE ) par ( mfrow = c ( 1 , 2 )) weighted.hist ( data $ short1_cov , data $ lgth , breaks = 0 : 50 ) weighted.hist ( data2 $ short1_cov , data2 $ lgth , breaks = 0 : 50 ) Weighted k-mer coverage histograms of the paired-end reads pre-trimmed (left) and post-trimmed (right). For the untrimmed read histogram (left) there is an expected coverage of around 13 with a coverage cut-off of around 7. For the trimmed read histogram (right) there is an expected coverage of around 9 with a coverage cut-off of around 5. If you disagree, feel free to try different settings, but first quit R before running velvetg : q() time velvetg run_21 -cov_cutoff 7 -exp_cov 13 -ins_length 92 time velvetg run_21trim -cov_cutoff 5 -exp_cov 9 -ins_length 92 Question How good does it look now? Answer Still not great Runtime: Reduced runtime Memory: Lower memory usage Question K-mer choice (Can you use k-mer 31 for a read of length 30 bp?) Answer K-mer has to be lower than the read length and the K-mer coverage should be sufficient to produce results. Question Does less data mean \u201cworse\u201d results? Answer Not necessarily. If you have lots of data you can safely remove poor data without too much impact on overall coverage. Compare the results, produced during the last exercises, with each other, Metric SRR023408 SRR023408.trimmed Overall Quality (1-5) 5 4 bp Coverage 95x (37bp; 7761796) 82x (32bp; 7761796) k-mer Coverage 43x (21); 33x (25) 30x (21); 20.5x (25) N50 (k-mer used) 2,803 (21) 2,914 (21) Question What would you consider as the \u201cbest\u201d assembly? Answer SRR023408.trimmed Question If you found a candidate, why do you consider it as \u201cbest\u201d assembly? Answer Overall data quality and coverage Question How else might you assess the the quality of an assembly? Hint Hawkeye Answer By trying to identify paired-end constraint violations using AMOS Hawkeye.","title":"Velvet and Data Quality"},{"location":"denovo/handout/paired_end/","text":"Assembling Paired-end Reads The use of paired-end data in de novo genome assembly results in better quality assemblies, particularly for larger, more complex genomes. In addition, paired-end constraint violation (expected distance and orientation of paired reads) can be used to identify misassemblies. If you are doing de novo assembly, pay the extra and get paired-ends: they\u2019re worth it! The data you will examine in this exercise is again from Staphylococcus aureus which has a genome of around 3MBases. The reads are Illumina paired end with an insert size of $~$350 bp. The required data can be downloaded from the SRA. Specifically, the run data (SRR022852) from the SRA Sample SRS004748. The following exercise focuses on preparing the paired-end FASTQ files ready for Velvet, using Velvet in paired-end mode and comparing results with Velvet\u2019s \u2019auto\u2019 option. First move to the directory you made for this exercise and make a suitable named directory for the exercise: cd /home/trainee/denovo/NGS/velvet/part2 mkdir SRS004748 cd SRS004748 There is no need to download the read files, as they are already stored locally. You will simply create a symlink to this pre-downloaded data using the following commands: ln -s /home/trainee/denovo/data/SRR022852_?.fastq.gz ./ It is interesting to monitor the computer\u2019s resource utilisation, particularly memory. A simple way to do this is to open a second terminal and in it type: top top is a program that continually monitors all the processes running on your computer, showing the resources used by each. Leave this running and refer to it periodically throughout your Velvet analyses. Particularly if they are taking a long time or whenever your curiosity gets the better of you. You should find that as this practical progresses, memory usage will increase significantly. Now, back to the first terminal, you are ready to run velveth and velvetg . The reads are -shortPaired and for the first run you should not use any parameters for velvetg . From this point on, where it will be informative to time your runs. This is very easy to do, just prefix the command to run the program with the command time . This will cause UNIX to report how long the program took to complete its task. Set the two stages of velvet running, whilst you watch the memory usage as reported by top . Time the velvetg stage: velveth run_25 25 -fmtAuto -create_binary -shortPaired -separate SRR022852_1.fastq.gz SRR022852_2.fastq.gz time velvetg run_25 What does -fmtAuto and -create_binary do? (see help menu) -fmtAuto tries to detect the correct format of the input files e.g. FASTA, FASTQ and whether they are compressed or not. -create_binary outputs sequences as a binary file. That means that velvetg can read the sequences from the binary file more quickly that from the original sequence files. Comment on the use of memory and CPU for velveth and velvetg ? velveth uses only one CPU while velvetg uses all possible CPUs for some parts of the calculation. How long did velvetg take? My own measurements are:\\ real 1m8.877s; user 4m15.324s; sys 0m4.716s Next, after saving your contigs.fa file from being overwritten, set the cut-off parameters that you investigated in the previous exercise and rerun velvetg . time and monitor the use of resources as previously. Start with -cov_cutoff 16 thus: mv run_25/contigs.fa run_25/contigs.fa.0 time velvetg run_25 -cov_cutoff 16 Up until now, velvetg has ignored the paired-end information. Now try running velvetg with both -cov_cutoff 16 and -exp_cov 26 , but first save your contigs.fa file. By using -cov_cutoff and -exp_cov , velvetg tries to estimate the insert length, which you will see in the velvetg output. The command is, of course: mv run_25/contigs.fa run_25/contigs.fa.1 time velvetg run_25 -cov_cutoff 16 -exp_cov 26 Comment on the time required, use of memory and CPU for velvetg ? Runtime is lower when velvet can reuse previously calculated data. By using -exp_cov , the memory usage increases. Which insert length does Velvet estimate? Paired-end library 1 has length: 228, sample standard deviation: 26 Next try running velvetg in \u2018paired-end mode\u2018. This entails running velvetg specifying the insert length with the parameter -ins_length set to 350. Even though velvet estimates the insert length it is always advisable to check / provide the insert length manually as velvet can get the statistics wrong due to noise. Just in case, save your last version of contigs.fa . The commands are: mv run_25/contigs.fa run_25/contigs.fa.2 time velvetg run_25 -cov_cutoff 16 -exp_cov 26 -ins_length 350 mv run_25/contigs.fa run_25/contigs.fa.3 How fast was this run? My own measurements are:\\ real 0m29.792s; user 1m4.372s; sys 0m3.880s Take a look into the Log file. What is the N50 value for the velvetg runs using the switches:\\ Base run: 19,510 bp -cov_cutoff 16 24,739 bp -cov_cutoff 16 -exp_cov 26 61,793 bp -cov_cutoff 16 -exp_cov 26 -ins_length 350 n50 of 62,740 bp; max 194,649 bp; total 2,871,093 bp Try giving the -cov_cutoff and/or -exp_cov parameters the value auto . See the velvetg help to show you how. The information Velvet prints during running includes information about the values used (coverage cut-off or insert length) when using the auto option. What coverage values does Velvet choose (hint: look at the output that Velvet produces while running)? Median coverage depth = 26.021837\\ Removing contigs with coverage $<$ 13.010918 \u2026 How does the N50 value change? n50 of 68,843 bp; max 194,645 bp; total 2,872,678 bp Run gnx on all the contig.fa files you have generated in the course of this exercise. The command will be: gnx -min 100 -nx 25,50,75 run_25/contigs.fa* For which runs are there Ns in the contigs.fa file and why? contigs.fa.2, contigs.fa.3, contigs.fa\\ Velvet tries to use the provided (or infers) the insert length and fills ambiguous regions with Ns. Comment on the number of contigs and total length generated for each run. Filename No. contigs Total length No. Ns Contigs.fa.0 631 2,830,659 0 Contigs.fa.1 580 2,832,670 0 Contigs.fa.2 166 2,849,919 4,847 Contigs.fa.3 166 2,856,795 11,713 Contigs.fa 163 2,857,439 11,526 : tab:velvetrunresults AMOS Hawkeye We will now output the assembly in the AMOS massage format and visualise the assembly using AMOS Hawkeye. Run velvetg with appropriate arguments and output the AMOS message file, then convert it to an AMOS bank and open it in Hawkeye: time velvetg run_25 -cov_cutoff 16 -exp_cov 26 -ins_length 350 -amos_file yes -read_trkg yes time bank-transact -c -b run_25/velvet_asm.bnk -m run_25/velvet_asm.afg hawkeye run_25/velvet_asm.bnk Looking at the scaffold view of a contig, comment on the proportion of \u201chappy mates\u201d to \u201ccompressed mates\u201d and \u201cstretched mates\u201d. Nearly all mates are compressed with no stretched mates and very few happy mates. What is the mean and standard deviation of the insert size reported under the Libraries tab? Mean: 350 bp SD: 35 bp Look at the actual distribution of insert sizes for this library. Can you explain where there is a difference between the mean and SD reported in those two places? We specified -ins_length 350 to the velvetg command. Velvet uses this value, in the AMOS message file that it outputs, rather than its own estimate. You can get AMOS to re-estimate the mean and SD of insert sizes using intra-contig pairs. First, close Hawkeye and then run the following commands before reopening the AMOS bank to see what has changed. asmQC -b run_25/velvet_asm.bnk -scaff -recompute -update -numsd 2 hawkeye run_25/velvet_asm.bnk Looking at the scaffold view of a contig, comment on the proportion of \u201chappy mates\u201d to \u201ccompressed mates\u201d and \u201cstretched mates\u201d. There are only a few compressed and stretched mates compared to happy mates. There are similar numbers of stretched and compressed mates. What is the mean and standard deviation of the insert size reported under the Libraries tab? TODO Mean: 226 bp SD: 25 bp Look at the actual distribution of insert sizes for this library. Does the mean and SD reported in both places now match? Yes Can you find a region with an unusually high proportion of stretched, compressed, incorrectly orientated or linking mates? What might this situation indicate? This would indicate a possible misassembly and worthy of further investigation. Look at the largest scaffold, there are stacks of stretched pairs which span contig boundaries. This indicates that the gap size has been underestimated during the scaffolding phase. Velvet and Data Quality So far we have used the raw read data without performing any quality control or read trimming prior to doing our velvet assemblies. Velvet does not use quality information present in FASTQ files. For this reason, it is vitally important to perform read QC and quality trimming. In doing so, we remove errors/noise from the dataset which in turn means velvet will run faster, will use less memory and will produce a better assembly. Assuming we haven\u2019t compromised too much on coverage. To investigate the effect of data quality, we will use the run data (SRR023408) from the SRA experiment SRX008042. The reads are Illumina paired end with an insert size of 92 bp. Go back to the main directory for this exercise and create and enter a new directory dedicated to this phase of the exercise. The commands are: cd /home/trainee/denovo/NGS/velvet/part2 mkdir SRX008042 cd SRX008042 Create symlinks to the read data files that we downloaded for you from the SRA: ln -s /home/trainee/denovo/data/SRR023408_?.fastq.gz ./ We will use FastQC, a tool you should be familiar with, to visualise the quality of our data. We will use FastQC in the Graphical User Interface (GUI) mode. Start FastQC and set the process running in the background, by using a trailing & , so we get control of our terminal back for entering more commands: fastqc &amp; Open the two compressed FASTQ files (File $->$ Open) by selecting them both and clicking OK). Look at tabs for both files: {width=\u201d80.00000%\u201d} Are the quality scores the same for both files? Overall yes Which value varies? Per sequence quality scores Take a look at the Per base sequence quality for both files. Did you note that it is not good for either file? The quality score of both files drop very fast. Qualities of the REV strand drop faster than the FWD strand. This is because the template has been sat around while the FWD strand was sequenced. At which positions would you cut the reads if we did \u201cfixed length trimming\u201d? Looking at the \u201cPer base quality\u201d and \u201cPer base sequence content\u201d, I would choose around 27 Why does the quality deteriorate towards the end of the read? Errors more likely for later cycles Does it make sense to trim the 5\u2019 start of reads? Looking at the \u201cPer base sequence content\u201d, yes - there is a clear signal at the beginning. Have a look at the other options that FastQC offers. Which other statistics could you use to support your trimming strategy? \u201cPer base sequence content\u201d, \u201cPer base GC content\u201d, \u201cKmer content\u201d, \u201cPer base sequence quality\u201d {width=\u201d80.00000%\u201d} Once you have decided what your trim points will be, close FastQC. We will use fastx_trimmer from the FASTX-Toolkit to perform fixed-length trimming. For usage information see the help: fastx_trimmer -h fastx_trimmer is not able to read compressed FASTQ files, so we first need to decompress the files ready for input. The suggestion (hopefully not far from your own thoughts?) is that you trim your reads as follows: gunzip &lt; SRR023408_1.fastq.gz &gt; SRR023408_1.fastq gunzip &lt; SRR023408_2.fastq.gz &gt; SRR023408_2.fastq fastx_trimmer -Q 33 -f 1 -l 32 -i SRR023408_1.fastq -o SRR023408_trim1.fastq fastx_trimmer -Q 33 -f 1 -l 27 -i SRR023408_2.fastq -o SRR023408_trim2.fastq Many NGS read files are large. This means that simply reading and writing files can become the bottleneck, also known as I/O bound. Therefore, it is often good practice to avoid unnecessary disk read/write. We could do what is called pipelining to send a stream of data from one command to another, using the pipe ( | ) character, without the need for intermediary files. The following command would achieve this: gunzip --to-stdout &lt; SRR023408_1.fastq.gz | fastx_trimmer -Q 33 -f 4 -l 32 -o SRR023408_trim1.fastq gunzip --to-stdout &lt; SRR023408_2.fastq.gz | fastx_trimmer -Q 33 -f 3 -l 29 -o SRR023408_trim2.fastq Now run velveth with a k-mer value of 21 for both the untrimmed and trimmed read files in -shortPaired mode. Separate the output of the two executions of velveth into suitably named directories, followed by velvetg : # untrimmed reads velveth run_21 21 -fmtAuto -create_binary -shortPaired -separate SRR023408_1.fastq SRR023408_2.fastq time velvetg run_21 # trimmed reads velveth run_21trim 21 -fmtAuto -create_binary -shortPaired -separate SRR023408_trim1.fastq SRR023408_trim2.fastq time velvetg run_21trim How long did the two velvetg runs take? run_25: real 3m16.132s; user 8m18.261s; sys 0m7.317s \\ run_25trim: real 1m18.611s; user 3m53.140s; sys 0m4.962s What N50 scores did you achieve? Untrimmed: 11\\ Trimmed: 15 What were the overall effects of trimming? Time saving, increased N50, reduced coverage The evidence is that trimming improved the assembly. The thing to do surely, is to run velvetg with the -cov_cutoff and -exp_cov . In order to use -cov_cutoff and -exp_cov sensibly, you need to investigate with R, as you did in the previous exercise, what parameter values to use. Start up R and produce the weighted histograms: ``` {style=\u201dR\u201d} R \u2013no-save library(plotrix) data <- read.table(\u201crun_21/stats.txt\u201d, header=TRUE) data2 <- read.table(\u201crun_21trim/stats.txt\u201d, header=TRUE) par(mfrow=c(1,2)) weighted.hist(data$short1_cov, data$lgth, breaks=0:50) weighted.hist(data2$short1_cov, data2$lgth, breaks=0:50) ![\\[fig:velvet\\_Rplot002\\] Weighted k-mer coverage histograms of the paired-end reads pre-trimmed (left) and post-trimmed (right).](handout/velvet/velvet_Rplot002.png){width=\"80.00000%\"} For the untrimmed read histogram (left) there is an expected coverage of around 13 with a coverage cut-off of around 7. For the trimmed read histogram (right) there is an expected coverage of around 9 with a coverage cut-off of around 5. If you disagree, feel free to try different settings, but first quit R before running `velvetg`: ``` {style=\"R\"} q() time velvetg run_21 -cov_cutoff 7 -exp_cov 13 -ins_length 92 time velvetg run_21trim -cov_cutoff 5 -exp_cov 9 -ins_length 92 How good does it look now?\\ Still not great Comment on:\\ Runtime Reduced runtime Memory Lower memory usage k-mer choice (Can you use k-mer 31 for a read of length 30 bp?) K-mer has to be lower than the read length and the K-mer coverage should be sufficient to produce results. Does less data mean \u201cworse\u201d results? Not necessarily. If you have lots of data you can safely remove poor data without too much impact on overall coverage. How would a smaller/larger k-mer size behave? Compare the results, produced during the last exercises, with each other: [0.9]{}[l|l|l|l]{} Metric & SRR022852 & SRR023408 & SRR023408.trimmed\\ Overall Quality (1-5) & & &\\ bp Coverage & & &\\ k-mer Coverage & & &\\ N50 (k-mer used) & & &\\ [0.9]{}[l|l|l|l]{} Metric & SRR022852 & SRR023408 & SRR023408.trimmed\\ Overall Quality (1-5) & 2 & 5 & 4\\ bp Coverage & 136 x (36 bp;11,374,488) & 95x (37bp; 7761796) & 82x (32bp; 7761796)\\ k-mer Coverage & 45x & 43x (21); 33x (25) & 30x (21); 20.5x (25)\\ N50 (k-mer used) & 68,843 (25) & 2,803 (21) & 2,914 (21)\\ What would you consider as the \u201cbest\u201d assembly? SRR022852 If you found a candidate, why do you consider it as \u201cbest\u201d assembly? Overall data quality and coverage How else might you assess the the quality of an assembly? Hint: Hawkeye. By trying to identify paired-end constraint violations using AMOS Hawkeye.","title":"Paired end"},{"location":"denovo/handout/paired_end/#assembling-paired-end-reads","text":"The use of paired-end data in de novo genome assembly results in better quality assemblies, particularly for larger, more complex genomes. In addition, paired-end constraint violation (expected distance and orientation of paired reads) can be used to identify misassemblies. If you are doing de novo assembly, pay the extra and get paired-ends: they\u2019re worth it! The data you will examine in this exercise is again from Staphylococcus aureus which has a genome of around 3MBases. The reads are Illumina paired end with an insert size of $~$350 bp. The required data can be downloaded from the SRA. Specifically, the run data (SRR022852) from the SRA Sample SRS004748. The following exercise focuses on preparing the paired-end FASTQ files ready for Velvet, using Velvet in paired-end mode and comparing results with Velvet\u2019s \u2019auto\u2019 option. First move to the directory you made for this exercise and make a suitable named directory for the exercise: cd /home/trainee/denovo/NGS/velvet/part2 mkdir SRS004748 cd SRS004748 There is no need to download the read files, as they are already stored locally. You will simply create a symlink to this pre-downloaded data using the following commands: ln -s /home/trainee/denovo/data/SRR022852_?.fastq.gz ./ It is interesting to monitor the computer\u2019s resource utilisation, particularly memory. A simple way to do this is to open a second terminal and in it type: top top is a program that continually monitors all the processes running on your computer, showing the resources used by each. Leave this running and refer to it periodically throughout your Velvet analyses. Particularly if they are taking a long time or whenever your curiosity gets the better of you. You should find that as this practical progresses, memory usage will increase significantly. Now, back to the first terminal, you are ready to run velveth and velvetg . The reads are -shortPaired and for the first run you should not use any parameters for velvetg . From this point on, where it will be informative to time your runs. This is very easy to do, just prefix the command to run the program with the command time . This will cause UNIX to report how long the program took to complete its task. Set the two stages of velvet running, whilst you watch the memory usage as reported by top . Time the velvetg stage: velveth run_25 25 -fmtAuto -create_binary -shortPaired -separate SRR022852_1.fastq.gz SRR022852_2.fastq.gz time velvetg run_25 What does -fmtAuto and -create_binary do? (see help menu) -fmtAuto tries to detect the correct format of the input files e.g. FASTA, FASTQ and whether they are compressed or not. -create_binary outputs sequences as a binary file. That means that velvetg can read the sequences from the binary file more quickly that from the original sequence files. Comment on the use of memory and CPU for velveth and velvetg ? velveth uses only one CPU while velvetg uses all possible CPUs for some parts of the calculation. How long did velvetg take? My own measurements are:\\ real 1m8.877s; user 4m15.324s; sys 0m4.716s Next, after saving your contigs.fa file from being overwritten, set the cut-off parameters that you investigated in the previous exercise and rerun velvetg . time and monitor the use of resources as previously. Start with -cov_cutoff 16 thus: mv run_25/contigs.fa run_25/contigs.fa.0 time velvetg run_25 -cov_cutoff 16 Up until now, velvetg has ignored the paired-end information. Now try running velvetg with both -cov_cutoff 16 and -exp_cov 26 , but first save your contigs.fa file. By using -cov_cutoff and -exp_cov , velvetg tries to estimate the insert length, which you will see in the velvetg output. The command is, of course: mv run_25/contigs.fa run_25/contigs.fa.1 time velvetg run_25 -cov_cutoff 16 -exp_cov 26 Comment on the time required, use of memory and CPU for velvetg ? Runtime is lower when velvet can reuse previously calculated data. By using -exp_cov , the memory usage increases. Which insert length does Velvet estimate? Paired-end library 1 has length: 228, sample standard deviation: 26 Next try running velvetg in \u2018paired-end mode\u2018. This entails running velvetg specifying the insert length with the parameter -ins_length set to 350. Even though velvet estimates the insert length it is always advisable to check / provide the insert length manually as velvet can get the statistics wrong due to noise. Just in case, save your last version of contigs.fa . The commands are: mv run_25/contigs.fa run_25/contigs.fa.2 time velvetg run_25 -cov_cutoff 16 -exp_cov 26 -ins_length 350 mv run_25/contigs.fa run_25/contigs.fa.3 How fast was this run? My own measurements are:\\ real 0m29.792s; user 1m4.372s; sys 0m3.880s Take a look into the Log file. What is the N50 value for the velvetg runs using the switches:\\ Base run: 19,510 bp -cov_cutoff 16 24,739 bp -cov_cutoff 16 -exp_cov 26 61,793 bp -cov_cutoff 16 -exp_cov 26 -ins_length 350 n50 of 62,740 bp; max 194,649 bp; total 2,871,093 bp Try giving the -cov_cutoff and/or -exp_cov parameters the value auto . See the velvetg help to show you how. The information Velvet prints during running includes information about the values used (coverage cut-off or insert length) when using the auto option. What coverage values does Velvet choose (hint: look at the output that Velvet produces while running)? Median coverage depth = 26.021837\\ Removing contigs with coverage $<$ 13.010918 \u2026 How does the N50 value change? n50 of 68,843 bp; max 194,645 bp; total 2,872,678 bp Run gnx on all the contig.fa files you have generated in the course of this exercise. The command will be: gnx -min 100 -nx 25,50,75 run_25/contigs.fa* For which runs are there Ns in the contigs.fa file and why? contigs.fa.2, contigs.fa.3, contigs.fa\\ Velvet tries to use the provided (or infers) the insert length and fills ambiguous regions with Ns. Comment on the number of contigs and total length generated for each run. Filename No. contigs Total length No. Ns Contigs.fa.0 631 2,830,659 0 Contigs.fa.1 580 2,832,670 0 Contigs.fa.2 166 2,849,919 4,847 Contigs.fa.3 166 2,856,795 11,713 Contigs.fa 163 2,857,439 11,526 : tab:velvetrunresults","title":"Assembling Paired-end Reads"},{"location":"denovo/handout/paired_end/#amos-hawkeye","text":"We will now output the assembly in the AMOS massage format and visualise the assembly using AMOS Hawkeye. Run velvetg with appropriate arguments and output the AMOS message file, then convert it to an AMOS bank and open it in Hawkeye: time velvetg run_25 -cov_cutoff 16 -exp_cov 26 -ins_length 350 -amos_file yes -read_trkg yes time bank-transact -c -b run_25/velvet_asm.bnk -m run_25/velvet_asm.afg hawkeye run_25/velvet_asm.bnk Looking at the scaffold view of a contig, comment on the proportion of \u201chappy mates\u201d to \u201ccompressed mates\u201d and \u201cstretched mates\u201d. Nearly all mates are compressed with no stretched mates and very few happy mates. What is the mean and standard deviation of the insert size reported under the Libraries tab? Mean: 350 bp SD: 35 bp Look at the actual distribution of insert sizes for this library. Can you explain where there is a difference between the mean and SD reported in those two places? We specified -ins_length 350 to the velvetg command. Velvet uses this value, in the AMOS message file that it outputs, rather than its own estimate. You can get AMOS to re-estimate the mean and SD of insert sizes using intra-contig pairs. First, close Hawkeye and then run the following commands before reopening the AMOS bank to see what has changed. asmQC -b run_25/velvet_asm.bnk -scaff -recompute -update -numsd 2 hawkeye run_25/velvet_asm.bnk Looking at the scaffold view of a contig, comment on the proportion of \u201chappy mates\u201d to \u201ccompressed mates\u201d and \u201cstretched mates\u201d. There are only a few compressed and stretched mates compared to happy mates. There are similar numbers of stretched and compressed mates. What is the mean and standard deviation of the insert size reported under the Libraries tab? TODO Mean: 226 bp SD: 25 bp Look at the actual distribution of insert sizes for this library. Does the mean and SD reported in both places now match? Yes Can you find a region with an unusually high proportion of stretched, compressed, incorrectly orientated or linking mates? What might this situation indicate? This would indicate a possible misassembly and worthy of further investigation. Look at the largest scaffold, there are stacks of stretched pairs which span contig boundaries. This indicates that the gap size has been underestimated during the scaffolding phase.","title":"AMOS Hawkeye"},{"location":"denovo/handout/paired_end/#velvet-and-data-quality","text":"So far we have used the raw read data without performing any quality control or read trimming prior to doing our velvet assemblies. Velvet does not use quality information present in FASTQ files. For this reason, it is vitally important to perform read QC and quality trimming. In doing so, we remove errors/noise from the dataset which in turn means velvet will run faster, will use less memory and will produce a better assembly. Assuming we haven\u2019t compromised too much on coverage. To investigate the effect of data quality, we will use the run data (SRR023408) from the SRA experiment SRX008042. The reads are Illumina paired end with an insert size of 92 bp. Go back to the main directory for this exercise and create and enter a new directory dedicated to this phase of the exercise. The commands are: cd /home/trainee/denovo/NGS/velvet/part2 mkdir SRX008042 cd SRX008042 Create symlinks to the read data files that we downloaded for you from the SRA: ln -s /home/trainee/denovo/data/SRR023408_?.fastq.gz ./ We will use FastQC, a tool you should be familiar with, to visualise the quality of our data. We will use FastQC in the Graphical User Interface (GUI) mode. Start FastQC and set the process running in the background, by using a trailing & , so we get control of our terminal back for entering more commands: fastqc &amp; Open the two compressed FASTQ files (File $->$ Open) by selecting them both and clicking OK). Look at tabs for both files: {width=\u201d80.00000%\u201d} Are the quality scores the same for both files? Overall yes Which value varies? Per sequence quality scores Take a look at the Per base sequence quality for both files. Did you note that it is not good for either file? The quality score of both files drop very fast. Qualities of the REV strand drop faster than the FWD strand. This is because the template has been sat around while the FWD strand was sequenced. At which positions would you cut the reads if we did \u201cfixed length trimming\u201d? Looking at the \u201cPer base quality\u201d and \u201cPer base sequence content\u201d, I would choose around 27 Why does the quality deteriorate towards the end of the read? Errors more likely for later cycles Does it make sense to trim the 5\u2019 start of reads? Looking at the \u201cPer base sequence content\u201d, yes - there is a clear signal at the beginning. Have a look at the other options that FastQC offers. Which other statistics could you use to support your trimming strategy? \u201cPer base sequence content\u201d, \u201cPer base GC content\u201d, \u201cKmer content\u201d, \u201cPer base sequence quality\u201d {width=\u201d80.00000%\u201d} Once you have decided what your trim points will be, close FastQC. We will use fastx_trimmer from the FASTX-Toolkit to perform fixed-length trimming. For usage information see the help: fastx_trimmer -h fastx_trimmer is not able to read compressed FASTQ files, so we first need to decompress the files ready for input. The suggestion (hopefully not far from your own thoughts?) is that you trim your reads as follows: gunzip &lt; SRR023408_1.fastq.gz &gt; SRR023408_1.fastq gunzip &lt; SRR023408_2.fastq.gz &gt; SRR023408_2.fastq fastx_trimmer -Q 33 -f 1 -l 32 -i SRR023408_1.fastq -o SRR023408_trim1.fastq fastx_trimmer -Q 33 -f 1 -l 27 -i SRR023408_2.fastq -o SRR023408_trim2.fastq Many NGS read files are large. This means that simply reading and writing files can become the bottleneck, also known as I/O bound. Therefore, it is often good practice to avoid unnecessary disk read/write. We could do what is called pipelining to send a stream of data from one command to another, using the pipe ( | ) character, without the need for intermediary files. The following command would achieve this: gunzip --to-stdout &lt; SRR023408_1.fastq.gz | fastx_trimmer -Q 33 -f 4 -l 32 -o SRR023408_trim1.fastq gunzip --to-stdout &lt; SRR023408_2.fastq.gz | fastx_trimmer -Q 33 -f 3 -l 29 -o SRR023408_trim2.fastq Now run velveth with a k-mer value of 21 for both the untrimmed and trimmed read files in -shortPaired mode. Separate the output of the two executions of velveth into suitably named directories, followed by velvetg : # untrimmed reads velveth run_21 21 -fmtAuto -create_binary -shortPaired -separate SRR023408_1.fastq SRR023408_2.fastq time velvetg run_21 # trimmed reads velveth run_21trim 21 -fmtAuto -create_binary -shortPaired -separate SRR023408_trim1.fastq SRR023408_trim2.fastq time velvetg run_21trim How long did the two velvetg runs take? run_25: real 3m16.132s; user 8m18.261s; sys 0m7.317s \\ run_25trim: real 1m18.611s; user 3m53.140s; sys 0m4.962s What N50 scores did you achieve? Untrimmed: 11\\ Trimmed: 15 What were the overall effects of trimming? Time saving, increased N50, reduced coverage The evidence is that trimming improved the assembly. The thing to do surely, is to run velvetg with the -cov_cutoff and -exp_cov . In order to use -cov_cutoff and -exp_cov sensibly, you need to investigate with R, as you did in the previous exercise, what parameter values to use. Start up R and produce the weighted histograms: ``` {style=\u201dR\u201d} R \u2013no-save library(plotrix) data <- read.table(\u201crun_21/stats.txt\u201d, header=TRUE) data2 <- read.table(\u201crun_21trim/stats.txt\u201d, header=TRUE) par(mfrow=c(1,2)) weighted.hist(data$short1_cov, data$lgth, breaks=0:50) weighted.hist(data2$short1_cov, data2$lgth, breaks=0:50) ![\\[fig:velvet\\_Rplot002\\] Weighted k-mer coverage histograms of the paired-end reads pre-trimmed (left) and post-trimmed (right).](handout/velvet/velvet_Rplot002.png){width=\"80.00000%\"} For the untrimmed read histogram (left) there is an expected coverage of around 13 with a coverage cut-off of around 7. For the trimmed read histogram (right) there is an expected coverage of around 9 with a coverage cut-off of around 5. If you disagree, feel free to try different settings, but first quit R before running `velvetg`: ``` {style=\"R\"} q() time velvetg run_21 -cov_cutoff 7 -exp_cov 13 -ins_length 92 time velvetg run_21trim -cov_cutoff 5 -exp_cov 9 -ins_length 92 How good does it look now?\\ Still not great Comment on:\\ Runtime Reduced runtime Memory Lower memory usage k-mer choice (Can you use k-mer 31 for a read of length 30 bp?) K-mer has to be lower than the read length and the K-mer coverage should be sufficient to produce results. Does less data mean \u201cworse\u201d results? Not necessarily. If you have lots of data you can safely remove poor data without too much impact on overall coverage. How would a smaller/larger k-mer size behave? Compare the results, produced during the last exercises, with each other: [0.9]{}[l|l|l|l]{} Metric & SRR022852 & SRR023408 & SRR023408.trimmed\\ Overall Quality (1-5) & & &\\ bp Coverage & & &\\ k-mer Coverage & & &\\ N50 (k-mer used) & & &\\ [0.9]{}[l|l|l|l]{} Metric & SRR022852 & SRR023408 & SRR023408.trimmed\\ Overall Quality (1-5) & 2 & 5 & 4\\ bp Coverage & 136 x (36 bp;11,374,488) & 95x (37bp; 7761796) & 82x (32bp; 7761796)\\ k-mer Coverage & 45x & 43x (21); 33x (25) & 30x (21); 20.5x (25)\\ N50 (k-mer used) & 68,843 (25) & 2,803 (21) & 2,914 (21)\\ What would you consider as the \u201cbest\u201d assembly? SRR022852 If you found a candidate, why do you consider it as \u201cbest\u201d assembly? Overall data quality and coverage How else might you assess the the quality of an assembly? Hint: Hawkeye. By trying to identify paired-end constraint violations using AMOS Hawkeye.","title":"Velvet and Data Quality"},{"location":"denovo-canu/handout/handout/","text":"Pacbio reads: assembly with command line tools Keywords: de novo assembly, PacBio, PacificBiosciences, Illumina, command line, Canu, Circlator, BWA, Spades, Pilon, Microbial Genomics Virtual Laboratory This tutorial demonstrates how to use long Pacbio sequence reads to assemble a bacterial genome, including correcting the assembly with short Illumina reads. Resources Tools (and versions) used in this tutorial include: canu 1.5 [recently updated] infoseq and sizeseq (part of EMBOSS) 6.6.0.0 circlator 1.5.1 [recently updated] bwa 0.7.15 samtools 1.3.1 makeblastdb and blastn (part of blast) 2.4.0+ pilon 1.20 Learning objectives At the end of this tutorial, be able to: Assemble and circularise a bacterial genome from PacBio sequence data. Recover small plasmids missed by long read sequencing, using Illumina data Explore the effect of polishing assembled sequences with a different data set. Overview Simplified version of workflow: Get data The files we need are: pacbio.fastq.gz : the PacBio reads illumina_R1.fastq.gz : the Illumina forward reads illumina_R2.fastq.gz : the Illumina reverse reads If you already have the files, skip forward to next section, Assemble . Otherwise, this section has information about how to find and move the files: PacBio files Open the command line. Navigate to or create the directory in which you want to work. If the files are already on your server, you can symlink by using ln -s real_file_path [e.g. data/sample_name/pacbio1.fastq.gz] chosen_symlink_name [e.g. pacbio1.fastq.gz] Alternatively, obtain the input files from elsewhere, e.g. from the BPA portal. (You will need a password.) Pacbio files are often stored in the format: Sample_name/Cell_name/Analysis_Results/long_file_name_1.fastq.gz We will use the longfilename.subreads.fastq.gz files. The reads are usually split into three separate files because they are so large. Right click on the first subreads.fastq.gz file and \u201ccopy link address\u201d. In the command line, type: wget --user username --password password [paste link URL for file] - Repeat for the other two subreads.fastq.gz files. - Join the files: cat pacbio*.fastq.gz > pacbio.fastq.gz - If the files are not gzipped, type: cat pacbio*.fastq | gzip > pacbio.fastq.gz Illumina files We will also use 2 x Illumina (Miseq) fastq.gz files. These are the R1.fastq.gz and R2.fastq.gz files. Symlink or \u201cwget\u201d these files as described above for PacBio files. Shorten the name of each of these files: mv longfilename_R1.fastq.gz illumina_R1.fastq.gz mv longfilename_R2.fastq.gz illumina_R2.fastq.gz Sample information The sample used in this tutorial is a gram-positive bacteria called Staphylococcus aureus (sample number 25747). This particular sample is from a strain that is resistant to the antibiotic methicillin (a type of penicillin). It is also called MRSA: methicillin-resistant Staphylococcus aureus . It was isolated from (human) blood and caused bacteraemia, an infection of the bloodstream. Assemble We will use the assembly software called Canu . Run Canu with these commands: canu -p canu -d canu_outdir genomeSize=2.8m -pacbio-raw pacbio.fastq.gz the first canu tells the program to run -p canu names prefix for output files (\u201ccanu\u201d) -d canu_outdir names output directory (\u201ccanu_outdir\u201d) genomeSize only has to be approximate. e.g. Staphylococcus aureus , 2.8m e.g. Streptococcus pyogenes , 1.8m Canu will correct, trim and assemble the reads. Various output will be displayed on the screen. Check the output Move into canu_outdir and ls to see the output files. The canu.contigs.fasta are the assembled sequences. The canu.unassembled.fasta are the reads that could not be assembled. The canu.correctedReads.fasta.gz are the corrected Pacbio reads that were used in the assembly. The canu.file.gfa is the graph of the assembly. Display summary information about the contigs: ( infoseq is a tool from EMBOSS ) infoseq canu.contigs.fasta This will show the contigs found by Canu. e.g., - tig00000001 2851805 This looks like a chromosome of approximately 2.8 million bases. This matches what we would expect for this sample. For other data, Canu may not be able to join all the reads into one contig, so there may be several contigs in the output. Also, the sample may contain some plasmids and these may be found full or partially by Canu as additional contigs. Change Canu parameters if required If the assembly is poor with many contigs, re-run Canu with extra sensitivity parameters; e.g. canu -p prefix -d outdir corMhapSensitivity=high corMinCoverage=0 genomeSize=2.8m -pacbio-raw pacbio.fastq.gz Questions Q: How do long- and short-read assembly methods differ? A: short reads: De Bruijn graphs; long reads: a move back towards simpler overlap-layout-consensus methods. Q: Where can we find out the what the approximate genome size should be for the species being assembled? A: NCBI Genomes - enter species name - click on Genome Assembly and Annotation report - sort table by clicking on the column header Size (Mb) - look at range of sizes in this column. Q: In the assembly output, what are the unassembled reads? Why are they there? Q: What are the corrected reads? How did canu correct the reads? Q: Where could you view the output .gfa and what would it show? Trim and circularise Run Circlator Circlator identifies and trims overhangs (on chromosomes and plasmids) and orients the start position at an appropriate gene (e.g. dnaA). It takes in the assembled contigs from Canu, as well as the corrected reads prepared by Canu. Overhangs are shown in blue: Adapted from Figure 1. Hunt et al. Genome Biology 2015 Move back into your main analysis folder. Run Circlator: circlator all --threads 8 --verbose canu_outdir/canu.contigs.fasta canu_outdir/canu.correctedReads.fasta.gz circlator_outdir --threads is the number of cores: change this to an appropriate number --verbose prints progress information to the screen canu_outdir/canu.contigs.fasta is the file path to the input Canu assembly canu_outdir/canu.correctedReads.fasta.gz is the file path to the corrected Pacbio reads - note, fastA not fastQ circlator_outdir is the name of the output directory. Some output will print to screen. When finished, it should say \u201cCircularized x of x contig(s)\u201d. Check the output Move into the circlator_outdir directory and ls to list files. Were the contigs circularised? : less 04.merge.circularise.log Yes, the contig was circularised (last column). Type \u201cq\u201d to exit. Where were the contigs oriented (which gene)? : less 06.fixstart.log - Look in the \u201cgene_name\u201d column. - The contig has been oriented at tr|A0A090N2A8|A0A090N2A8_STAAU, which is another name for dnaA. This is typically used as the start of bacterial chromosome sequences. What are the trimmed contig sizes? : infoseq 06.fixstart.fasta tig00000001 2823331 (28564 bases trimmed) This trimmed part is the overlap. Re-name the contigs file : The trimmed contigs are in the file called 06.fixstart.fasta . Re-name it contig1.fasta : mv 06.fixstart.fasta contig1.fasta Open this file in a text editor (e.g. nano: nano contig1.fasta ) and change the header to \u201c>chromosome\u201d. Move the file back into the main folder ( mv contig1.fasta ../ ). Options If all the contigs have not circularised with Circlator, an option is to change the --b2r_length_cutoff setting to approximately 2X the average read depth. Questions Q: Were all the contigs circularised? Why/why not? Q: Circlator can set the start of the sequence at a particular gene. Which gene does it use? Is this appropriate for all contigs? A: Uses dnaA for the chromosomal contig. For other contigs, uses a centrally-located gene. However, ideally, plasmids would be oriented on a gene such as repA. It is possible to provide a file to Circlator to do this. Find smaller plasmids Pacbio reads are long, and may have been longer than small plasmids. We will look for any small plasmids using the Illumina reads. This section involves several steps: Use the Canu+Circlator output of a trimmed assembly contig. Map all the Illumina reads against this Pacbio-assembled contig. Extract any reads that didn\u2019t map and assemble them together: this could be a plasmid, or part of a plasmid. Look for overhang: if found, trim. Align Illumina reads to the PacBio contig Index the contigs file: bwa index contig1.fasta Align Illumina reads using using bwa mem: bwa mem -t 8 contig1.fasta illumina_R1.fastq.gz illumina_R2.fastq.gz | samtools sort > aln.bam bwa mem is the alignment tool -t 8 is the number of cores: choose an appropriate number contig1.fasta is the input assembly file illumina_R1.fastq.gz illumina_R2.fastq.gz are the Illumina reads | samtools sort pipes the output to samtools to sort > aln.bam sends the alignment to the file aln.bam Extract unmapped Illumina reads Index the alignment file: samtools index aln.bam Extract the fastq files from the bam alignment - those reads that were unmapped to the Pacbio alignment - and save them in various \u201cunmapped\u201d files: samtools fastq -f 4 -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq aln.bam fastq is a command that coverts a .bam file into fastq format -f 4 : only output unmapped reads -1 : put R1 reads into a file called unmapped.R1.fastq -2 : put R2 reads into a file called unmapped.R2.fastq -s : put singleton reads into a file called unmapped.RS.fastq aln.bam : input alignment file We now have three files of the unampped reads: unmapped.R1.fastq , unmapped.R2.fastq , unmapped.RS.fastq . Assemble the unmapped reads Assemble with Spades: spades.py -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq --careful --cov-cutoff auto -o spades_assembly -1 is input file forward -2 is input file reverse -s is unpaired --careful minimizes mismatches and short indels --cov-cutoff auto computes the coverage threshold (rather than the default setting, \u201coff\u201d) -o is the output directory Move into the output directory ( spades_assembly ) and look at the contigs: infoseq contigs.fasta - 78 contigs were assembled, with the max length of 2250 (the first contig). - All other nodes are < 650kb so we will disregard as they are unlikely to be plasmids. - Type \u201cq\u201d to exit. - We will extract the first sequence (NODE_1): samtools faidx contigs.fasta samtools faidx contigs.fasta NODE_1_length_2550_cov_496.613 > contig2.fasta This is now saved as contig2.fasta Open in nano and change header to \u201c>plasmid\u201d. Trim the plasmid To trim any overhang on this plasmid, we will blast the start of contig2 against itself. Take the start of the contig: head -n 10 contig2.fasta > contig2.fa.head We want to see if it matches the end (overhang). Format the assembly file for blast: makeblastdb -in contig2.fasta -dbtype nucl Blast the start of the assembly (.head file) against all of the assembly: blastn -query contig2.fa.head -db contig2.fasta -evalue 1e-3 -dust no -out contig2.bls Look at contig2.bls to see hits: less contig2.bls The first hit is at start, as expected. The second hit is at 2474 all the way to the end - 2550. This is the overhang. Trim to position 2473. Index the plasmid.fa file: samtools faidx contig2.fasta Trim: samtools faidx contig2.fasta plasmid:1-2473 > plasmid.fa.trimmed plasmid is the name of the contig, and we want the sequence from 1-2473. Open this file in nano ( nano plasmid.fa.trimmed ) and change the header to \u201c>plasmid\u201d, save. We now have a trimmed plasmid. Move file back into main folder: cp plasmid.fa.trimmed ../ Move into the main folder. Plasmid contig orientation The bacterial chromosome was oriented at the gene dnaA. Plasmids are often oriented at the replication gene, but this is highly variable and there is no established convention. Here we will orient the plasmid at a gene found by Prodigal, in Circlator: circlator fixstart plasmid.fa.trimmed plasmid_fixstart fixstart is an option in Circlator just to orient a sequence. plasmid.fa.trimmed is our small plasmid. plasmid_fixstart is the prefix for the output files. View the output: less plasmid_fixstart.log The plasmid has been oriented at a gene predicted by Prodigal, and the break-point is at position 1200. Change the file name: cp plasmid_fixstart.fasta contig2.fasta Collect contigs cat contig1.fasta contig2.fasta > genome.fasta See the contigs and sizes: infoseq genome.fasta chromosome: 2823331 plasmid: 2473 Questions Q: Why is this section so complicated? A: Finding small plasmids is difficult for many reasons! This paper has a nice summary: On the (im)possibility to reconstruct plasmids from whole genome short-read sequencing data. doi: https://doi.org/10.1101/086744 Q: Why can PacBio sequencing miss small plasmids? A: Library prep size selection Q: We extract unmapped Illumina reads and assemble these to find small plasmids. What could they be missing? A: Repeats that have mapped to the PacBio assembly. Q: How do you find a plasmid in a Bandage graph? A: It is probably circular, matches the size of a known plasmid, has a rep gene\u2026 Q: Are there easier ways to find plasmids? A: Possibly. One option is the program called Unicycler which may automate many of these steps. https://github.com/rrwick/Unicycler Correct We will correct the Pacbio assembly with Illumina reads. Make an alignment file Align the Illumina reads (R1 and R2) to the draft PacBio assembly, e.g. genome.fasta : bwa index genome.fasta bwa mem -t 32 genome.fasta illumina_R1.fastq.gz illumina_R2.fastq.gz | samtools sort > aln.bam -t is the number of cores: set this to an appropriate number. (To find out how many you have, grep -c processor /proc/cpuinfo ). Index the files: samtools index aln.bam samtools faidx genome.fasta Now we have an alignment file to use in Pilon: aln.bam Run Pilon Run: pilon --genome genome.fasta --frags aln.bam --output pilon1 --fix all --mindepth 0.5 --changes --verbose --threads 32 --genome is the name of the input assembly to be corrected --frags is the alignment of the reads against the assembly --output is the name of the output prefix --fix is an option for types of corrections --mindepth gives a minimum read depth to use --changes produces an output file of the changes made --verbose prints information to the screen during the run --threads : set this to an appropriate number Look at the changes file: less pilon1.changes Example: Look at the details of the fasta file: infoseq pilon1.fasta chromosome - 2823340 (net +9 bases) plasmid - 2473 (no change) Option: If there are many changes, run Pilon again, using the pilon1.fasta file as the input assembly, and the Illumina reads to correct. Genome output Change the file name: cp pilon1.fasta assembly.fasta We now have the corrected genome assembly of Staphylococcus aureus in .fasta format, containing a chromosome and a small plasmid. Questions Q: Why don\u2019t we correct earlier in the assembly process? A: We need to circularise the contigs and trim overhangs first. Q: Why can we use some reads (Illumina) to correct other reads (PacBio) ? A: Illumina reads have higher accuracy Q: Could we just use PacBio reads to assemble the genome? A: Yes, if accuracy adequate. Short-read assembly: a comparison So far, we have assembled the long PacBio reads into one contig (the chromosome) and found an additional plasmid in the Illumina short reads. If we only had Illumina reads, we could also assemble these using the tool Spades. You can try this here or try it later on your own data. Get data We will use the same Illumina data as we used above: illumina_R1.fastq.gz : the Illumina forward reads illumina_R2.fastq.gz : the Illumina reverse reads Assemble Run Spades: spades.py -1 illumina_R1.fastq.gz -2 illumina_R2.fastq.gz --careful --cov-cutoff auto -o spades_assembly_all_illumina -1 is input file of forward reads -2 is input file of reverse reads --careful minimizes mismatches and short indels --cov-cutoff auto computes the coverage threshold (rather than the default setting, \u201coff\u201d) -o is the output directory Results Move into the output directory and look at the contigs: infoseq contigs.fasta Questions How many contigs were found by Spades? many How does this compare to the number of contigs found by assembling the long read data with Canu? many more. Does it matter that an assembly is in many contigs? Yes broken genes => missing/incorrect annotations less information about structure: e.g. number of plasmids No Many or all genes may still be annotated Gene location is useful (e.g. chromosome, plasmid1) but not always essential (e.g. presence/absence of particular resistance genes) How can we get more information about the assembly from Spades? Look at the assembly graph assembly_graph.fastg , e.g. in the program Bandage. This shows how contigs are related, albeit with ambiguity in some places. Next Further analyses Annotate with Prokka. Comparative genomics, e.g. with Roary. Links Details of bas.h5 files Canu manual and gitub repository Circlator article and github repository Pilon article and github repository Notes on finishing and evaluating assemblies.","title":"De Novo Canu"},{"location":"denovo-canu/handout/handout/#pacbio-reads-assembly-with-command-line-tools","text":"Keywords: de novo assembly, PacBio, PacificBiosciences, Illumina, command line, Canu, Circlator, BWA, Spades, Pilon, Microbial Genomics Virtual Laboratory This tutorial demonstrates how to use long Pacbio sequence reads to assemble a bacterial genome, including correcting the assembly with short Illumina reads.","title":"Pacbio reads: assembly with command line tools"},{"location":"denovo-canu/handout/handout/#resources","text":"Tools (and versions) used in this tutorial include: canu 1.5 [recently updated] infoseq and sizeseq (part of EMBOSS) 6.6.0.0 circlator 1.5.1 [recently updated] bwa 0.7.15 samtools 1.3.1 makeblastdb and blastn (part of blast) 2.4.0+ pilon 1.20","title":"Resources"},{"location":"denovo-canu/handout/handout/#learning-objectives","text":"At the end of this tutorial, be able to: Assemble and circularise a bacterial genome from PacBio sequence data. Recover small plasmids missed by long read sequencing, using Illumina data Explore the effect of polishing assembled sequences with a different data set.","title":"Learning objectives"},{"location":"denovo-canu/handout/handout/#overview","text":"Simplified version of workflow:","title":"Overview"},{"location":"denovo-canu/handout/handout/#get-data","text":"The files we need are: pacbio.fastq.gz : the PacBio reads illumina_R1.fastq.gz : the Illumina forward reads illumina_R2.fastq.gz : the Illumina reverse reads If you already have the files, skip forward to next section, Assemble . Otherwise, this section has information about how to find and move the files:","title":"Get data"},{"location":"denovo-canu/handout/handout/#pacbio-files","text":"Open the command line. Navigate to or create the directory in which you want to work. If the files are already on your server, you can symlink by using ln -s real_file_path [e.g. data/sample_name/pacbio1.fastq.gz] chosen_symlink_name [e.g. pacbio1.fastq.gz] Alternatively, obtain the input files from elsewhere, e.g. from the BPA portal. (You will need a password.) Pacbio files are often stored in the format: Sample_name/Cell_name/Analysis_Results/long_file_name_1.fastq.gz We will use the longfilename.subreads.fastq.gz files. The reads are usually split into three separate files because they are so large. Right click on the first subreads.fastq.gz file and \u201ccopy link address\u201d. In the command line, type: wget --user username --password password [paste link URL for file] - Repeat for the other two subreads.fastq.gz files. - Join the files: cat pacbio*.fastq.gz > pacbio.fastq.gz - If the files are not gzipped, type: cat pacbio*.fastq | gzip > pacbio.fastq.gz","title":"PacBio files"},{"location":"denovo-canu/handout/handout/#illumina-files","text":"We will also use 2 x Illumina (Miseq) fastq.gz files. These are the R1.fastq.gz and R2.fastq.gz files. Symlink or \u201cwget\u201d these files as described above for PacBio files. Shorten the name of each of these files: mv longfilename_R1.fastq.gz illumina_R1.fastq.gz mv longfilename_R2.fastq.gz illumina_R2.fastq.gz","title":"Illumina files"},{"location":"denovo-canu/handout/handout/#sample-information","text":"The sample used in this tutorial is a gram-positive bacteria called Staphylococcus aureus (sample number 25747). This particular sample is from a strain that is resistant to the antibiotic methicillin (a type of penicillin). It is also called MRSA: methicillin-resistant Staphylococcus aureus . It was isolated from (human) blood and caused bacteraemia, an infection of the bloodstream.","title":"Sample information"},{"location":"denovo-canu/handout/handout/#assemble","text":"We will use the assembly software called Canu . Run Canu with these commands: canu -p canu -d canu_outdir genomeSize=2.8m -pacbio-raw pacbio.fastq.gz the first canu tells the program to run -p canu names prefix for output files (\u201ccanu\u201d) -d canu_outdir names output directory (\u201ccanu_outdir\u201d) genomeSize only has to be approximate. e.g. Staphylococcus aureus , 2.8m e.g. Streptococcus pyogenes , 1.8m Canu will correct, trim and assemble the reads. Various output will be displayed on the screen.","title":"Assemble"},{"location":"denovo-canu/handout/handout/#check-the-output","text":"Move into canu_outdir and ls to see the output files. The canu.contigs.fasta are the assembled sequences. The canu.unassembled.fasta are the reads that could not be assembled. The canu.correctedReads.fasta.gz are the corrected Pacbio reads that were used in the assembly. The canu.file.gfa is the graph of the assembly. Display summary information about the contigs: ( infoseq is a tool from EMBOSS ) infoseq canu.contigs.fasta This will show the contigs found by Canu. e.g., - tig00000001 2851805 This looks like a chromosome of approximately 2.8 million bases. This matches what we would expect for this sample. For other data, Canu may not be able to join all the reads into one contig, so there may be several contigs in the output. Also, the sample may contain some plasmids and these may be found full or partially by Canu as additional contigs.","title":"Check the output"},{"location":"denovo-canu/handout/handout/#change-canu-parameters-if-required","text":"If the assembly is poor with many contigs, re-run Canu with extra sensitivity parameters; e.g. canu -p prefix -d outdir corMhapSensitivity=high corMinCoverage=0 genomeSize=2.8m -pacbio-raw pacbio.fastq.gz","title":"Change Canu parameters if required"},{"location":"denovo-canu/handout/handout/#questions","text":"Q: How do long- and short-read assembly methods differ? A: short reads: De Bruijn graphs; long reads: a move back towards simpler overlap-layout-consensus methods. Q: Where can we find out the what the approximate genome size should be for the species being assembled? A: NCBI Genomes - enter species name - click on Genome Assembly and Annotation report - sort table by clicking on the column header Size (Mb) - look at range of sizes in this column. Q: In the assembly output, what are the unassembled reads? Why are they there? Q: What are the corrected reads? How did canu correct the reads? Q: Where could you view the output .gfa and what would it show?","title":"Questions"},{"location":"denovo-canu/handout/handout/#trim-and-circularise","text":"","title":"Trim and circularise"},{"location":"denovo-canu/handout/handout/#run-circlator","text":"Circlator identifies and trims overhangs (on chromosomes and plasmids) and orients the start position at an appropriate gene (e.g. dnaA). It takes in the assembled contigs from Canu, as well as the corrected reads prepared by Canu. Overhangs are shown in blue: Adapted from Figure 1. Hunt et al. Genome Biology 2015 Move back into your main analysis folder. Run Circlator: circlator all --threads 8 --verbose canu_outdir/canu.contigs.fasta canu_outdir/canu.correctedReads.fasta.gz circlator_outdir --threads is the number of cores: change this to an appropriate number --verbose prints progress information to the screen canu_outdir/canu.contigs.fasta is the file path to the input Canu assembly canu_outdir/canu.correctedReads.fasta.gz is the file path to the corrected Pacbio reads - note, fastA not fastQ circlator_outdir is the name of the output directory. Some output will print to screen. When finished, it should say \u201cCircularized x of x contig(s)\u201d.","title":"Run Circlator"},{"location":"denovo-canu/handout/handout/#check-the-output_1","text":"Move into the circlator_outdir directory and ls to list files. Were the contigs circularised? : less 04.merge.circularise.log Yes, the contig was circularised (last column). Type \u201cq\u201d to exit. Where were the contigs oriented (which gene)? : less 06.fixstart.log - Look in the \u201cgene_name\u201d column. - The contig has been oriented at tr|A0A090N2A8|A0A090N2A8_STAAU, which is another name for dnaA. This is typically used as the start of bacterial chromosome sequences. What are the trimmed contig sizes? : infoseq 06.fixstart.fasta tig00000001 2823331 (28564 bases trimmed) This trimmed part is the overlap. Re-name the contigs file : The trimmed contigs are in the file called 06.fixstart.fasta . Re-name it contig1.fasta : mv 06.fixstart.fasta contig1.fasta Open this file in a text editor (e.g. nano: nano contig1.fasta ) and change the header to \u201c>chromosome\u201d. Move the file back into the main folder ( mv contig1.fasta ../ ).","title":"Check the output"},{"location":"denovo-canu/handout/handout/#options","text":"If all the contigs have not circularised with Circlator, an option is to change the --b2r_length_cutoff setting to approximately 2X the average read depth.","title":"Options"},{"location":"denovo-canu/handout/handout/#questions_1","text":"Q: Were all the contigs circularised? Why/why not? Q: Circlator can set the start of the sequence at a particular gene. Which gene does it use? Is this appropriate for all contigs? A: Uses dnaA for the chromosomal contig. For other contigs, uses a centrally-located gene. However, ideally, plasmids would be oriented on a gene such as repA. It is possible to provide a file to Circlator to do this.","title":"Questions"},{"location":"denovo-canu/handout/handout/#find-smaller-plasmids","text":"Pacbio reads are long, and may have been longer than small plasmids. We will look for any small plasmids using the Illumina reads. This section involves several steps: Use the Canu+Circlator output of a trimmed assembly contig. Map all the Illumina reads against this Pacbio-assembled contig. Extract any reads that didn\u2019t map and assemble them together: this could be a plasmid, or part of a plasmid. Look for overhang: if found, trim.","title":"Find smaller plasmids"},{"location":"denovo-canu/handout/handout/#align-illumina-reads-to-the-pacbio-contig","text":"Index the contigs file: bwa index contig1.fasta Align Illumina reads using using bwa mem: bwa mem -t 8 contig1.fasta illumina_R1.fastq.gz illumina_R2.fastq.gz | samtools sort > aln.bam bwa mem is the alignment tool -t 8 is the number of cores: choose an appropriate number contig1.fasta is the input assembly file illumina_R1.fastq.gz illumina_R2.fastq.gz are the Illumina reads | samtools sort pipes the output to samtools to sort > aln.bam sends the alignment to the file aln.bam","title":"Align Illumina reads to the PacBio contig"},{"location":"denovo-canu/handout/handout/#extract-unmapped-illumina-reads","text":"Index the alignment file: samtools index aln.bam Extract the fastq files from the bam alignment - those reads that were unmapped to the Pacbio alignment - and save them in various \u201cunmapped\u201d files: samtools fastq -f 4 -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq aln.bam fastq is a command that coverts a .bam file into fastq format -f 4 : only output unmapped reads -1 : put R1 reads into a file called unmapped.R1.fastq -2 : put R2 reads into a file called unmapped.R2.fastq -s : put singleton reads into a file called unmapped.RS.fastq aln.bam : input alignment file We now have three files of the unampped reads: unmapped.R1.fastq , unmapped.R2.fastq , unmapped.RS.fastq .","title":"Extract unmapped Illumina reads"},{"location":"denovo-canu/handout/handout/#assemble-the-unmapped-reads","text":"Assemble with Spades: spades.py -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq --careful --cov-cutoff auto -o spades_assembly -1 is input file forward -2 is input file reverse -s is unpaired --careful minimizes mismatches and short indels --cov-cutoff auto computes the coverage threshold (rather than the default setting, \u201coff\u201d) -o is the output directory Move into the output directory ( spades_assembly ) and look at the contigs: infoseq contigs.fasta - 78 contigs were assembled, with the max length of 2250 (the first contig). - All other nodes are < 650kb so we will disregard as they are unlikely to be plasmids. - Type \u201cq\u201d to exit. - We will extract the first sequence (NODE_1): samtools faidx contigs.fasta samtools faidx contigs.fasta NODE_1_length_2550_cov_496.613 > contig2.fasta This is now saved as contig2.fasta Open in nano and change header to \u201c>plasmid\u201d.","title":"Assemble the unmapped reads"},{"location":"denovo-canu/handout/handout/#trim-the-plasmid","text":"To trim any overhang on this plasmid, we will blast the start of contig2 against itself. Take the start of the contig: head -n 10 contig2.fasta > contig2.fa.head We want to see if it matches the end (overhang). Format the assembly file for blast: makeblastdb -in contig2.fasta -dbtype nucl Blast the start of the assembly (.head file) against all of the assembly: blastn -query contig2.fa.head -db contig2.fasta -evalue 1e-3 -dust no -out contig2.bls Look at contig2.bls to see hits: less contig2.bls The first hit is at start, as expected. The second hit is at 2474 all the way to the end - 2550. This is the overhang. Trim to position 2473. Index the plasmid.fa file: samtools faidx contig2.fasta Trim: samtools faidx contig2.fasta plasmid:1-2473 > plasmid.fa.trimmed plasmid is the name of the contig, and we want the sequence from 1-2473. Open this file in nano ( nano plasmid.fa.trimmed ) and change the header to \u201c>plasmid\u201d, save. We now have a trimmed plasmid. Move file back into main folder: cp plasmid.fa.trimmed ../ Move into the main folder.","title":"Trim the plasmid"},{"location":"denovo-canu/handout/handout/#plasmid-contig-orientation","text":"The bacterial chromosome was oriented at the gene dnaA. Plasmids are often oriented at the replication gene, but this is highly variable and there is no established convention. Here we will orient the plasmid at a gene found by Prodigal, in Circlator: circlator fixstart plasmid.fa.trimmed plasmid_fixstart fixstart is an option in Circlator just to orient a sequence. plasmid.fa.trimmed is our small plasmid. plasmid_fixstart is the prefix for the output files. View the output: less plasmid_fixstart.log The plasmid has been oriented at a gene predicted by Prodigal, and the break-point is at position 1200. Change the file name: cp plasmid_fixstart.fasta contig2.fasta","title":"Plasmid contig orientation"},{"location":"denovo-canu/handout/handout/#collect-contigs","text":"cat contig1.fasta contig2.fasta > genome.fasta See the contigs and sizes: infoseq genome.fasta chromosome: 2823331 plasmid: 2473","title":"Collect contigs"},{"location":"denovo-canu/handout/handout/#questions_2","text":"Q: Why is this section so complicated? A: Finding small plasmids is difficult for many reasons! This paper has a nice summary: On the (im)possibility to reconstruct plasmids from whole genome short-read sequencing data. doi: https://doi.org/10.1101/086744 Q: Why can PacBio sequencing miss small plasmids? A: Library prep size selection Q: We extract unmapped Illumina reads and assemble these to find small plasmids. What could they be missing? A: Repeats that have mapped to the PacBio assembly. Q: How do you find a plasmid in a Bandage graph? A: It is probably circular, matches the size of a known plasmid, has a rep gene\u2026 Q: Are there easier ways to find plasmids? A: Possibly. One option is the program called Unicycler which may automate many of these steps. https://github.com/rrwick/Unicycler","title":"Questions"},{"location":"denovo-canu/handout/handout/#correct","text":"We will correct the Pacbio assembly with Illumina reads.","title":"Correct"},{"location":"denovo-canu/handout/handout/#make-an-alignment-file","text":"Align the Illumina reads (R1 and R2) to the draft PacBio assembly, e.g. genome.fasta : bwa index genome.fasta bwa mem -t 32 genome.fasta illumina_R1.fastq.gz illumina_R2.fastq.gz | samtools sort > aln.bam -t is the number of cores: set this to an appropriate number. (To find out how many you have, grep -c processor /proc/cpuinfo ). Index the files: samtools index aln.bam samtools faidx genome.fasta Now we have an alignment file to use in Pilon: aln.bam","title":"Make an alignment file"},{"location":"denovo-canu/handout/handout/#run-pilon","text":"Run: pilon --genome genome.fasta --frags aln.bam --output pilon1 --fix all --mindepth 0.5 --changes --verbose --threads 32 --genome is the name of the input assembly to be corrected --frags is the alignment of the reads against the assembly --output is the name of the output prefix --fix is an option for types of corrections --mindepth gives a minimum read depth to use --changes produces an output file of the changes made --verbose prints information to the screen during the run --threads : set this to an appropriate number Look at the changes file: less pilon1.changes Example: Look at the details of the fasta file: infoseq pilon1.fasta chromosome - 2823340 (net +9 bases) plasmid - 2473 (no change) Option: If there are many changes, run Pilon again, using the pilon1.fasta file as the input assembly, and the Illumina reads to correct.","title":"Run Pilon"},{"location":"denovo-canu/handout/handout/#genome-output","text":"Change the file name: cp pilon1.fasta assembly.fasta We now have the corrected genome assembly of Staphylococcus aureus in .fasta format, containing a chromosome and a small plasmid.","title":"Genome output"},{"location":"denovo-canu/handout/handout/#questions_3","text":"Q: Why don\u2019t we correct earlier in the assembly process? A: We need to circularise the contigs and trim overhangs first. Q: Why can we use some reads (Illumina) to correct other reads (PacBio) ? A: Illumina reads have higher accuracy Q: Could we just use PacBio reads to assemble the genome? A: Yes, if accuracy adequate.","title":"Questions"},{"location":"denovo-canu/handout/handout/#short-read-assembly-a-comparison","text":"So far, we have assembled the long PacBio reads into one contig (the chromosome) and found an additional plasmid in the Illumina short reads. If we only had Illumina reads, we could also assemble these using the tool Spades. You can try this here or try it later on your own data.","title":"Short-read assembly: a comparison"},{"location":"denovo-canu/handout/handout/#get-data_1","text":"We will use the same Illumina data as we used above: illumina_R1.fastq.gz : the Illumina forward reads illumina_R2.fastq.gz : the Illumina reverse reads","title":"Get data"},{"location":"denovo-canu/handout/handout/#assemble_1","text":"Run Spades: spades.py -1 illumina_R1.fastq.gz -2 illumina_R2.fastq.gz --careful --cov-cutoff auto -o spades_assembly_all_illumina -1 is input file of forward reads -2 is input file of reverse reads --careful minimizes mismatches and short indels --cov-cutoff auto computes the coverage threshold (rather than the default setting, \u201coff\u201d) -o is the output directory","title":"Assemble"},{"location":"denovo-canu/handout/handout/#results","text":"Move into the output directory and look at the contigs: infoseq contigs.fasta","title":"Results"},{"location":"denovo-canu/handout/handout/#questions_4","text":"How many contigs were found by Spades? many How does this compare to the number of contigs found by assembling the long read data with Canu? many more. Does it matter that an assembly is in many contigs? Yes broken genes => missing/incorrect annotations less information about structure: e.g. number of plasmids No Many or all genes may still be annotated Gene location is useful (e.g. chromosome, plasmid1) but not always essential (e.g. presence/absence of particular resistance genes) How can we get more information about the assembly from Spades? Look at the assembly graph assembly_graph.fastg , e.g. in the program Bandage. This shows how contigs are related, albeit with ambiguity in some places.","title":"Questions"},{"location":"denovo-canu/handout/handout/#next","text":"","title":"Next"},{"location":"denovo-canu/handout/handout/#further-analyses","text":"Annotate with Prokka. Comparative genomics, e.g. with Roary.","title":"Further analyses"},{"location":"denovo-canu/handout/handout/#links","text":"Details of bas.h5 files Canu manual and gitub repository Circlator article and github repository Pilon article and github repository Notes on finishing and evaluating assemblies.","title":"Links"},{"location":"eukaryotic/","text":"denovo-module-euk Bioinformatics Training Platform (BTP) Module: Eukaryote Genome Assembly Topic Eukaryote Genome Assembly Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes Assess general assembly approach, kmer spectra and biases. Visually inspect the kmer spectra and KAT plots Run a first pass eukaryotic assembly and do goal checks Develop validation metrics or tools for NGS data and assembly. Improving methods and pipelines for genome assembly. Convince the lab guys to tweak protocols. Time Required 3.5 hrs License The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"denovo-module-euk"},{"location":"eukaryotic/#denovo-module-euk","text":"Bioinformatics Training Platform (BTP) Module: Eukaryote Genome Assembly Topic Eukaryote Genome Assembly Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes Assess general assembly approach, kmer spectra and biases. Visually inspect the kmer spectra and KAT plots Run a first pass eukaryotic assembly and do goal checks Develop validation metrics or tools for NGS data and assembly. Improving methods and pipelines for genome assembly. Convince the lab guys to tweak protocols. Time Required 3.5 hrs","title":"denovo-module-euk"},{"location":"eukaryotic/#license","text":"The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"License"},{"location":"eukaryotic/datasets/","text":"Information about what datasets are required for a module\u2019s tutorial exercises are described in a data.yaml file. As well describing where to source the dataset files from, we also need to describe where on the filesystem the files should reside and who should own them.","title":"Home"},{"location":"eukaryotic/handout/denovo_practical/","text":"Commands for DeNovo Training Exercise #1: Fusarium first pass with a goal Goal: Identify a fusarium sample is \u201ccloser\u201d to F. graminearum or F.pseudograminearum Previous knowledge: F. graminareum has a cluster producing PKS6 and NRSP7, while F. pseudograminareum produces PKS40 and NRPS32 Data: Proteins sequences for: F. graminareum (non necrotrophic): PKS6 and NRSP7 F. pseudograminareum (necrotrophic): PKS40 and NRPS32 Blast database of cereal pathogen proteins. Strategy: Check PKS6-NRSP7 and PKS40-NRPS32 cluster presence. Assembly goals: Assembly goal (I): to capture a good enough representation of the protein-coding space to get blast matches Assembly goal (II): to accurately represent the relevant whole cluster loci in a single sequence. Task 1.1: First pass assembly, k=71 First, assembly: cd denovo/fusarium mkdir abyss_k71 cd abyss_k71 abyss-pe in=\"../CS3270_A8733_GCCAAT_L001_R1.fastq ../CS3270_A8733_GCCAAT_L001_R2.fastq\" k=71 name=CS3270_abyss_k71 np=4 > CS3270_abyss_k71.log 2>&1 Stats\u2026 Ok, no stats, but we can always use abyss-fac abyss-fac CS3270_abyss_k71-contigs.fa n |n:500 |L50 |min |N80 |N50 |N20 |E-size |max |sum |name ------ |------ |------ |------ |------ |------ |------ |------ |------ |------ |------ 27 |13 |2 |970 |6004 |13202 |52602 |28712 |52602 |112849 |CS3270_abyss_k71-unitigs.fa 5 |1 |1 |128429 |128429 |128429 |128429 |128429 |128429 |128429 |CS3270_abyss_k71-contigs.fa Strange! Time for some analysis: Check frequencies for kmers kept/discarded/etc. Check spectra-cn and compare with expectations. less CS3270_abyss_k71.log less coverage.hist gnuplot gnuplot> set xrange [0:50] gnuplot> set yrange [0:4000000] gnuplot> plot \"coverage.hist\" gnuplot gnuplot> set xrange [0:200] gnuplot> set yrange [0:5000] gnuplot> plot \"coverage.hist\" kat comp -o reads_vs_abyss_k71 -t 4 -C -D '../*.fastq' CS3270_abyss_k71-contigs.fa Looks like we are not assembling this bit, let\u2019s have another look at the spectra kat comp -o reads_vs_abyss_k71 -t 4 -C -D --d1_bins 2000 '../*.fastq' CS3270_abyss_k71-contigs.fa kat plot spectra-cn -y 600 -x 2000 -o reads_vs_abyss1-main.mx.spectra-cn_2000.png reads_vs_abyss_k71-main.mx Take the output and blast it in NCBI. What is it? Surprising? This assembly will get us nowhere, let\u2019s choose a lower K to gain coverage and start again. Task 1.2: First pass assembly, k=27 cd denovo/fusarium mkdir abyss_k27 cd abyss_k27 abyss-pe in=\"../CS3270_A8733_GCCAAT_L001_R1.fastq ../CS3270_A8733_GCCAAT_L001_R2.fastq\" k=27 name=CS3270_abyss_k27 np=4 > CS3270_abyss_k27.log 2>&1 Stats look better: n n:500 L50 min N80 N50 N20 E-size max sum name 30645 2717 430 502 11354 25336 47966 31027 147694 36.14e6 CS3270_abyss_k27-unitigs.fa 21511 350 33 527 157565 338989 630228 407098 1265237 36.52e6 CS3270_abyss_k27-contigs.fa 21327 205 17 527 332444 716132 1265237 791882 1880850 36.51e6 CS3270_abyss_k27-scaffolds.fa Let\u2019s check a bit anyway: less CS3270_abyss_k27.log less coverage.hist gnuplot gnuplot> set xrange [0:50] gnuplot> set yrange [0:4000000] gnuplot> plot \"coverage.hist\" kat comp -o reads_vs_abyss_k27 -t 4 -C -D '../*.fastq' CS3270_abyss_k27-scaffolds.fa Question: any tools you can use to check kmer spectra at any K before assembling? Question: can you predict what will happen if you use KAT with larger K values? Task 1.3: Will the assembly answer the biological question? Use blast and the proteins to check\u2026 Exercise #2: Chalara scaffolding using LMP Task 2.1: let\u2019s have a look at the PE assembly abyss-pe name=cha1 k=27 in=\"../LIB2570_raw_R1.fastq ../LIB2570_raw_R2.fastq\" np=4 n n:500 L50 min N80 N50 N20 E-size max sum name 394789 11681 2094 500 2880 6254 12303 7936 44414 44.07e6 cha1-unitigs.fa 394199 11673 2097 500 2887 6255 12303 7937 44414 44.11e6 cha1-contigs.fa 394161 11647 2095 500 2898 6269 12303 7944 44414 44.12e6 cha1-scaffolds.fa abyss-pe name=cha2 k=61 in=\"../LIB2570_raw_R1.fastq ../LIB2570_raw_R2.fastq\" np=4 n n:500 L50 min N80 N50 N20 E-size max sum name \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 130547 12596 1770 500 3352 8379 16380 10518 54300 50.75e6 cha2-unitigs.fa 130210 12575 1771 500 3363 8382 16380 10525 54300 50.78e6 cha2-contigs.fa 130182 12555 1769 500 3377 8394 16380 10534 54300 50.78e6 cha2-scaffolds.fa Contiguity is worse than fusarium, why? Answers: Genome characteristics. Data: Coverage Error distributions Read sizes Fragment sizes Kmer spectra Task 2.2: Let\u2019s put some LMP in there. abyss-pe name=chalmp1 k=61 in=\"../LIB2570_raw_R1.fastq ../LIB2570_raw_R2.fastq\" mp=\"lmp1\" lmp1=\"../LIB8209_raw_R1.fastq ../LIB8209_raw_R2.fastq\" np=4 n |n:500 |L50 |min |N80 |N50 |N20 |E-size |max |sum |name \u2014 |\u2014 |\u2014 |\u2014 |\u2014 |\u2014 |\u2014 |\u2014 |\u2014 |\u2014 |\u2014 130548 |12596 |1770 |500 |3352 |8379 |16380 |10518 |54300 |50.75e6 |chalmp1-unitigs.fa 130211 |12575 |1771 |500 |3363 |8382 |16380 |10525 |54300 |50.78e6 |chalmp1-contigs.fa 130148 |12545 |1769 |500 |3380 |8400 |16380 |10535 |54300 |50.78e6 |chalmp1-scaffolds.fa So\u2026 what happened? Data: Kmer spectra Fragment sizes Any hints on the protocol? A not-so-obvious property: kat plot spectra-mx --intersection -x 30 -y 15000000 -o pe_vs_lmp-main.mx.spectra-mx.png pe_vs_lmp-main.mx LMP require pre-processing, right? Task 2.3: Let\u2019s try with processed LMP Prior task (already made) preprocess the LMP with nextclip. abyss-pe name=chalmpproc1 k=61 in=\"../../cha_raw/LIB2570_raw_R1.fastq ../../cha_raw/LIB2570_raw_R2.fastq\" mp=\"proclmp1\" proclmp1=\"../LIB8209_preproc_R1.fastq ../LIB8209_preproc_R2.fastq\" np=4 n n:500 L50 min N80 N50 N20 E-size max sum name 130548 12596 1770 500 3352 8379 16380 10518 54300 50.75e6 chalmpproc1-unitigs.fa 130211 12575 1771 500 3363 8382 16380 10525 54300 50.78e6 chalmpproc1-contigs.fa 122061 6679 167 500 18609 87510 187171 106178 397967 51.15e6 chalmpproc1-scaffolds.fa That\u2019s much better! Excercise #3: Chalara: beyond first pass Do you remember these? Genome characteristics. Data: Coverage Error distributions Read sizes Fragment sizes Kmer spectra Let\u2019s think a bit and try to improve the assembly\u2026 Question: if you look at the pre-processed LMP, do you notice anything peculiar? Example: Testing the inclussion of heavily pre-procesed LMP coverage into the DBG abyss-pe name=chalmp2 k=61 se=\"../LIB8209_preproc_single.fastq\" lib=\"lmp1\" lmp1=\"../LIB8209_preproc_R1.fastq ../LIB8209_preproc_R2.fastq\" np=4 >chaproclmp2.log 2>&1 n n:500 L50 min N80 N50 N20 E-size max sum name 128306 10150 1182 500 4954 12585 24777 15693 68684 51.03e6 chaproclmp4-unitigs.fa 118939 5772 362 500 15717 41870 82033 52819 252066 52.24e6 chaproclmp4-contigs.fa 116139 4014 141 500 41145 109273 224986 133450 460979 52.56e6 chaproclmp4-scaffolds.fa","title":"Denovo practical"},{"location":"eukaryotic/handout/denovo_practical/#commands-for-denovo-training","text":"","title":"Commands for DeNovo Training"},{"location":"eukaryotic/handout/denovo_practical/#exercise-1-fusarium-first-pass-with-a-goal","text":"","title":"Exercise #1: Fusarium first pass with a goal"},{"location":"eukaryotic/handout/denovo_practical/#goal-identify-a-fusarium-sample-is-closer-to-f-graminearum-or-fpseudograminearum","text":"Previous knowledge: F. graminareum has a cluster producing PKS6 and NRSP7, while F. pseudograminareum produces PKS40 and NRPS32 Data: Proteins sequences for: F. graminareum (non necrotrophic): PKS6 and NRSP7 F. pseudograminareum (necrotrophic): PKS40 and NRPS32 Blast database of cereal pathogen proteins. Strategy: Check PKS6-NRSP7 and PKS40-NRPS32 cluster presence. Assembly goals: Assembly goal (I): to capture a good enough representation of the protein-coding space to get blast matches Assembly goal (II): to accurately represent the relevant whole cluster loci in a single sequence.","title":"Goal: Identify a fusarium sample is \"closer\" to F. graminearum or F.pseudograminearum"},{"location":"eukaryotic/handout/denovo_practical/#task-11-first-pass-assembly-k71","text":"First, assembly: cd denovo/fusarium mkdir abyss_k71 cd abyss_k71 abyss-pe in=\"../CS3270_A8733_GCCAAT_L001_R1.fastq ../CS3270_A8733_GCCAAT_L001_R2.fastq\" k=71 name=CS3270_abyss_k71 np=4 > CS3270_abyss_k71.log 2>&1 Stats\u2026 Ok, no stats, but we can always use abyss-fac abyss-fac CS3270_abyss_k71-contigs.fa n |n:500 |L50 |min |N80 |N50 |N20 |E-size |max |sum |name ------ |------ |------ |------ |------ |------ |------ |------ |------ |------ |------ 27 |13 |2 |970 |6004 |13202 |52602 |28712 |52602 |112849 |CS3270_abyss_k71-unitigs.fa 5 |1 |1 |128429 |128429 |128429 |128429 |128429 |128429 |128429 |CS3270_abyss_k71-contigs.fa Strange! Time for some analysis: Check frequencies for kmers kept/discarded/etc. Check spectra-cn and compare with expectations. less CS3270_abyss_k71.log less coverage.hist gnuplot gnuplot> set xrange [0:50] gnuplot> set yrange [0:4000000] gnuplot> plot \"coverage.hist\" gnuplot gnuplot> set xrange [0:200] gnuplot> set yrange [0:5000] gnuplot> plot \"coverage.hist\" kat comp -o reads_vs_abyss_k71 -t 4 -C -D '../*.fastq' CS3270_abyss_k71-contigs.fa Looks like we are not assembling this bit, let\u2019s have another look at the spectra kat comp -o reads_vs_abyss_k71 -t 4 -C -D --d1_bins 2000 '../*.fastq' CS3270_abyss_k71-contigs.fa kat plot spectra-cn -y 600 -x 2000 -o reads_vs_abyss1-main.mx.spectra-cn_2000.png reads_vs_abyss_k71-main.mx Take the output and blast it in NCBI. What is it? Surprising? This assembly will get us nowhere, let\u2019s choose a lower K to gain coverage and start again.","title":"Task 1.1: First pass assembly, k=71"},{"location":"eukaryotic/handout/denovo_practical/#task-12-first-pass-assembly-k27","text":"cd denovo/fusarium mkdir abyss_k27 cd abyss_k27 abyss-pe in=\"../CS3270_A8733_GCCAAT_L001_R1.fastq ../CS3270_A8733_GCCAAT_L001_R2.fastq\" k=27 name=CS3270_abyss_k27 np=4 > CS3270_abyss_k27.log 2>&1 Stats look better: n n:500 L50 min N80 N50 N20 E-size max sum name 30645 2717 430 502 11354 25336 47966 31027 147694 36.14e6 CS3270_abyss_k27-unitigs.fa 21511 350 33 527 157565 338989 630228 407098 1265237 36.52e6 CS3270_abyss_k27-contigs.fa 21327 205 17 527 332444 716132 1265237 791882 1880850 36.51e6 CS3270_abyss_k27-scaffolds.fa Let\u2019s check a bit anyway: less CS3270_abyss_k27.log less coverage.hist gnuplot gnuplot> set xrange [0:50] gnuplot> set yrange [0:4000000] gnuplot> plot \"coverage.hist\" kat comp -o reads_vs_abyss_k27 -t 4 -C -D '../*.fastq' CS3270_abyss_k27-scaffolds.fa","title":"Task 1.2: First pass assembly, k=27"},{"location":"eukaryotic/handout/denovo_practical/#question-any-tools-you-can-use-to-check-kmer-spectra-at-any-k-before-assembling","text":"","title":"Question: any tools you can use to check kmer spectra at any K before assembling?"},{"location":"eukaryotic/handout/denovo_practical/#question-can-you-predict-what-will-happen-if-you-use-kat-with-larger-k-values","text":"","title":"Question: can you predict what will happen if you use KAT with larger K values?"},{"location":"eukaryotic/handout/denovo_practical/#task-13-will-the-assembly-answer-the-biological-question","text":"Use blast and the proteins to check\u2026","title":"Task 1.3: Will the assembly answer the biological question?"},{"location":"eukaryotic/handout/denovo_practical/#exercise-2-chalara-scaffolding-using-lmp","text":"","title":"Exercise #2: Chalara scaffolding using LMP"},{"location":"eukaryotic/handout/denovo_practical/#task-21-lets-have-a-look-at-the-pe-assembly","text":"abyss-pe name=cha1 k=27 in=\"../LIB2570_raw_R1.fastq ../LIB2570_raw_R2.fastq\" np=4 n n:500 L50 min N80 N50 N20 E-size max sum name 394789 11681 2094 500 2880 6254 12303 7936 44414 44.07e6 cha1-unitigs.fa 394199 11673 2097 500 2887 6255 12303 7937 44414 44.11e6 cha1-contigs.fa 394161 11647 2095 500 2898 6269 12303 7944 44414 44.12e6 cha1-scaffolds.fa abyss-pe name=cha2 k=61 in=\"../LIB2570_raw_R1.fastq ../LIB2570_raw_R2.fastq\" np=4 n n:500 L50 min N80 N50 N20 E-size max sum name \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 130547 12596 1770 500 3352 8379 16380 10518 54300 50.75e6 cha2-unitigs.fa 130210 12575 1771 500 3363 8382 16380 10525 54300 50.78e6 cha2-contigs.fa 130182 12555 1769 500 3377 8394 16380 10534 54300 50.78e6 cha2-scaffolds.fa Contiguity is worse than fusarium, why? Answers: Genome characteristics. Data: Coverage Error distributions Read sizes Fragment sizes Kmer spectra","title":"Task 2.1: let's have a look at the PE assembly"},{"location":"eukaryotic/handout/denovo_practical/#task-22-lets-put-some-lmp-in-there","text":"abyss-pe name=chalmp1 k=61 in=\"../LIB2570_raw_R1.fastq ../LIB2570_raw_R2.fastq\" mp=\"lmp1\" lmp1=\"../LIB8209_raw_R1.fastq ../LIB8209_raw_R2.fastq\" np=4 n |n:500 |L50 |min |N80 |N50 |N20 |E-size |max |sum |name \u2014 |\u2014 |\u2014 |\u2014 |\u2014 |\u2014 |\u2014 |\u2014 |\u2014 |\u2014 |\u2014 130548 |12596 |1770 |500 |3352 |8379 |16380 |10518 |54300 |50.75e6 |chalmp1-unitigs.fa 130211 |12575 |1771 |500 |3363 |8382 |16380 |10525 |54300 |50.78e6 |chalmp1-contigs.fa 130148 |12545 |1769 |500 |3380 |8400 |16380 |10535 |54300 |50.78e6 |chalmp1-scaffolds.fa So\u2026 what happened? Data: Kmer spectra Fragment sizes Any hints on the protocol? A not-so-obvious property: kat plot spectra-mx --intersection -x 30 -y 15000000 -o pe_vs_lmp-main.mx.spectra-mx.png pe_vs_lmp-main.mx LMP require pre-processing, right?","title":"Task 2.2: Let's put some LMP in there."},{"location":"eukaryotic/handout/denovo_practical/#task-23-lets-try-with-processed-lmp","text":"Prior task (already made) preprocess the LMP with nextclip. abyss-pe name=chalmpproc1 k=61 in=\"../../cha_raw/LIB2570_raw_R1.fastq ../../cha_raw/LIB2570_raw_R2.fastq\" mp=\"proclmp1\" proclmp1=\"../LIB8209_preproc_R1.fastq ../LIB8209_preproc_R2.fastq\" np=4 n n:500 L50 min N80 N50 N20 E-size max sum name 130548 12596 1770 500 3352 8379 16380 10518 54300 50.75e6 chalmpproc1-unitigs.fa 130211 12575 1771 500 3363 8382 16380 10525 54300 50.78e6 chalmpproc1-contigs.fa 122061 6679 167 500 18609 87510 187171 106178 397967 51.15e6 chalmpproc1-scaffolds.fa That\u2019s much better!","title":"Task 2.3: Let's try with processed LMP"},{"location":"eukaryotic/handout/denovo_practical/#excercise-3-chalara-beyond-first-pass","text":"Do you remember these? Genome characteristics. Data: Coverage Error distributions Read sizes Fragment sizes Kmer spectra Let\u2019s think a bit and try to improve the assembly\u2026","title":"Excercise #3: Chalara: beyond first pass"},{"location":"eukaryotic/handout/denovo_practical/#question-if-you-look-at-the-pre-processed-lmp-do-you-notice-anything-peculiar","text":"","title":"Question: if you look at the pre-processed LMP, do you notice anything peculiar?"},{"location":"eukaryotic/handout/denovo_practical/#example-testing-the-inclussion-of-heavily-pre-procesed-lmp-coverage-into-the-dbg","text":"abyss-pe name=chalmp2 k=61 se=\"../LIB8209_preproc_single.fastq\" lib=\"lmp1\" lmp1=\"../LIB8209_preproc_R1.fastq ../LIB8209_preproc_R2.fastq\" np=4 >chaproclmp2.log 2>&1 n n:500 L50 min N80 N50 N20 E-size max sum name 128306 10150 1182 500 4954 12585 24777 15693 68684 51.03e6 chaproclmp4-unitigs.fa 118939 5772 362 500 15717 41870 82033 52819 252066 52.24e6 chaproclmp4-contigs.fa 116139 4014 141 500 41145 109273 224986 133450 460979 52.56e6 chaproclmp4-scaffolds.fa","title":"Example: Testing the inclussion of heavily pre-procesed LMP coverage into the DBG"},{"location":"eukaryotic/handout/handout/","text":"Key Learning Outcomes After completing this module the trainee should be able to: Assess general assembly approach, kmer spectra and biases. Visually inspect the kmer spectra and KAT plots Run a first pass eukaryotic assembly and do goal checks Develop validation metrics or tools for NGS data and assembly. Improving methods and pipelines for genome assembly. Convince the lab guys to tweak protocols. Resources You\u2019ll be Using Tools Used Kmer Analysis Tool kit: https://github.com/TGAC/KAT Nextclip: https://github.com/richardmleggett/nextclip Abyss: http://www.bcgsc.ca/platform/bioinfo/software/abyss Soap Denovo: http://soap.genomics.org.cn/soapdenovo.html SOAPec: http://soap.genomics.org.cn/about.html BLAST: http://blast.ncbi.nlm.nih.gov/Blast.cgi First Pass Genome Assembly Assuming by now you are familiar with the general concept of de novo assembly, kmers and the de Bruijn graph based assembler. In this tutorial we will use ABySS to perform the first pass assembly of a eukaryotic genome and look at various parameters to assess the information content of the input data and choice of assembly parameters. sequence data.\\ Genome assembly is a challenging problem requiring heavy computational resources, expertise and time. Before you beging the process of denovo assembly there are a number of points you need to consider: What is the objective of your assembly experiment? What biological question(s) you have? Is assembly strictly neccessary for the purpose in question? Do you have right kind of data and enough coverage to start with? Do you have suitable computaitonal resources to run this assembly? Remember that the assembly is just a probabilistic model of a genome, condensing the information from the experimental evidence. All the information is already present in the experimental results. The goal of the assembly is to find the right motifs, the correct number of times, in correct order and position. Fusarium first pass with a goal Goal: Identify a fusarium sample is ``closer\u201d to F. graminearum or F.pseudograminearum Previous knowledge F. graminearum has a cluster producing PKS6 and NRSP7, while F. pseudograminearum produces PKS40 and NRPS32 Data Proteins sequences for: F. graminearum (non necrotrophic): PKS6 and NRSP7 F. pseudograminearum (necrotrophic): PKS40 and NRPS32 Strategy: Check PKS6-NRSP7 and PKS40-NRPS32 cluster presence. Assembly goals: Assembly goal (I): to capture a good enough representation of the protein-coding space to get blast matches Assembly goal (II): to accurately represent the relevant whole cluster loci in a single sequence. Prepare the Environment Open the Terminal. First, go to the right folder, where the data are stored. cd /home/trainee/eukaryotic ls Task1.1: First pass assembly, k=71 Let\u2019s assemble Fusarium with abyss, k=71 cd /home/trainee/eukaryotic mkdir abyss_k71 cd abyss_k71 abyss-pe in=\"../CS3270_A8733_GCCAAT_L001_R1.fastq ../CS3270_A8733_GCCAAT_L001_R2.fastq\" k=71 name=CS3270_abyss_k71 np=4 &gt; CS3270_abyss_k71.log 2&gt;&amp;1 Description of the arguments used in the command: k : = kmer size np : = number of processors to be used sequence file names : = R1 and R2 reads of a paired end sequence data Let\u2019s look at the statistics of the assembly we just did\u2026 Ok, there is no stats available in the folder, but we can always use abyss-fac to get the stats: abyss-fac CS3270_abyss_k71-*tigs.fa | tee CS3270_abyss_k71-stats.tab less CS3270_abyss_k71-stats.tab n n:500 L50 min N80 N50 N20 E-size max sum name 27 13 2 970 6004 13202 52602 28712 52602 112849 CS3270_abyss_k71-unitigs.fa 5 1 1 128429 128429 128429 128429 128429 128429 128429 CS3270_abyss_k71-contigs.fa [tab:fusariumk71] Question How many unitigs/contigs do you have in the assembly?\\ Answer 27/5 Question What are the length statistics of your assembly? Answer in the table above Question Does it match what you think before the assembly and why?\\ Answer No The assembly is looking strange! It\u2019s time for some analysis: Check frequencies for kmers kept/discarded/etc. Check spectra-cn and compare with expectations. Let\u2019s do this by the following commands: less CS3270_abyss_k71.log less coverage.hist We will now plot the values from the coverage.hist : gnuplot &lt;Press enter&gt; gnuplot&gt; set xrange [0:200] gnuplot&gt; set yrange [0:5000] gnuplot&gt; plot \"coverage.hist\" Type exit or quit to leave the gnuplot gnuplot&gt; exit Looks like we are not assembling this bit, let\u2019s have another look at the spectra kat comp -o reads_vs_abyss_k71 -t 4 -C -D --d1_bins 2000 '../*.fastq' CS3270_abyss_k71-contigs.fa Description of the arguments used in the command: o : = Path prefix for files generated by this program. t : = The number of threads to use. C : = Whether the jellyfish hash for input 1 contains K-mers produced for both strands D : = Whether the jellyfish hash for input 2 contains K-mers produced for both strands \u2013d1_bins : = Number of bins for the first dataset. i.e. number of rows in the matrix kat plot spectra-cn -y 600 -x 2000 -o reads_vs_abyss1-main.mx.spectra-cn_2000.png reads_vs_abyss_k71-main.mx Description of the arguments used in the command: x : = Maximum value for the x-axis (default value auto calculated from matrix, otherwise 1000) y : = Maximum value for the y-axis (default value auto calculated from matrix if possible, otherwise, 1000000) o : = The path to the output file The kmer spectra for Fusarium assembly with abyss, k=71 should be looking like this: [H] [fig:fusariumk71] Take the output and BLAST it in NCBI. What is it? Surprising? Choosing a wrong k value (too large in this case) and just running a typical assembly job, we can end up with something quite more interesting. It is easy by comparison to spot some missing content, alongside duplications and triplications (and quadruplications and so on) that should not be there. This assembly will get us nowhere, let\u2019s choose a lower K to gain coverage and start again. Task1.2: First pass assembly, k=27 We now assemble fusarium with abyss and k=27: cd /home/trainee/eukaryotic mkdir abyss_k27 cd abyss_k27 abyss-pe in=\"../CS3270_A8733_GCCAAT_L001_R1.fastq ../CS3270_A8733_GCCAAT_L001_R2.fastq\" k=27 name=CS3270_abyss_k27 np=4 &gt; CS3270_abyss_k27.log 2&gt;&amp;1 Let\u2019s look at the stats by doing: less CS3270_abyss_k27-stats.tab Stats look better: n n:500 L50 min N80 N50 N20 E-size max sum name 30645 2717 430 502 11354 25336 47966 31027 147694 36.14e6 CS3270_abyss_k27-unitigs.fa 21511 350 33 527 157565 338989 630228 407098 1265237 36.52e6 CS3270_abyss_k27-contigs.fa 21327 205 17 527 332444 716132 1265237 791882 1880850 36.51e6 CS3270_abyss_k27-scaffolds.fa [tab:fusariumk27] Let\u2019s check a bit anyway: less CS3270_abyss_k27.log less coverage.hist How is the coverage plot looking now? gnuplot gnuplot&gt; set xrange [0:50] gnuplot&gt; set xrange [0:4000000] gnuplot&gt; plot \"coverage.hist\" gnuplot&gt; exit K-mer spectrum: kat plot spectra-cn -y 1000 -x 1000 -o reads_vs_abyss1-main.mx.spectra-cn_noabsent.png reads_vs_abyss1-main.mx kat comp -o reads_vs_abyss_k27 -t 4 -C -D '../*.fastq' CS3270_abyss_k27-scaffolds.fa [H] [fig:fusariumk27] Question Any tools you can use to check kmer spectra at any K before assembling? Answer KAT Question Can you predict what will happen if you use KAT with larger K values?\\ Question Will the assembly answer the biological question? Use BLAST or BLAT and the databases to check\u2026 makeblastdb -in CS3270_abyss_k27-scaffolds.fa -dbtype nucl blat \u2013t=dnax \u2013q=prot \u2013minIdentity=90 CS3270_abyss_k27-scaffolds.fa test_genes.fasta out.psl References De novo genome assembly: what every biologist should know Nature Methods 9, 333 \u2013 337 (2012) doi:10.1038/nmeth.1935","title":"Eukaryote Genome Assembly"},{"location":"eukaryotic/handout/handout/#key-learning-outcomes","text":"After completing this module the trainee should be able to: Assess general assembly approach, kmer spectra and biases. Visually inspect the kmer spectra and KAT plots Run a first pass eukaryotic assembly and do goal checks Develop validation metrics or tools for NGS data and assembly. Improving methods and pipelines for genome assembly. Convince the lab guys to tweak protocols.","title":"Key Learning Outcomes"},{"location":"eukaryotic/handout/handout/#resources-youll-be-using","text":"","title":"Resources You\u2019ll be Using"},{"location":"eukaryotic/handout/handout/#tools-used","text":"Kmer Analysis Tool kit: https://github.com/TGAC/KAT Nextclip: https://github.com/richardmleggett/nextclip Abyss: http://www.bcgsc.ca/platform/bioinfo/software/abyss Soap Denovo: http://soap.genomics.org.cn/soapdenovo.html SOAPec: http://soap.genomics.org.cn/about.html BLAST: http://blast.ncbi.nlm.nih.gov/Blast.cgi","title":"Tools Used"},{"location":"eukaryotic/handout/handout/#first-pass-genome-assembly","text":"Assuming by now you are familiar with the general concept of de novo assembly, kmers and the de Bruijn graph based assembler. In this tutorial we will use ABySS to perform the first pass assembly of a eukaryotic genome and look at various parameters to assess the information content of the input data and choice of assembly parameters. sequence data.\\ Genome assembly is a challenging problem requiring heavy computational resources, expertise and time. Before you beging the process of denovo assembly there are a number of points you need to consider: What is the objective of your assembly experiment? What biological question(s) you have? Is assembly strictly neccessary for the purpose in question? Do you have right kind of data and enough coverage to start with? Do you have suitable computaitonal resources to run this assembly? Remember that the assembly is just a probabilistic model of a genome, condensing the information from the experimental evidence. All the information is already present in the experimental results. The goal of the assembly is to find the right motifs, the correct number of times, in correct order and position.","title":"First Pass Genome Assembly"},{"location":"eukaryotic/handout/handout/#fusarium-first-pass-with-a-goal","text":"Goal: Identify a fusarium sample is ``closer\u201d to F. graminearum or F.pseudograminearum Previous knowledge F. graminearum has a cluster producing PKS6 and NRSP7, while F. pseudograminearum produces PKS40 and NRPS32 Data Proteins sequences for: F. graminearum (non necrotrophic): PKS6 and NRSP7 F. pseudograminearum (necrotrophic): PKS40 and NRPS32 Strategy: Check PKS6-NRSP7 and PKS40-NRPS32 cluster presence. Assembly goals: Assembly goal (I): to capture a good enough representation of the protein-coding space to get blast matches Assembly goal (II): to accurately represent the relevant whole cluster loci in a single sequence.","title":"Fusarium first pass with a goal"},{"location":"eukaryotic/handout/handout/#prepare-the-environment","text":"Open the Terminal. First, go to the right folder, where the data are stored. cd /home/trainee/eukaryotic ls","title":"Prepare the Environment"},{"location":"eukaryotic/handout/handout/#task11-first-pass-assembly-k71","text":"Let\u2019s assemble Fusarium with abyss, k=71 cd /home/trainee/eukaryotic mkdir abyss_k71 cd abyss_k71 abyss-pe in=\"../CS3270_A8733_GCCAAT_L001_R1.fastq ../CS3270_A8733_GCCAAT_L001_R2.fastq\" k=71 name=CS3270_abyss_k71 np=4 &gt; CS3270_abyss_k71.log 2&gt;&amp;1 Description of the arguments used in the command: k : = kmer size np : = number of processors to be used sequence file names : = R1 and R2 reads of a paired end sequence data Let\u2019s look at the statistics of the assembly we just did\u2026 Ok, there is no stats available in the folder, but we can always use abyss-fac to get the stats: abyss-fac CS3270_abyss_k71-*tigs.fa | tee CS3270_abyss_k71-stats.tab less CS3270_abyss_k71-stats.tab n n:500 L50 min N80 N50 N20 E-size max sum name 27 13 2 970 6004 13202 52602 28712 52602 112849 CS3270_abyss_k71-unitigs.fa 5 1 1 128429 128429 128429 128429 128429 128429 128429 CS3270_abyss_k71-contigs.fa [tab:fusariumk71] Question How many unitigs/contigs do you have in the assembly?\\ Answer 27/5 Question What are the length statistics of your assembly? Answer in the table above Question Does it match what you think before the assembly and why?\\ Answer No The assembly is looking strange! It\u2019s time for some analysis: Check frequencies for kmers kept/discarded/etc. Check spectra-cn and compare with expectations. Let\u2019s do this by the following commands: less CS3270_abyss_k71.log less coverage.hist We will now plot the values from the coverage.hist : gnuplot &lt;Press enter&gt; gnuplot&gt; set xrange [0:200] gnuplot&gt; set yrange [0:5000] gnuplot&gt; plot \"coverage.hist\" Type exit or quit to leave the gnuplot gnuplot&gt; exit Looks like we are not assembling this bit, let\u2019s have another look at the spectra kat comp -o reads_vs_abyss_k71 -t 4 -C -D --d1_bins 2000 '../*.fastq' CS3270_abyss_k71-contigs.fa Description of the arguments used in the command: o : = Path prefix for files generated by this program. t : = The number of threads to use. C : = Whether the jellyfish hash for input 1 contains K-mers produced for both strands D : = Whether the jellyfish hash for input 2 contains K-mers produced for both strands \u2013d1_bins : = Number of bins for the first dataset. i.e. number of rows in the matrix kat plot spectra-cn -y 600 -x 2000 -o reads_vs_abyss1-main.mx.spectra-cn_2000.png reads_vs_abyss_k71-main.mx Description of the arguments used in the command: x : = Maximum value for the x-axis (default value auto calculated from matrix, otherwise 1000) y : = Maximum value for the y-axis (default value auto calculated from matrix if possible, otherwise, 1000000) o : = The path to the output file The kmer spectra for Fusarium assembly with abyss, k=71 should be looking like this: [H] [fig:fusariumk71] Take the output and BLAST it in NCBI. What is it? Surprising? Choosing a wrong k value (too large in this case) and just running a typical assembly job, we can end up with something quite more interesting. It is easy by comparison to spot some missing content, alongside duplications and triplications (and quadruplications and so on) that should not be there. This assembly will get us nowhere, let\u2019s choose a lower K to gain coverage and start again.","title":"Task1.1: First pass assembly, k=71"},{"location":"eukaryotic/handout/handout/#task12-first-pass-assembly-k27","text":"We now assemble fusarium with abyss and k=27: cd /home/trainee/eukaryotic mkdir abyss_k27 cd abyss_k27 abyss-pe in=\"../CS3270_A8733_GCCAAT_L001_R1.fastq ../CS3270_A8733_GCCAAT_L001_R2.fastq\" k=27 name=CS3270_abyss_k27 np=4 &gt; CS3270_abyss_k27.log 2&gt;&amp;1 Let\u2019s look at the stats by doing: less CS3270_abyss_k27-stats.tab Stats look better: n n:500 L50 min N80 N50 N20 E-size max sum name 30645 2717 430 502 11354 25336 47966 31027 147694 36.14e6 CS3270_abyss_k27-unitigs.fa 21511 350 33 527 157565 338989 630228 407098 1265237 36.52e6 CS3270_abyss_k27-contigs.fa 21327 205 17 527 332444 716132 1265237 791882 1880850 36.51e6 CS3270_abyss_k27-scaffolds.fa [tab:fusariumk27] Let\u2019s check a bit anyway: less CS3270_abyss_k27.log less coverage.hist How is the coverage plot looking now? gnuplot gnuplot&gt; set xrange [0:50] gnuplot&gt; set xrange [0:4000000] gnuplot&gt; plot \"coverage.hist\" gnuplot&gt; exit K-mer spectrum: kat plot spectra-cn -y 1000 -x 1000 -o reads_vs_abyss1-main.mx.spectra-cn_noabsent.png reads_vs_abyss1-main.mx kat comp -o reads_vs_abyss_k27 -t 4 -C -D '../*.fastq' CS3270_abyss_k27-scaffolds.fa [H] [fig:fusariumk27] Question Any tools you can use to check kmer spectra at any K before assembling? Answer KAT Question Can you predict what will happen if you use KAT with larger K values?\\ Question Will the assembly answer the biological question? Use BLAST or BLAT and the databases to check\u2026 makeblastdb -in CS3270_abyss_k27-scaffolds.fa -dbtype nucl blat \u2013t=dnax \u2013q=prot \u2013minIdentity=90 CS3270_abyss_k27-scaffolds.fa test_genes.fasta out.psl","title":"Task1.2: First pass assembly, k=27"},{"location":"eukaryotic/handout/handout/#references","text":"De novo genome assembly: what every biologist should know Nature Methods 9, 333 \u2013 337 (2012) doi:10.1038/nmeth.1935","title":"References"},{"location":"eukaryotic/presentations/","text":"This directory is intended to store presentation files used to provide context to the hands-on aspect of the module.","title":"Home"},{"location":"eukaryotic/tools/","text":"The tools used in the module\u2019s tutorial exercises are captured in a tools.yaml file. This file can then be used by a workshop deployment script to download and install the tools required by the module.","title":"Home"},{"location":"post-workshop/","text":"btp-module-nectar Bioinformatics Training Platform (BTP) Module: Using the NeCTAR Research Cloud Topic How to access the Australian NeCTAR Research Cloud Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes How to access computational resources on the Australian NeCTAR Research Cloud How to instantiate VM\u2019s on the NeCTAR Research Cloud How to setup your own local VirtualBox VM How to connect to a VM using an NX client Time Required 1 hr License The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"btp-module-nectar"},{"location":"post-workshop/#btp-module-nectar","text":"Bioinformatics Training Platform (BTP) Module: Using the NeCTAR Research Cloud Topic How to access the Australian NeCTAR Research Cloud Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes How to access computational resources on the Australian NeCTAR Research Cloud How to instantiate VM\u2019s on the NeCTAR Research Cloud How to setup your own local VirtualBox VM How to connect to a VM using an NX client Time Required 1 hr","title":"btp-module-nectar"},{"location":"post-workshop/#license","text":"The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"License"},{"location":"preamble/preamble/","text":"Providing Feedback While we endeavour to deliver a workshop with quality content and documentation in a venue conducive to an exciting, well run hands-on workshop with a bunch of knowledgeable and likable trainers, we know there are things we could do better. Whilst we want to know what didn\u2019t quite hit the mark for you, what would be most helpful and least depressing, would be for you to provide ways to improve the workshop. i.e. constructive feedback. After all, if we knew something wasn\u2019t going to work, we wouldn\u2019t have done it or put it into the workshop in the first place! Remember, we\u2019re experts in the field of bioinformatics not experts in the field of biology! Clearly, we also want to know what we did well! This gives us that \u201cfeel good\u201d factor which will see us through those long days and nights in the lead up to such hands-on workshops! With that in mind, we\u2019ll provide three really high tech mechanism through which you can provide anonymous feedback during the workshop: A sheet of paper, from a flip-chart, sporting a \u201chappy\u201d face and a \u201cnot so happy\u201d face. Armed with a stack of colourful post-it notes, your mission is to see how many comments you can stick on the \u201chappy\u201d side! Some empty ruled pages at the back of this handout. Use them for your own personal notes or for write specific comments/feedback about the workshop as it progresses. An online post-workshop evaluation survey. We\u2019ll ask you to complete this before you leave. If you\u2019ve used the blank pages at the back of this handout to make feedback notes, you\u2019ll be able to provide more specific and helpful feedback with the least amount of brain-drain! Document Structure We have provided you with an electronic copy of the workshop\u2019s hands-on tutorial documents. We have done this for two reasons: 1) you will have something to take away with you at the end of the workshop, and 2) you can save time (mis)typing commands on the command line by using copy-and-paste. While you could fly through the hands-on sessions doing copy-and-paste you will learn more if you take the time, saved from not having to type all those commands, to understand what each command is doing! The commands to enter at a terminal look something like this: tophat --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_2cells genome/ZV9 data/2cells_1.fastq data/2cells_2.fastq The following styled code is not to be entered at a terminal, it is simply to show you the syntax of the command. You must use your own judgement to substitute in the correct arguments, options, filenames etc ``` {style=\u201dcommand_syntax\u201d} tophat [options]* The following is an example how of R commands are styled : `` ` { style = \"R\" } R -- no - save library ( plotrix ) data <- read.table ( \"run_25/stats.txt\" , header = TRUE ) weighted.hist ( data $ short1_cov + data $ short2_cov , data $ lgth , breaks = 0 : 70 ) q () The following icons are used in the margin, throughout the documentation to help you navigate around the document more easily: \\hspace*{.2cm} Important\\ For reference\\ Follow these steps\\ Questions to answer\\ Warning - STOP and read\\ Bonus exercise for fast learners\\ Advanced exercise for super-fast learners\\ Resources Used We have provided you with an environment which contains all the tools and data you need for the duration of this workshop. However, we also provide details about the tools and data used by each module at the start of the respective module documentation.","title":"Workshop Information"},{"location":"preamble/preamble/#providing-feedback","text":"While we endeavour to deliver a workshop with quality content and documentation in a venue conducive to an exciting, well run hands-on workshop with a bunch of knowledgeable and likable trainers, we know there are things we could do better. Whilst we want to know what didn\u2019t quite hit the mark for you, what would be most helpful and least depressing, would be for you to provide ways to improve the workshop. i.e. constructive feedback. After all, if we knew something wasn\u2019t going to work, we wouldn\u2019t have done it or put it into the workshop in the first place! Remember, we\u2019re experts in the field of bioinformatics not experts in the field of biology! Clearly, we also want to know what we did well! This gives us that \u201cfeel good\u201d factor which will see us through those long days and nights in the lead up to such hands-on workshops! With that in mind, we\u2019ll provide three really high tech mechanism through which you can provide anonymous feedback during the workshop: A sheet of paper, from a flip-chart, sporting a \u201chappy\u201d face and a \u201cnot so happy\u201d face. Armed with a stack of colourful post-it notes, your mission is to see how many comments you can stick on the \u201chappy\u201d side! Some empty ruled pages at the back of this handout. Use them for your own personal notes or for write specific comments/feedback about the workshop as it progresses. An online post-workshop evaluation survey. We\u2019ll ask you to complete this before you leave. If you\u2019ve used the blank pages at the back of this handout to make feedback notes, you\u2019ll be able to provide more specific and helpful feedback with the least amount of brain-drain!","title":"Providing Feedback"},{"location":"preamble/preamble/#document-structure","text":"We have provided you with an electronic copy of the workshop\u2019s hands-on tutorial documents. We have done this for two reasons: 1) you will have something to take away with you at the end of the workshop, and 2) you can save time (mis)typing commands on the command line by using copy-and-paste. While you could fly through the hands-on sessions doing copy-and-paste you will learn more if you take the time, saved from not having to type all those commands, to understand what each command is doing! The commands to enter at a terminal look something like this: tophat --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_2cells genome/ZV9 data/2cells_1.fastq data/2cells_2.fastq The following styled code is not to be entered at a terminal, it is simply to show you the syntax of the command. You must use your own judgement to substitute in the correct arguments, options, filenames etc ``` {style=\u201dcommand_syntax\u201d} tophat [options]* The following is an example how of R commands are styled : `` ` { style = \"R\" } R -- no - save library ( plotrix ) data <- read.table ( \"run_25/stats.txt\" , header = TRUE ) weighted.hist ( data $ short1_cov + data $ short2_cov , data $ lgth , breaks = 0 : 70 ) q () The following icons are used in the margin, throughout the documentation to help you navigate around the document more easily: \\hspace*{.2cm} Important\\ For reference\\ Follow these steps\\ Questions to answer\\ Warning - STOP and read\\ Bonus exercise for fast learners\\ Advanced exercise for super-fast learners\\","title":"Document Structure"},{"location":"preamble/preamble/#resources-used","text":"We have provided you with an environment which contains all the tools and data you need for the duration of this workshop. However, we also provide details about the tools and data used by each module at the start of the respective module documentation.","title":"Resources Used"},{"location":"qc/","text":"NGS Quality Control Bioinformatics Training Platform (BTP) Module: NGS Quality Control Topic NGS Quality Control Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes Assess the overall quality of NGS (FastQ format) sequence reads Visualise the quality, and other associated matrices, of reads to decide on filters and cutoffs for cleaning up data ready for downstream analysis Clean up adaptors and pre-process the sequence data for further analysis Time Required 2 hrs License The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"NGS Quality Control"},{"location":"qc/#ngs-quality-control","text":"Bioinformatics Training Platform (BTP) Module: NGS Quality Control Topic NGS Quality Control Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes Assess the overall quality of NGS (FastQ format) sequence reads Visualise the quality, and other associated matrices, of reads to decide on filters and cutoffs for cleaning up data ready for downstream analysis Clean up adaptors and pre-process the sequence data for further analysis Time Required 2 hrs","title":"NGS Quality Control"},{"location":"qc/#license","text":"The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"License"},{"location":"qc/handout/","text":"NGS Quality Control Acknowledgements This module was developed by Bioplatforms Australia, in partnership with CSIRO, and with support from the European Bioinformatics Institute (EBI), a member of the European Molecular Biology Laboratory (EMBL) in the UK. The module content has been maintained and updated by a network of dedicated [bioinformatics trainers] ( http://www.bioplatforms.com/bioinformatics-training/ ) from around Australia. The Bioinformatics Training Platform was developed in collaboration with the Monash Bioinformatics Platform. The development of this module is also supported by the NSW Office of Health and Medical Research and the Commonwealth Government National Collaborative Research Infrastructure Strategy (NCRIS).","title":"NGS Quality Control"},{"location":"qc/handout/#ngs-quality-control","text":"","title":"NGS Quality Control"},{"location":"qc/handout/#acknowledgements","text":"This module was developed by Bioplatforms Australia, in partnership with CSIRO, and with support from the European Bioinformatics Institute (EBI), a member of the European Molecular Biology Laboratory (EMBL) in the UK. The module content has been maintained and updated by a network of dedicated [bioinformatics trainers] ( http://www.bioplatforms.com/bioinformatics-training/ ) from around Australia. The Bioinformatics Training Platform was developed in collaboration with the Monash Bioinformatics Platform. The development of this module is also supported by the NSW Office of Health and Medical Research and the Commonwealth Government National Collaborative Research Infrastructure Strategy (NCRIS).","title":"Acknowledgements"},{"location":"qc/handout/handout/","text":"Key Learning Outcomes After completing this practical the trainee should be able to: Assess the overall quality of NGS (FastQ format) sequence reads Visualise the quality, and other associated matrices, of reads to decide on filters and cutoffs for cleaning up data ready for downstream analysis Clean up adaptors and pre-process the sequence data for further analysis Resources You\u2019ll be Using Tools Used FastQC: http://www.bioinformatics.babraham.ac.uk/projects/fastqc/ Skewer: http://sourceforge.net/projects/skewer/ FASTX-Toolkit: http://hannonlab.cshl.edu/fastx_toolkit/ Useful Links FASTQ Encoding: ( http://en.wikipedia.org/wiki/FASTQ_format#Encoding ) Author Information Primary Author(s): Sonika Tyagi: sonika.tyagi@monash.edu Contributor(s): Nandan Deshpande: n.deshpande@unsw.edu.au Introduction Going on a blind date with your read set? For a better understanding of the consequences please check the data quality! For the purpose of this tutorial we are focusing only on Illumina sequencing which uses \u2019sequence by synthesis\u2019 technology in a highly parallel fashion. Although Illumina high throughput sequencing provides highly accurate sequence data, several sequence artifacts, including base calling errors and small insertions/deletions, poor quality reads and primer/adapter contamination are quite common in the high throughput sequencing data. The primary errors are substitution errors. The error rates can vary from 0.5-2.0% with errors mainly rising in frequency at the 3\u2019 ends of reads. One way to investigate sequence data quality is to visualize the quality scores and other metrics in a compact manner to get an idea about the quality of a read data set. Read data sets can be improved by pre processing in different ways like trimming off low quality bases, cleaning up any sequencing adapters, removing PCR duplicates and screening for contamination. We can also look at other statistics such as, sequence length distribution, base composition, sequence complexity, presence of ambiguous bases etc. to assess the overall quality of the data set. Highly redundant coverage (>15X) of the genome can be used to correct sequencing errors in the reads before assembly. Various k-mer based error correction methods exist but are beyond the scope of this tutorial. Quality Value Encoding Schema Quality scoring calculates a set of predictors for each base call, and then uses the predictor values to look up the Q-score in a quality table. Quality tables are created to provide optimally accurate quality predictions for runs generated by a specific configuration of sequencing platform and version of chemistry ( www.illumina.com ). In order to use a single character to encode Phred qualities, ASCII characters are used. All ASCII characters have a decimal number associated with them but the first 32 characters are non-printable (e.g. backspace, shift, return, escape). Therefore, the first printable ASCII character is number 33, the exclamation mark ( ! ). In Phred+33 encoded quality values the exclamation mark takes the Phred quality score of zero. Early Solexa (now Illumina) sequencing needed to encode negative quality values. Because ASCII characters < 33 are non-printable, using the Phred+33 encoding was not possible. Therefore, they simply moved the offset from 33 to 64 thus inventing the Phred+64 encoded quality values. In this encoding a Phred quality of zero is denoted by the ASCII number 64 (the @ character). Since Illumina 1.8, quality values are now encoded using Phred+33. FASTQ does not provide a way to describe what quality encoding is used for the quality values. Therefore, you should find this out from your sequencing provider. Alternatively, you may be able to figure this out by determining what ASCII characters are present in the FASTQ file. E.g the presence of numbers in the quality strings, can only mean the quality values are Phred+33 encoded. However, due to the overlapping nature of the Phred+33 and Phred+64 encoding schema it is not always possible to identify what encoding is in use. For example, if the only characters seen in the quality string are ( @ABCDEFGHI ), then it is impossible to know if you have really good Phred+33 encoded qualities or really bad Phred+64 encoded qualities. For a graphical representation of the different ASCII characters used in the two encoding schema see: ( http://en.wikipedia.org/wiki/FASTQ_format#Encoding ). Q-score encoding implemented with the Novaseq platform In order to reduce the data footprints Illumina has come up with a new method to reduce quality score resolution and optimise data storae. The new Q-score encoding now follows an 8 level mapping of individual quality scores (0-40 or >40) [See Table 1 ]. With the new scoring scheme the original scores 20-24 may form one bin and the quality scores in that bin mapped to a new value of 22. This can be thought of as simply replacing all the occurrences of scores 20, 21, 23, 24 with a new score of 22 in the output sequence. Illumina claims that with the new Q-scoring system the reduction in the Illumina raw sequence format (.bcl) is typically > 50% and the resulting sorted BAM les are reduced by ~30%. Quality Score Bins Mapped quality scores N (no call) N (no call) 2-9 6 10-19 15 20-24 22 25-29 27 30-34 33 35-39 37 >=40 40 Table 1: Novaseq Q-score bins mapping Prepare the Environment To investigate sequence data quality we will demonstrate tools called FastQC and Skewer. FastQC will process and present the reports in a visual manner. Based on the results, the sequence data can be processed using the Skewer. We will use one data set in this practical, which can be found in the QC directory on your desktop. Open the Terminal and go to the directory where the data are stored: cd ls cd qc pwd At any time, help can be displayed for FastQC using the following command: fastqc -h Look at SYNOPSIS (Usage) and options after typing fastqc -h Quality Visualisation We have a file for a good quality and bad quality statistics. FastQC generates results in the form of a zipped and unzipped directory for each input file. Execute the following command on the two files: fastqc -f fastq qcdemo_R1.fastq.gz fastqc -f fastq qcdemo_R2.fastq.gz View the FastQC report file of the bad data using a web browser such as firefox. The & sign puts the job in the background. firefox qcdemo_R2_fastqc.html &amp; The report file will have a Basic Statistics table and various graphs and tables for different quality statistics e.g.: Property Value Filename qcdemo_R2.fastq.gz File type Conventional base calls Encoding Sanger / Illumina 1.9 Total Sequences 1000000 Filtered Sequences 0 Sequence length 150 %GC 37 Table 2: Summary statistics for bad_example_untrimmed Figure 1: bad_example_untrimmed_QC_plot A Phred quality score (or Q-score) expresses an error probability. In particular, it serves as a convenient and compact way to communicate very small error probabilities. The probability that base A is wrong (P(A)) is expressed by a quality score, Q(A), according to the relationship: Q(A) =-10 log10(P(A)) The relationship between the quality score and error probability is demonstrated with the following table: Quality score, Q(A) Error probability, P(A) Accuracy of base call 10 0.1 90% 20 0.01 99% 30 0.001 99.9% 40 0.0001 99.99% 50 0.00001 99.999% Table 3: Quality Error Probabilities Question How many sequences were there in your file? What is the read length? Answer 1,000,000. read length=150bp Question Does the quality score values vary throughout the read length? Hint Look at the \u2019per base sequence quality plot\u2019 Answer Yes. Quality scores are dropping towards the end of the reads. Question What is the quality score range you see? Answer 2-40 Question At around which position do the scores start falling below Q20 for the 25% quartile range (25%of reads below Q20)? Answer Around 30 bp position Question How can we trim the reads to filter out the low quality data? Answer By trimming off the bases after a fixed position of the read or by trimming off bases based on the quality score. Good Quality Data View the FastQC report files fastqc_report.html to see examples of a good quality data and compare the quality plot with that of the bad_example_fastqc . firefox qcdemo_R1_fastqc.html &amp; Sequencing errors can complicate the downstream analysis, which normally requires that reads be aligned to each other (for genome assembly) or to a reference genome (for detection of mutations). Sequence reads containing errors may lead to ambiguous paths in the assembly or improper gaps. In variant analysis projects sequence reads are aligned against the reference genome. The errors in the reads may lead to more mismatches than expected from mutations alone. But if these errors can be removed or corrected, the read alignments and hence the variant detection will improve. The assemblies will also improve after pre-processing the reads to remove errors. Read Trimming Read trimming can be done in a variety of different ways. Choose a method which best suits your data. Here we are giving examples of fixed-length trimming and quality-based trimming. Quality Based Trimming Base call quality scores can be used to dynamically determine the trim points for each read. A quality score threshold and minimum read length following trimming can be used to remove low quality data. The previous FastQC results show R1 is fine but R2 has low quality at the end. There is no adaptor contamination though. We will be using Skewer to perform the quality trimming. Run the following command to quality trim a set of paired end data. cd /home/trainee/qc skewer -t 4 -l 50 -q 30 -Q 25 -m pe -o qcdemo qcdemo_R1.fastq.gz qcdemo_R2.fastq.gz -t: number of threads to use -l: min length to keep after trimming -q: Quality threshold used for trimming at 3\u2019 end -Q: mean quality threshold for a read -m: pair-end mode Run FastQC on the quality trimmed file and visualise the quality scores. Look at the last files generated, are the file names same as the input ? ls -ltr Run Fastqc on the quality trimmed files: fastqc -f fastq qcdemo-trimmed-pair1.fastq fastqc -f fastq qcdemo-trimmed-pair2.fastq Visualise the fastqc results: firefox qcdemo-trimmed-pair1_fastqc.html &amp; firefox qcdemo-trimmed-pair2_fastqc.html &amp; Let\u2019s look at the quality from the second reads. The output should look like: Property Value Filename qcdemo-trimmed-pair2.fastq File type Conventional base calls Encoding Sanger / Illumina 1.9 Total Sequences 742262 Filtered Sequences 0 Sequence length 50-150 %GC 37 Table 4: Summary Statistics of QC_demo_R1_trimmed Figure 2: bad_example_quality_trimmed_plot Question Did the number of total reads in R1 and R2 change after trimming? Answer Quality trimming discarded >25000 reads. However, we retain a lot of maximal length reads which have good quality all the way to the ends. Question What reads lengths were obtained after quality based trimming? Answer 50-150 Reads <50 bp, following quality trimming, were discarded. Question Did you observe adapter sequences in the data? Answer No. (Hint: look at the overrepresented sequences) Question How can you use -a option with fastqc? (Hint: try fastqc -h). Answer Adaptors can be supplied in a file for screening. Adapter Clipping Sometimes sequence reads may end up getting the leftover of adapters and primers used in the sequencing process. It\u2019s good practice to screen your data for these possible contamination for more sensitive alignment and assembly based analysis. This is particularly important when read lengths can be longer than the molecules being sequenced. For example when sequencing miRNAs. Various QC tools are available to screen and/or clip these adapter/primer sequences from your data. Apart from skewer which will be using today the following two tools are also useful for trimming and removing adapter sequence: Cutadapt: http://code.google.com/p/cutadapt/ Trimmomatic: http://www.usadellab.org/cms/?page=trimmomatic Here we are demonstrating Skewer to trim a given adapter sequence. cd /home/trainee/qc fastqc -f fastq adaptorQC.fastq.gz firefox adaptorQC_fastqc.html skewer -x TGGAATTCTCGGGTGCCAAGGT -t 20 -l 10 -L 35 -q 30 adaptorQC.fastq.gz -x: adaptor sequence used -t: number of threads to use -l: min length to keep after trimming -L: Max length to keep after trimming, in this experiment we were expecting only small RNA fragments -Q: Quality threshold used for trimming at 3\u2019 end. Use -m option to control the end you want to trim Run FastQC on the adapter trimmed file and visualise the quality scores. Fastqc now shows adaptor free results. fastqc adaptorQC.fastq-trimmed.fastq firefox adaptorQC.fastq-trimmed_fastqc.html &amp; Fixed Length Trimming We will not cover fixed length trimming but provide the following for your information. Low quality read ends can be trimmed using a fixed-length trimming. We will use the fastx_trimmer from the FASTX-Toolkit. Usage message to find out various options you can use with this tool. Type fastx_trimmer -h at anytime to display help. We will now do fixed-length trimming of the bad_example.fastq file using the following command. You should still be in the qc directory, if not cd back in. cd /home/trainee/qc fastqc -f fastq bad_example.fastq fastx_trimmer -h fastx_trimmer -Q 33 -f 1 -l 80 -i bad_example.fastq -o bad_example_trimmed01.fastq We used the following options in the command above: -Q 33: Indicates the input quality scores are Phred+33 encoded -f: First base to be retained in the output -l: Last base to be retained in the output -i: Input FASTQ file name -o: Output file name Run FastQC on the trimmed file and visualise the quality scores of the trimmed file. fastqc -f fastq bad_example_trimmed01.fastq firefox bad_example_trimmed01_fastqc.html &amp; The output should look like: Property Value Filename bad_example_trimmed01.fastq File type Conventional base calls Encoding Sanger / Illumina 1.9 Total Sequences 40000 Filtered Sequences 0 Sequence length 80 %GC 48 Table 5: Summary Statistics of bad_example_trimmed summary Figure 3: bad_example_trimmed_plot Question What values would you use for -f if you wanted to trim off 10 bases at the 5\u2019 end of the reads? Answer -f 11","title":"Quality Control"},{"location":"qc/handout/handout/#key-learning-outcomes","text":"After completing this practical the trainee should be able to: Assess the overall quality of NGS (FastQ format) sequence reads Visualise the quality, and other associated matrices, of reads to decide on filters and cutoffs for cleaning up data ready for downstream analysis Clean up adaptors and pre-process the sequence data for further analysis","title":"Key Learning Outcomes"},{"location":"qc/handout/handout/#resources-youll-be-using","text":"","title":"Resources You\u2019ll be Using"},{"location":"qc/handout/handout/#tools-used","text":"FastQC: http://www.bioinformatics.babraham.ac.uk/projects/fastqc/ Skewer: http://sourceforge.net/projects/skewer/ FASTX-Toolkit: http://hannonlab.cshl.edu/fastx_toolkit/","title":"Tools Used"},{"location":"qc/handout/handout/#useful-links","text":"FASTQ Encoding: ( http://en.wikipedia.org/wiki/FASTQ_format#Encoding )","title":"Useful Links"},{"location":"qc/handout/handout/#author-information","text":"Primary Author(s): Sonika Tyagi: sonika.tyagi@monash.edu Contributor(s): Nandan Deshpande: n.deshpande@unsw.edu.au","title":"Author Information"},{"location":"qc/handout/handout/#introduction","text":"Going on a blind date with your read set? For a better understanding of the consequences please check the data quality! For the purpose of this tutorial we are focusing only on Illumina sequencing which uses \u2019sequence by synthesis\u2019 technology in a highly parallel fashion. Although Illumina high throughput sequencing provides highly accurate sequence data, several sequence artifacts, including base calling errors and small insertions/deletions, poor quality reads and primer/adapter contamination are quite common in the high throughput sequencing data. The primary errors are substitution errors. The error rates can vary from 0.5-2.0% with errors mainly rising in frequency at the 3\u2019 ends of reads. One way to investigate sequence data quality is to visualize the quality scores and other metrics in a compact manner to get an idea about the quality of a read data set. Read data sets can be improved by pre processing in different ways like trimming off low quality bases, cleaning up any sequencing adapters, removing PCR duplicates and screening for contamination. We can also look at other statistics such as, sequence length distribution, base composition, sequence complexity, presence of ambiguous bases etc. to assess the overall quality of the data set. Highly redundant coverage (>15X) of the genome can be used to correct sequencing errors in the reads before assembly. Various k-mer based error correction methods exist but are beyond the scope of this tutorial.","title":"Introduction"},{"location":"qc/handout/handout/#quality-value-encoding-schema","text":"Quality scoring calculates a set of predictors for each base call, and then uses the predictor values to look up the Q-score in a quality table. Quality tables are created to provide optimally accurate quality predictions for runs generated by a specific configuration of sequencing platform and version of chemistry ( www.illumina.com ). In order to use a single character to encode Phred qualities, ASCII characters are used. All ASCII characters have a decimal number associated with them but the first 32 characters are non-printable (e.g. backspace, shift, return, escape). Therefore, the first printable ASCII character is number 33, the exclamation mark ( ! ). In Phred+33 encoded quality values the exclamation mark takes the Phred quality score of zero. Early Solexa (now Illumina) sequencing needed to encode negative quality values. Because ASCII characters < 33 are non-printable, using the Phred+33 encoding was not possible. Therefore, they simply moved the offset from 33 to 64 thus inventing the Phred+64 encoded quality values. In this encoding a Phred quality of zero is denoted by the ASCII number 64 (the @ character). Since Illumina 1.8, quality values are now encoded using Phred+33. FASTQ does not provide a way to describe what quality encoding is used for the quality values. Therefore, you should find this out from your sequencing provider. Alternatively, you may be able to figure this out by determining what ASCII characters are present in the FASTQ file. E.g the presence of numbers in the quality strings, can only mean the quality values are Phred+33 encoded. However, due to the overlapping nature of the Phred+33 and Phred+64 encoding schema it is not always possible to identify what encoding is in use. For example, if the only characters seen in the quality string are ( @ABCDEFGHI ), then it is impossible to know if you have really good Phred+33 encoded qualities or really bad Phred+64 encoded qualities. For a graphical representation of the different ASCII characters used in the two encoding schema see: ( http://en.wikipedia.org/wiki/FASTQ_format#Encoding ).","title":"Quality Value Encoding Schema"},{"location":"qc/handout/handout/#q-score-encoding-implemented-with-the-novaseq-platform","text":"In order to reduce the data footprints Illumina has come up with a new method to reduce quality score resolution and optimise data storae. The new Q-score encoding now follows an 8 level mapping of individual quality scores (0-40 or >40) [See Table 1 ]. With the new scoring scheme the original scores 20-24 may form one bin and the quality scores in that bin mapped to a new value of 22. This can be thought of as simply replacing all the occurrences of scores 20, 21, 23, 24 with a new score of 22 in the output sequence. Illumina claims that with the new Q-scoring system the reduction in the Illumina raw sequence format (.bcl) is typically > 50% and the resulting sorted BAM les are reduced by ~30%. Quality Score Bins Mapped quality scores N (no call) N (no call) 2-9 6 10-19 15 20-24 22 25-29 27 30-34 33 35-39 37 >=40 40 Table 1: Novaseq Q-score bins mapping","title":"Q-score encoding implemented with the Novaseq platform"},{"location":"qc/handout/handout/#prepare-the-environment","text":"To investigate sequence data quality we will demonstrate tools called FastQC and Skewer. FastQC will process and present the reports in a visual manner. Based on the results, the sequence data can be processed using the Skewer. We will use one data set in this practical, which can be found in the QC directory on your desktop. Open the Terminal and go to the directory where the data are stored: cd ls cd qc pwd At any time, help can be displayed for FastQC using the following command: fastqc -h Look at SYNOPSIS (Usage) and options after typing fastqc -h","title":"Prepare the Environment"},{"location":"qc/handout/handout/#quality-visualisation","text":"We have a file for a good quality and bad quality statistics. FastQC generates results in the form of a zipped and unzipped directory for each input file. Execute the following command on the two files: fastqc -f fastq qcdemo_R1.fastq.gz fastqc -f fastq qcdemo_R2.fastq.gz View the FastQC report file of the bad data using a web browser such as firefox. The & sign puts the job in the background. firefox qcdemo_R2_fastqc.html &amp; The report file will have a Basic Statistics table and various graphs and tables for different quality statistics e.g.: Property Value Filename qcdemo_R2.fastq.gz File type Conventional base calls Encoding Sanger / Illumina 1.9 Total Sequences 1000000 Filtered Sequences 0 Sequence length 150 %GC 37 Table 2: Summary statistics for bad_example_untrimmed Figure 1: bad_example_untrimmed_QC_plot A Phred quality score (or Q-score) expresses an error probability. In particular, it serves as a convenient and compact way to communicate very small error probabilities. The probability that base A is wrong (P(A)) is expressed by a quality score, Q(A), according to the relationship: Q(A) =-10 log10(P(A)) The relationship between the quality score and error probability is demonstrated with the following table: Quality score, Q(A) Error probability, P(A) Accuracy of base call 10 0.1 90% 20 0.01 99% 30 0.001 99.9% 40 0.0001 99.99% 50 0.00001 99.999% Table 3: Quality Error Probabilities Question How many sequences were there in your file? What is the read length? Answer 1,000,000. read length=150bp Question Does the quality score values vary throughout the read length? Hint Look at the \u2019per base sequence quality plot\u2019 Answer Yes. Quality scores are dropping towards the end of the reads. Question What is the quality score range you see? Answer 2-40 Question At around which position do the scores start falling below Q20 for the 25% quartile range (25%of reads below Q20)? Answer Around 30 bp position Question How can we trim the reads to filter out the low quality data? Answer By trimming off the bases after a fixed position of the read or by trimming off bases based on the quality score.","title":"Quality Visualisation"},{"location":"qc/handout/handout/#good-quality-data","text":"View the FastQC report files fastqc_report.html to see examples of a good quality data and compare the quality plot with that of the bad_example_fastqc . firefox qcdemo_R1_fastqc.html &amp; Sequencing errors can complicate the downstream analysis, which normally requires that reads be aligned to each other (for genome assembly) or to a reference genome (for detection of mutations). Sequence reads containing errors may lead to ambiguous paths in the assembly or improper gaps. In variant analysis projects sequence reads are aligned against the reference genome. The errors in the reads may lead to more mismatches than expected from mutations alone. But if these errors can be removed or corrected, the read alignments and hence the variant detection will improve. The assemblies will also improve after pre-processing the reads to remove errors.","title":"Good Quality Data"},{"location":"qc/handout/handout/#read-trimming","text":"Read trimming can be done in a variety of different ways. Choose a method which best suits your data. Here we are giving examples of fixed-length trimming and quality-based trimming.","title":"Read Trimming"},{"location":"qc/handout/handout/#quality-based-trimming","text":"Base call quality scores can be used to dynamically determine the trim points for each read. A quality score threshold and minimum read length following trimming can be used to remove low quality data. The previous FastQC results show R1 is fine but R2 has low quality at the end. There is no adaptor contamination though. We will be using Skewer to perform the quality trimming. Run the following command to quality trim a set of paired end data. cd /home/trainee/qc skewer -t 4 -l 50 -q 30 -Q 25 -m pe -o qcdemo qcdemo_R1.fastq.gz qcdemo_R2.fastq.gz -t: number of threads to use -l: min length to keep after trimming -q: Quality threshold used for trimming at 3\u2019 end -Q: mean quality threshold for a read -m: pair-end mode Run FastQC on the quality trimmed file and visualise the quality scores. Look at the last files generated, are the file names same as the input ? ls -ltr Run Fastqc on the quality trimmed files: fastqc -f fastq qcdemo-trimmed-pair1.fastq fastqc -f fastq qcdemo-trimmed-pair2.fastq Visualise the fastqc results: firefox qcdemo-trimmed-pair1_fastqc.html &amp; firefox qcdemo-trimmed-pair2_fastqc.html &amp; Let\u2019s look at the quality from the second reads. The output should look like: Property Value Filename qcdemo-trimmed-pair2.fastq File type Conventional base calls Encoding Sanger / Illumina 1.9 Total Sequences 742262 Filtered Sequences 0 Sequence length 50-150 %GC 37 Table 4: Summary Statistics of QC_demo_R1_trimmed Figure 2: bad_example_quality_trimmed_plot Question Did the number of total reads in R1 and R2 change after trimming? Answer Quality trimming discarded >25000 reads. However, we retain a lot of maximal length reads which have good quality all the way to the ends. Question What reads lengths were obtained after quality based trimming? Answer 50-150 Reads <50 bp, following quality trimming, were discarded. Question Did you observe adapter sequences in the data? Answer No. (Hint: look at the overrepresented sequences) Question How can you use -a option with fastqc? (Hint: try fastqc -h). Answer Adaptors can be supplied in a file for screening.","title":"Quality Based Trimming"},{"location":"qc/handout/handout/#adapter-clipping","text":"Sometimes sequence reads may end up getting the leftover of adapters and primers used in the sequencing process. It\u2019s good practice to screen your data for these possible contamination for more sensitive alignment and assembly based analysis. This is particularly important when read lengths can be longer than the molecules being sequenced. For example when sequencing miRNAs. Various QC tools are available to screen and/or clip these adapter/primer sequences from your data. Apart from skewer which will be using today the following two tools are also useful for trimming and removing adapter sequence: Cutadapt: http://code.google.com/p/cutadapt/ Trimmomatic: http://www.usadellab.org/cms/?page=trimmomatic Here we are demonstrating Skewer to trim a given adapter sequence. cd /home/trainee/qc fastqc -f fastq adaptorQC.fastq.gz firefox adaptorQC_fastqc.html skewer -x TGGAATTCTCGGGTGCCAAGGT -t 20 -l 10 -L 35 -q 30 adaptorQC.fastq.gz -x: adaptor sequence used -t: number of threads to use -l: min length to keep after trimming -L: Max length to keep after trimming, in this experiment we were expecting only small RNA fragments -Q: Quality threshold used for trimming at 3\u2019 end. Use -m option to control the end you want to trim Run FastQC on the adapter trimmed file and visualise the quality scores. Fastqc now shows adaptor free results. fastqc adaptorQC.fastq-trimmed.fastq firefox adaptorQC.fastq-trimmed_fastqc.html &amp;","title":"Adapter Clipping"},{"location":"qc/handout/handout/#fixed-length-trimming","text":"We will not cover fixed length trimming but provide the following for your information. Low quality read ends can be trimmed using a fixed-length trimming. We will use the fastx_trimmer from the FASTX-Toolkit. Usage message to find out various options you can use with this tool. Type fastx_trimmer -h at anytime to display help. We will now do fixed-length trimming of the bad_example.fastq file using the following command. You should still be in the qc directory, if not cd back in. cd /home/trainee/qc fastqc -f fastq bad_example.fastq fastx_trimmer -h fastx_trimmer -Q 33 -f 1 -l 80 -i bad_example.fastq -o bad_example_trimmed01.fastq We used the following options in the command above: -Q 33: Indicates the input quality scores are Phred+33 encoded -f: First base to be retained in the output -l: Last base to be retained in the output -i: Input FASTQ file name -o: Output file name Run FastQC on the trimmed file and visualise the quality scores of the trimmed file. fastqc -f fastq bad_example_trimmed01.fastq firefox bad_example_trimmed01_fastqc.html &amp; The output should look like: Property Value Filename bad_example_trimmed01.fastq File type Conventional base calls Encoding Sanger / Illumina 1.9 Total Sequences 40000 Filtered Sequences 0 Sequence length 80 %GC 48 Table 5: Summary Statistics of bad_example_trimmed summary Figure 3: bad_example_trimmed_plot Question What values would you use for -f if you wanted to trim off 10 bases at the 5\u2019 end of the reads? Answer -f 11","title":"Fixed Length Trimming"},{"location":"qc/handout/license/","text":"This work is licensed under a Creative Commons Attribution 3.0 Unported License and the below text is a summary of the main terms of the full Legal Code (the full license) available at http://creativecommons.org/licenses/by/3.0/legalcode . You are free: to copy, distribute, display, and perform the work to make derivative works to make commercial use of the work Under the following conditions: Attribution - You must give the original author credit. With the understanding that: Waiver - Any of the above conditions can be waived if you get permission from the copyright holder. Public Domain - Where the work or any of its elements is in the public domain under applicable law, that status is in no way affected by the license. Other Rights - In no way are any of the following rights affected by the license: Your fair dealing or fair use rights, or other applicable copyright exceptions and limitations; The author\u2019s moral rights; Rights other persons may have either in the work itself or in how the work is used, such as publicity or privacy rights. Notice - For any reuse or distribution, you must make clear to others the license terms of this work.","title":"License"},{"location":"rna-seq/","text":"This repository is a template for developing your own Bioinformatics Training Platform (BTP) workshop module. Therefore, if you haven\u2019t already, you should head over to the btp-workshop-template repository and follow one of the workflows documented there. Table of Contents Overview General Design/Layout Overview The README.md files found in each subdirectory of this repository provide documentation on the structure, function, and usage of the btp-module-template in the context of the BTP framework. General Design/Layout These types of workshop modules are designed to be self contained. That is they contain all the required information to permit trainers to reuse and repurpose them quickly and easily. They contain the following 4 major elements: Handout - This often includes background information, step-by-step exercises, questions and answers as well as bonus exercises for those who progress rapidly. Data sets - This describes where to obtain the data sets, used in the handout exercises, and where on the computer system they should be located. Tools - This describes which tools are used in the handout exercises and how to install them. Presentations - This is where you will find presentations for introducing concepts that are explored in the handout exercises. ======= btp-module-rna-seq Bioinformatics Training Platform (BTP) Module: RNA-Seq Topic RNA-Seq Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes Understand and perform a simple RNA-Seq analysis workflow. Perform spliced alignments to an indexed reference genome using TopHat. Perform transcript assembly using Cufflinks. Visualize transcript alignments and annotation in a genome browser such as IGV. Be able to identify differential gene expression between two experimental conditions. Be familiar with R environment and be able to run R based RNA-seq packages. Time Required 6 hrs License The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"Home"},{"location":"rna-seq/#table-of-contents","text":"Overview General Design/Layout","title":"Table of Contents"},{"location":"rna-seq/#overview","text":"The README.md files found in each subdirectory of this repository provide documentation on the structure, function, and usage of the btp-module-template in the context of the BTP framework.","title":"Overview"},{"location":"rna-seq/#general-designlayout","text":"These types of workshop modules are designed to be self contained. That is they contain all the required information to permit trainers to reuse and repurpose them quickly and easily. They contain the following 4 major elements: Handout - This often includes background information, step-by-step exercises, questions and answers as well as bonus exercises for those who progress rapidly. Data sets - This describes where to obtain the data sets, used in the handout exercises, and where on the computer system they should be located. Tools - This describes which tools are used in the handout exercises and how to install them. Presentations - This is where you will find presentations for introducing concepts that are explored in the handout exercises. =======","title":"General Design/Layout"},{"location":"rna-seq/#btp-module-rna-seq","text":"Bioinformatics Training Platform (BTP) Module: RNA-Seq Topic RNA-Seq Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes Understand and perform a simple RNA-Seq analysis workflow. Perform spliced alignments to an indexed reference genome using TopHat. Perform transcript assembly using Cufflinks. Visualize transcript alignments and annotation in a genome browser such as IGV. Be able to identify differential gene expression between two experimental conditions. Be familiar with R environment and be able to run R based RNA-seq packages. Time Required 6 hrs","title":"btp-module-rna-seq"},{"location":"rna-seq/#license","text":"The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"License"},{"location":"rna-seq/datasets/","text":"Information about what datasets are required for a module\u2019s tutorial exercises are described in a data.yaml file. As well describing where to source the dataset files from, we also need to describe where on the filesystem the files should reside and who should own them.","title":"Home"},{"location":"rna-seq/handout/handout/","text":"Key Learning Outcomes After completing this practical the trainee should be able to: Understand and perform a simple RNA-Seq analysis workflow. Perform spliced alignments to an indexed reference genome using TopHat. Visualize spliced transcript alignments in a genome browser such as IGV. Be able to identify differential gene expression between two experimental conditions. We also have bonus exercises where you can learn to: Perform transcript assembly using Stringtie. analysis. Visualize transcript alignments and annotation in a genome browser such as IGV. Author Information Primary Author(s): Sonika Tyagi, Monash University and Susan Corley UNSW Contributor)s) Myrto Kostadima EBI UK Resources You\u2019ll be Using Tools Used Tophat : https://ccb.jhu.edu/software/tophat/index.shtml Stringtie : http://https://ccb.jhu.edu/software/stringtie/ Samtools : http://www.htslib.org/ BEDTools : https://github.com/arq5x/bedtools2 UCSC tools : http://hgdownload.cse.ucsc.edu/admin/exe/ IGV : http://www.broadinstitute.org/igv/ FeatureCount : http://subread.sourceforge.net/ edgeR pakcage : https://bioconductor.org/packages/release/bioc/html/edgeR.html Sources of Data http://www.ebi.ac.uk/ena/data/view/ERR022484 \\ http://www.ebi.ac.uk/ena/data/view/ERR022485 \\ http://www.pnas.org/content/suppl/2008/12/16/0807121105.DCSupplemental Introduction The goal of this hands-on session is to perform some basic tasks in the downstream analysis of RNA-seq data.\\ First we will use RNA-seq data from zebrafish. You will align one set of reads to the zebrafish using Tophat2. You will then view the aligned reads using the IGV viewer. We will also demonstrate how gene counts can be derived from this data. You will go on to assembly a transcriptome from the read data using cufflinks. We will show you how this type of data may be analysed for differential expression. The second part of the tutorial will focus on RNA-seq data from a human experiment (cancer cell line versus normal cells). You will use the Bioconductor packages edgeR and voom (limma) to determine differential gene expression. The results from this analysis will then be used in the final session which introduces you to some of the tools used to gain biological insight into the results of a differential expression analysis Prepare the Environment We will use a dataset derived from sequencing of mRNA from Danio rerio embryos in two different developmental stages. Sequencing was performed on the Illumina platform and generated 76bp paired-end sequence data using polyA selected RNA. Due to the time constraints of the practical we will only use a subset of the reads. The data files are contained in the subdirectory called data and are the following: 2cells_1.fastq and 2cells_2.fastq : \\ These files are based on RNA-seq data of a 2-cell zebrafish embryo 6h_1.fastq and 6h_2.fastq : \\ These files are based on RNA-seq data of zebrafish embryos 6h post fertilization Open the Terminal and go to the rnaseq working directory: cd /home/trainee/rnaseq/ All commands entered into the terminal for this tutorial should be from within the /home/trainee/rnaseq directory. Check that the data directory contains the above-mentioned files by typing: shellls data Alignment There are numerous tools for performing short read alignment and the choice of aligner should be carefully made according to the analysis goals/requirements. Here we will use Tophat2, a widely used ultrafast aligner that performs spliced alignments. Tophat2 is based on the Bowtie2 aligner and uses an indexed genome for the alignment to speed up the alignment and keep its memory footprint small. The the index for the Danio rerio genome has been created for you. The command to create an index is as follows. You DO NOT need to run this command yourself - we have done this for you. - bowtie2-build genome/Danio_rerio.Zv9.66.dna.fa genome/ZV9 Tophat2 has a number of parameters in order to perform the alignment. To view them all type: tophat2 --help The general format of the tophat2 command is: tophat2 [ options ] * <index_base> <reads_1> <reads_2> Where the last two arguments are the .fastq files of the paired end reads, and the argument before is the basename of the indexed genome. The quality values in the FASTQ files used in this hands-on session are Phred+33 encoded. We explicitly tell tophat of this fact by using the command line argument \u2013solexa-quals . You can look at the first few reads in the file data/2cells_1.fastq with: head -n 20 data/2cells_1.fastq Some other parameters that we are going to use to run Tophat are listed below: -g : Maximum number of multihits allowed. Short reads are likely to map to more than one location in the genome even though these reads can have originated from only one of these regions. In RNA-seq we allow for a limited number of multihits, and in this case we ask Tophat to report only reads that map at most onto 2 different loci. \u2013library-type : Before performing any type of RNA-seq analysis you need to know a few things about the library preparation. Was it done using a strand-specific protocol or not? If yes, which strand? In our data the protocol was NOT strand specific. -j : Improve spliced alignment by providing Tophat with annotated splice junctions. Pre-existing genome annotation is an advantage when analysing RNA-seq data. This file contains the coordinates of annotated splice junctions from Ensembl. These are stored under the sub-directory annotation in a file called ZV9.spliceSites . -o : This specifies in which subdirectory Tophat should save the output files. Given that for every run the name of the output files is the same, we specify different directories for each run. It takes some time (approx. 20 min) to perform tophat spliced alignments, even for this subset of reads. Therefore, we have pre-aligned the 2cells data for you using the following command: You DO NOT need to run this command yourself - we have done this for you. tophat2 --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_2cells genome/ZV9 data/2cells_1.fastq data/2cells_2.fastq Align the 6h data yourself using the following command: # Takes approx. 20mins tophat2 --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_6h genome/ZV9 data/6h_1.fastq data/6h_2.fastq The 6h read alignment will take approx. 20 min to complete. Therefore, we\u2019ll take a look at some of the files, generated by tophat, for the pre-computed 2cells data. Tophat generates several files in the specified output directory. The most important files are listed below. accepted_hits.bam : This file contains the list of read alignments in BAM format. align_summary.txt : Provides a detailed summary of the read-alignments. unmapped.bam : This file contains the unmapped reads. The complete documentation can be found at: https://ccb.jhu.edu/software/tophat/manual.shtml Alignment Visualisation in IGV The Integrative Genomics Viewer (IGV) is able to provide a visualisation of read alignments given a reference sequence and a BAM file. We\u2019ll visualise the information contained in the accepted_hits.bam and junctions.bed files for the pre-computed 2cells data. The former, contains the tophat spliced alignments of the reads to the reference while the latter stores the coordinates of the splice junctions present in the data set. Open the rnaseq directory on your Desktop and double-click the tophat subdirectory and then the ZV9_2cells directory. Launch IGV by double-clicking the \u201cIGV 2.3.*\u201d icon on the Desktop (ignore any warnings that you may get as it opens). NOTE: IGV may take several minutes to load for the first time, please be patient. Choose \u201cZebrafish (Zv9)\u201d from the drop-down box in the top left of the IGV window. Else you can also load the genome fasta file. Load the accepted_hits.sorted.bam file by clicking the \u201cFile\u201d menu, selecting \u201cLoad from File\u201d and navigating to the Desktop/rnaseq/tophat/ZV9_2cells directory. Rename the track by right-clicking on its name and choosing \u201cRename Track\u201d. Give it a meaningful name like \u201c2cells BAM\u201d. Load the junctions.bed from the same directory and rename the track \u201c2cells Junctions BED\u201d. Load the Ensembl annotations file Danio_rerio.Zv9.66.gtf stored in the rnaseq/annotation directory. Navigate to a region on chromosome 12 by typing chr12 : 20 , 270 , 921 - 20 , 300 , 943 into the search box at the top of the IGV window. Keep zooming to view the bam file alignments Some useful IGV manuals can be found below http://www.broadinstitute.org/software/igv/interpreting_insert_size \\ http://www.broadinstitute.org/software/igv/alignmentdata Does the file \u2019align_summary.txt\u2019 look interesting? What information does it provide? As the name suggests, the file provides a details summary of the alignment statistics. One other important file is \u2019unmapped.bam\u2019. This file contains the unampped reads. Can you identify the splice junctions from the BAM file? Splice junctions can be identified in the alignment BAM files. These are the aligned RNA-Seq reads that have skipped-bases from the reference genome (most likely introns). Are the junctions annotated for CBY1 consistent with the annotation? Read alignment supports an extended length in exon 5 to the gene model (cby1-001) Once tophat finishes aligning the 6h data you will need to sort the alignments found in the BAM file and then index the sorted BAM file. samtools sort tophat/ZV9_6h/accepted_hits.bam tophat/ZV9_6h/accepted_hits.sorted samtools index tophat/ZV9_6h/accepted_hits.sorted.bam Load the sorted BAM file into IGV, as described previously, and rename the track appropriately. Generating Gene Counts In RNAseq experiments the digital gene expression is recorded as the gene counts or number of reads aligning to a known gene feature. If you have a well annotated genome, you can use the gene structure file in a standard gene annotation format (GTF or GFF)) along with the spliced alignment file to quantify the known genes. We will demonstrate a utility called FeatureCounts that comes with the Subread package. mkdir gene_counts featureCounts -a annotation/Danio_rerio.Zv9.66.gtf -t exon -g gene_id -o gene_counts/gene_counts.txt tophat/ZV9_6h/accepted_hits.sorted.bam tophat/ZV9_2cells/accepted_hits.sorted.bam Isoform Expression and Transcriptome Assembly For non-model organisms and genomes with draft assemblies and incomplete annotations, it is a common practice to take and assembly based approach to generate gene structures followed by the quantification step. There are a number of reference based transcript assemblers available that can be used for this purpose such as, cufflinks, stringtie. These assemblers can give gene or isoform level assemblies that can be used to perform a gene/isoform level quantification. These assemblers require an alignment of reads with a reference genome or transcriptome as an input. The second optional input is a known gene structure in GTF or GFF format. These gene annotation files contain information of known genes and may be helpful in reconstruction of transcripts that are of low abundance on your sample. StringTie will also assemble and construct a transcriptome containing any unannotated genes. Furthermore, its output data is able to directly be utilised in differential expression resources such as Cuffdiff, Ballgown or other packers in R (edgeR, DESeq2). To run string tie, the basic command is: Stringtie has a number of parameters in order to perform transcriptome assembly and quantification. To view them all type: stringtie --help We aim to reconstruct the transcriptome for both samples by using the Ensembl annotation as a guide. The Ensembl annotation for Danio rerio is available in annotation/Danio_rerio.Zv9.66.gtf . The general format of the stringtie command is: stringtie &lt;aligned_reads_file.bam&gt; [options] Where the input is the aligned reads (either in SAM or BAM format). Perform transcriptome assembly, strictly using the supplied GTF annotations, for the 2cells and 6h data using cufflinks: For instance, to run StringTie on 2cells BAM file: stringtie -p 8 -G annotation/Danio_rerio.Zv9.66.gtf -o stringtie/ZV9_2cells.gft tophat/ZV9_2cells/accepted_hits.sorted.bam Repeat this command on the 6h BAM file: stringtie -p 8 -G annotation/Danio_rerio.Zv9.66.gtf -o stringtie/ZV9_6h.gft tophat/ZV9_6h/accepted_hits.sorted.bam : -p = number of processing threads : -G = reference annotation file : -o = output file location and name. Most RNA sequencing experiments will have multiple samples. As such, Stringtie has a function to merge all transcripts assembled across the samples, which enables inter-sample comparisons to be made. It does this by first assembling all transcripts within each sample, merging the the transcripts and re-running the assembly based off a merged list of genes. Prior to running a merge function within stringtie: We need to create a mergelist.txt file before utilising the merge function. This is a list of all .gtf files created by stringtie in the above step and can be made in a text editor such as vi or nano. For this exercise, there is a file pre-made for you. The txt file will direct the merge function to look for the names of all the .gtf files that we want to merge. To merge StringTie files ls stringtie/*.gtf &gt;mergelist.txt less mergelist.txt stringtie --merge -p 8 -G annotation/Danio_rerio.Zv9.66.gtf -o stringtie/stringtie_merged.gtf mergelist.txt (mergelist.txt is a text file that will have a list of all .gtf generated by stringtie in the above steps) Note how a reference gene annotation (Danio_rerio.Zv9.66.gtf) is utilised in this case. This makes use of information we already have in order to assemble the transcriptome of our RNA seq data. The next step is an optional step to see how your merged annotation compares to reference gene annotation provided. gffcompare is a tool that determines how many of your annotations match the reference (either fully or partial) and how many were completely novel. The output of this ulitily gives you: Sensitivity is defined as the proportion of genes from the annotation that are correctly reconstructed Precision (also known as positive predictive value) captures the proportion of the output that overlaps the annotation To compare how merged annotation compares to reference gffcompare -G -r annotation/Danio_rerio.Zv9.66.gtf stringtie/stringtie_merged.gtf : -r = reference genome in form of gtf : -G = tells gffcompare to compare all transcripts in the input file (stringtie/stringtie_merged.gtf), even those that might be redundant : -o = all output of gffcompare has a gffcmp. prefix, unless otherwise defined by user Now that we are happy with our merged annotation, we need to estimate the abundance of each transcript within each sample. Stringtie is designed to create output files that are immediately usable in next step of our RNA sequencing pipeline. In this next example, we can direct stringtie to simultaneously create a .ctab file required for ballgown. To estimate abundances and create table counts for 2cell stringtie -e -B -p 8 -G stringtie/stringtie_merged.gtf -o ballgown/ZV9_6h.gtf tophat/ZV9_6h/accepted_hits.sorted.bam To estimate abundances and create table counts for 2cell stringtie -e -B -p 8 -G stringtie/stringtie_merged.gtf -o ballgown/ZV9_2cells.gtf tophat/ZV9_2cells/accepted_hits.sorted.bam : -e = limits the processing of read alignments to only estimate and output the assembled transcripts matching the reference transcripts (given by -G). Reads with no reference transcripts will be entirely skipped : -B = enables output of Ballgown input table files (*.ctab) containing coverage data for the reference transcripts given with the -G option. If -o is used, StringTie will write the *.ctab files in the same directory as the output GTF. : -p = refers to number of processing threads : -G = refers to a reference annotation (in GTF or GFF format) Visulaizing transcript assembly Go back to IGV and load the pre-computed, GTF-guided transcriptome assembly for the 2cells data ( stringtie/ZV9_2cells.gtf ). Rename the track as \u201c2cells GTF-Guided Transcripts\u201d. In the search box type ENSDART00000082297 in order for the browser to zoom in to the gene of interest. Do you observe any difference between the Ensembl GTF annotations and the GTF-guided transcripts assembled by cufflinks (the \u201c2cells GTF-Guided Transcripts\u201d track)? Yes. It appears that the Ensembl annotations may have truncated the last exon. However, our data also doesn\u2019t contain reads that span between the last two exons. Differential Gene Expression Analysis using edgeR Experiment Design The example we are working through today follows a case Study set out in the edgeR Users Guide (4.3 Androgen-treated prostate cancer cells (RNA-Seq, two groups) which is based on an experiment conducted by Li et al. (2008, Proc Natl Acad Sci USA, 105, 20179-84). The researches used a prostate cancer cell line (LNCaP cells). These cells are sensitive to stimulation by male hormones (androgens). Three replicate RNA samples were collected from LNCaP cells treated with an androgen hormone (DHT). Four replicates were collected from cells treated with an inactive compound. Each of the seven samples was run on a lane (7 lanes) of an Illumina flow cell to produce 35 bp reads. The experimental design was therefore: [H] rrr Lane & Treatment & Label \\ 1 & Control & Con1\\ 2 & Control & Con2\\ 3 & Control & Con3\\ 4 & Control & Con4\\ 5 & DHT & DHT1\\ 6 & DHT & DHT2\\ 7 & DHT & DHT3\\ [tab:experimental d esign] This workflow requires raw gene count files and these can be generated using a utility called featureCounts as demonstrated above. We are using a pre-computed gene counts data (stored in pnas_expression.txt ) for this exercise. Prepare the Environment Prepare the environment and load R: cd /home/trainee/rnaseq/edgeR R (press enter) Once on the R prompt. Load libraries: library(edgeR) library(biomaRt) library(gplots) library(\"limma\") library(\"RColorBrewer\") library(\"org.Hs.eg.db\") Read in Data Read in count table and experimental design: data <- read.delim ( \"pnas_expression.txt\" , row.names = 1 , header = T ) targets <- read.delim ( \"Targets.txt\" , header = T ) colnames ( data ) <- targets $ Label head ( data , n = 20 ) Add Gene annotation The data set only includes the Ensembl gene id and the counts. It is useful to have other annotations such as the gene symbol and entrez id. Next we will add in these annotations. We will use the BiomaRt package to do this. We start by using the useMart function of BiomaRt to access the human data base of ensemble gene ids. human&lt;-useMart(host=\"www.ensembl.org\", \"ENSEMBL_MART_ENSEMBL\", dataset=\"hsapiens_gene_ensembl\") attributes=c(\"ensembl_gene_id\", \"entrezgene\",\"hgnc_symbol\") We create a vector of our ensemble gene ids. ensembl_names&lt;-rownames(data) head(ensembl_names) We then use the function getBM to get the gene symbol data we want.This takes about a minute. genemap&lt;-getBM(attributes, filters=\"ensembl_gene_id\", values=ensembl_names, mart=human) Have a look at the start of the genemap dataframe. head(genemap) We then match the data we have retrieved to our dataset. idx &lt;-match(ensembl_names, genemap$ensembl_gene_id) data$entrezgene &lt;-genemap$entrezgene [ idx ] data$hgnc_symbol &lt;-genemap$hgnc_symbol [ idx ] Ann &lt;- cbind(rownames(data), data$hgnc_symbol, data$entrezgene) colnames(Ann)&lt;-c(\"Ensembl\", \"Symbol\", \"Entrez\") Ann&lt;-as.data.frame(Ann) Let\u2019s check and see that this additional information is there. head(data) Data checks Create DGEList object: treatment &lt;-factor(c(rep(\"Control\",4), rep(\"DHT\",3)), levels=c(\"Control\", \"DHT\")) y &lt;-DGEList(counts=data[,1:7], group=treatment, genes=Ann) Check the dimensions of the object: dim(y) We see we have 37435 rows (i.e. genes) and 7 columns (samples). Now we will filter out genes with low counts by only keeping those rows where the count per million (cpm) is at least 1 in at least three samples: keep <- rowSums ( cpm ( y ) > 1 ) >= 3 y <- y [ keep , ] How many rows (genes) are retained now dim(y) would give you 16494 How many genes were filtered out? do 37435-16494. As we have removed the lowly expressed genes the total number of counts per sample has not changed greatly. Let us check the total number of reads per sample in the original data (data) and now after filtering. Before: colSums(data[,1:7]) After filtering: colSums(y$counts) We will now perform normalization to take account of different library size: y <- calcNormFactors ( y ) We will check the calculated normalization factors: y$samples Lets have a look at whether the samples cluster by condition. (You should produce a plot as shown in Figure 4): plotMDS(y, col=as.numeric(y$samples$group)) [H] [fig:MDS plot] Does the MDS plot indicate a difference in gene expression between the Controls and the DHT treated samples? The MDS plot shows us that the controls are separated from the DHT treated cells. This indicates that there is a difference in gene expression between the conditions. We will now estimate the dispersion. We start by estimating the common dispersion. The common dispersion estimates the overall Biological Coefficient of Variation (BCV) of the dataset averaged over all genes. By using verbose we get the Disp and BCV values printed on the screen y &lt;- estimateCommonDisp(y, verbose=T) What value to you see for BCV? We now estimate gene-specific dispersion. y &lt;- estimateTagwiseDisp(y) We will plot the tagwise dispersion and the common dispersion (You should obtain a plot as shown in the Figure 5): plotBCV(y) [H] [fig:BCV plot] We see here that the common dispersion estimates the overall Biological Coefficient of Variation (BCV) of the dataset averaged over all genes. The common dispersion is 0.02 and the BCV is the square root of the common dispersion (sqrt[0.02] = 0.14). A BCV of 14% is typical for cell line experiment. As you can see from the plot the BCV of some genes (generally those with low expression) can be much higher than the common dispersion. For example we see genes with a reasonable level of expression with tagwise dispersion of 0.4 indicating 40% variation between samples. If we used the common dispersion for these genes instead of the tagwise dispersion what effect would this have? If we simply used the common dispersion for these genes we would underestimate biological variability, which in turn affects whether these genes would be identified as being differentially expressed between conditions.It is recommended to use the tagwise dispersion, which takes account of gene-to-gene variability. Now that we have normalized our data and also calculated the variability of gene expression between samples we are in a position to perform differential expression testing.As this is a simple comparison between two conditions, androgen treatment and placebo treatment we can use the exact test for the negative binomial distribution (Robinson and Smyth, 2008). Testing for Differential Expression We now test for differentially expressed BCV genes: et <- exactTest ( y ) Now we will use the topTags function to adjust for multiple testing. We will use the Benjimini Hochberg (\u201cBH\u201d) method and we will produce a table of results: res <- topTags ( et , n = nrow ( y $ counts ), adjust.method = \"BH\" ) $ table Let\u2019s have a look at the first rows of the table: head(res) To get a summary of the number of differentially expressed genes we can use the decideTestsDGE function. summary ( de <- decideTestsDGE ( et )) This tells us that 2096 genes are downregulated and 2339 genes are upregulated at 5% FDR.We will now make subsets of the most significant upregulated and downregulated genes. alpha=0.05 lfc=1.5 edgeR_res_sig&lt;-res[res$FDR&lt;alpha,] edgeR_res_sig_lfc &lt;-edgeR_res_sig[abs(edgeR_res_sig$logFC) &gt;= lfc,]head(edgeR_res_sig, n=20)nrow(edgeR_res_sig)nrow(edgeR_res_sig_lfc) We can write out these results to our current directory. write.table(edgeR_res_sig , \"edgeR_res_sig.txt\", sep=\"\\t\", col.names=NA, quote=F) write.table(edgeR_res_sig_lfc , \"edgeR_res_sig_lfc.txt\", sep=\"\\t\", col.names=NA, quote=F) How many differentially expressed genes are there? 4435 How many upregulated genes and downregulated genes do we have? 2339 2096 q() You can save your workspace by typing Y on prompt. Please note that the output files you are creating are saved in your present working directory. If you are not sure where you are in the file system try typing pwd on your command prompt to find out. References Trapnell, C., Pachter, L. & Salzberg, S. L. TopHat: discovering splice junctions with RNA-Seq. Bioinformatics 25, 1105-1111 (2009). Trapnell, C. et al. Transcript assembly and quantification by RNA-Seq reveals unannotated transcripts and isoform switching during cell differentiation. Nat. Biotechnol. 28, 511-515 (2010). Langmead, B., Trapnell, C., Pop, M. & Salzberg, S. L. Ultrafast and memory-efficient alignment of short DNA sequences to the human genome. Genome Biol. 10, R25 (2009). Roberts, A., Pimentel, H., Trapnell, C. & Pachter, L. Identification of novel transcripts in annotated genomes using RNA-Seq. Bioinformatics 27, 2325-2329 (2011). Roberts, A., Trapnell, C., Donaghey, J., Rinn, J. L. & Pachter, L. Improving RNA-Seq expression estimates by correcting for fragment bias. Genome Biol. 12, R22 (2011). Robinson MD, McCarthy DJ and Smyth GK. edgeR: a Bioconductor package for differential expression analysis of digital gene expression data. Bioinformatics, 26 (2010). Robinson MD and Smyth GK Moderated statistical tests for assessing differences in tag abundance. Bioinformatics, 23, pp. -6. Robinson MD and Smyth GK (2008). Small-sample estimation of negative binomial dispersion, with applications to SAGE data.\u201d Biostatistics, 9. McCarthy, J. D, Chen, Yunshun, Smyth and K. G (2012). Differential expression analysis of multifactor RNA-Seq experiments with respect to biological variation. Nucleic Acids Research, 40(10), pp. -9.","title":"RNA-Seq"},{"location":"rna-seq/handout/handout/#key-learning-outcomes","text":"After completing this practical the trainee should be able to: Understand and perform a simple RNA-Seq analysis workflow. Perform spliced alignments to an indexed reference genome using TopHat. Visualize spliced transcript alignments in a genome browser such as IGV. Be able to identify differential gene expression between two experimental conditions. We also have bonus exercises where you can learn to: Perform transcript assembly using Stringtie. analysis. Visualize transcript alignments and annotation in a genome browser such as IGV.","title":"Key Learning Outcomes"},{"location":"rna-seq/handout/handout/#author-information","text":"Primary Author(s): Sonika Tyagi, Monash University and Susan Corley UNSW Contributor)s) Myrto Kostadima EBI UK","title":"Author Information"},{"location":"rna-seq/handout/handout/#resources-youll-be-using","text":"","title":"Resources You\u2019ll be Using"},{"location":"rna-seq/handout/handout/#tools-used","text":"Tophat : https://ccb.jhu.edu/software/tophat/index.shtml Stringtie : http://https://ccb.jhu.edu/software/stringtie/ Samtools : http://www.htslib.org/ BEDTools : https://github.com/arq5x/bedtools2 UCSC tools : http://hgdownload.cse.ucsc.edu/admin/exe/ IGV : http://www.broadinstitute.org/igv/ FeatureCount : http://subread.sourceforge.net/ edgeR pakcage : https://bioconductor.org/packages/release/bioc/html/edgeR.html","title":"Tools Used"},{"location":"rna-seq/handout/handout/#sources-of-data","text":"http://www.ebi.ac.uk/ena/data/view/ERR022484 \\ http://www.ebi.ac.uk/ena/data/view/ERR022485 \\ http://www.pnas.org/content/suppl/2008/12/16/0807121105.DCSupplemental","title":"Sources of Data"},{"location":"rna-seq/handout/handout/#introduction","text":"The goal of this hands-on session is to perform some basic tasks in the downstream analysis of RNA-seq data.\\ First we will use RNA-seq data from zebrafish. You will align one set of reads to the zebrafish using Tophat2. You will then view the aligned reads using the IGV viewer. We will also demonstrate how gene counts can be derived from this data. You will go on to assembly a transcriptome from the read data using cufflinks. We will show you how this type of data may be analysed for differential expression. The second part of the tutorial will focus on RNA-seq data from a human experiment (cancer cell line versus normal cells). You will use the Bioconductor packages edgeR and voom (limma) to determine differential gene expression. The results from this analysis will then be used in the final session which introduces you to some of the tools used to gain biological insight into the results of a differential expression analysis","title":"Introduction"},{"location":"rna-seq/handout/handout/#prepare-the-environment","text":"We will use a dataset derived from sequencing of mRNA from Danio rerio embryos in two different developmental stages. Sequencing was performed on the Illumina platform and generated 76bp paired-end sequence data using polyA selected RNA. Due to the time constraints of the practical we will only use a subset of the reads. The data files are contained in the subdirectory called data and are the following: 2cells_1.fastq and 2cells_2.fastq : \\ These files are based on RNA-seq data of a 2-cell zebrafish embryo 6h_1.fastq and 6h_2.fastq : \\ These files are based on RNA-seq data of zebrafish embryos 6h post fertilization Open the Terminal and go to the rnaseq working directory: cd /home/trainee/rnaseq/ All commands entered into the terminal for this tutorial should be from within the /home/trainee/rnaseq directory. Check that the data directory contains the above-mentioned files by typing: shellls data","title":"Prepare the Environment"},{"location":"rna-seq/handout/handout/#alignment","text":"There are numerous tools for performing short read alignment and the choice of aligner should be carefully made according to the analysis goals/requirements. Here we will use Tophat2, a widely used ultrafast aligner that performs spliced alignments. Tophat2 is based on the Bowtie2 aligner and uses an indexed genome for the alignment to speed up the alignment and keep its memory footprint small. The the index for the Danio rerio genome has been created for you. The command to create an index is as follows. You DO NOT need to run this command yourself - we have done this for you. - bowtie2-build genome/Danio_rerio.Zv9.66.dna.fa genome/ZV9 Tophat2 has a number of parameters in order to perform the alignment. To view them all type: tophat2 --help The general format of the tophat2 command is: tophat2 [ options ] * <index_base> <reads_1> <reads_2> Where the last two arguments are the .fastq files of the paired end reads, and the argument before is the basename of the indexed genome. The quality values in the FASTQ files used in this hands-on session are Phred+33 encoded. We explicitly tell tophat of this fact by using the command line argument \u2013solexa-quals . You can look at the first few reads in the file data/2cells_1.fastq with: head -n 20 data/2cells_1.fastq Some other parameters that we are going to use to run Tophat are listed below: -g : Maximum number of multihits allowed. Short reads are likely to map to more than one location in the genome even though these reads can have originated from only one of these regions. In RNA-seq we allow for a limited number of multihits, and in this case we ask Tophat to report only reads that map at most onto 2 different loci. \u2013library-type : Before performing any type of RNA-seq analysis you need to know a few things about the library preparation. Was it done using a strand-specific protocol or not? If yes, which strand? In our data the protocol was NOT strand specific. -j : Improve spliced alignment by providing Tophat with annotated splice junctions. Pre-existing genome annotation is an advantage when analysing RNA-seq data. This file contains the coordinates of annotated splice junctions from Ensembl. These are stored under the sub-directory annotation in a file called ZV9.spliceSites . -o : This specifies in which subdirectory Tophat should save the output files. Given that for every run the name of the output files is the same, we specify different directories for each run. It takes some time (approx. 20 min) to perform tophat spliced alignments, even for this subset of reads. Therefore, we have pre-aligned the 2cells data for you using the following command: You DO NOT need to run this command yourself - we have done this for you. tophat2 --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_2cells genome/ZV9 data/2cells_1.fastq data/2cells_2.fastq Align the 6h data yourself using the following command: # Takes approx. 20mins tophat2 --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_6h genome/ZV9 data/6h_1.fastq data/6h_2.fastq The 6h read alignment will take approx. 20 min to complete. Therefore, we\u2019ll take a look at some of the files, generated by tophat, for the pre-computed 2cells data. Tophat generates several files in the specified output directory. The most important files are listed below. accepted_hits.bam : This file contains the list of read alignments in BAM format. align_summary.txt : Provides a detailed summary of the read-alignments. unmapped.bam : This file contains the unmapped reads. The complete documentation can be found at: https://ccb.jhu.edu/software/tophat/manual.shtml","title":"Alignment"},{"location":"rna-seq/handout/handout/#alignment-visualisation-in-igv","text":"The Integrative Genomics Viewer (IGV) is able to provide a visualisation of read alignments given a reference sequence and a BAM file. We\u2019ll visualise the information contained in the accepted_hits.bam and junctions.bed files for the pre-computed 2cells data. The former, contains the tophat spliced alignments of the reads to the reference while the latter stores the coordinates of the splice junctions present in the data set. Open the rnaseq directory on your Desktop and double-click the tophat subdirectory and then the ZV9_2cells directory. Launch IGV by double-clicking the \u201cIGV 2.3.*\u201d icon on the Desktop (ignore any warnings that you may get as it opens). NOTE: IGV may take several minutes to load for the first time, please be patient. Choose \u201cZebrafish (Zv9)\u201d from the drop-down box in the top left of the IGV window. Else you can also load the genome fasta file. Load the accepted_hits.sorted.bam file by clicking the \u201cFile\u201d menu, selecting \u201cLoad from File\u201d and navigating to the Desktop/rnaseq/tophat/ZV9_2cells directory. Rename the track by right-clicking on its name and choosing \u201cRename Track\u201d. Give it a meaningful name like \u201c2cells BAM\u201d. Load the junctions.bed from the same directory and rename the track \u201c2cells Junctions BED\u201d. Load the Ensembl annotations file Danio_rerio.Zv9.66.gtf stored in the rnaseq/annotation directory. Navigate to a region on chromosome 12 by typing chr12 : 20 , 270 , 921 - 20 , 300 , 943 into the search box at the top of the IGV window. Keep zooming to view the bam file alignments Some useful IGV manuals can be found below http://www.broadinstitute.org/software/igv/interpreting_insert_size \\ http://www.broadinstitute.org/software/igv/alignmentdata Does the file \u2019align_summary.txt\u2019 look interesting? What information does it provide? As the name suggests, the file provides a details summary of the alignment statistics. One other important file is \u2019unmapped.bam\u2019. This file contains the unampped reads. Can you identify the splice junctions from the BAM file? Splice junctions can be identified in the alignment BAM files. These are the aligned RNA-Seq reads that have skipped-bases from the reference genome (most likely introns). Are the junctions annotated for CBY1 consistent with the annotation? Read alignment supports an extended length in exon 5 to the gene model (cby1-001) Once tophat finishes aligning the 6h data you will need to sort the alignments found in the BAM file and then index the sorted BAM file. samtools sort tophat/ZV9_6h/accepted_hits.bam tophat/ZV9_6h/accepted_hits.sorted samtools index tophat/ZV9_6h/accepted_hits.sorted.bam Load the sorted BAM file into IGV, as described previously, and rename the track appropriately.","title":"Alignment Visualisation in IGV"},{"location":"rna-seq/handout/handout/#generating-gene-counts","text":"In RNAseq experiments the digital gene expression is recorded as the gene counts or number of reads aligning to a known gene feature. If you have a well annotated genome, you can use the gene structure file in a standard gene annotation format (GTF or GFF)) along with the spliced alignment file to quantify the known genes. We will demonstrate a utility called FeatureCounts that comes with the Subread package. mkdir gene_counts featureCounts -a annotation/Danio_rerio.Zv9.66.gtf -t exon -g gene_id -o gene_counts/gene_counts.txt tophat/ZV9_6h/accepted_hits.sorted.bam tophat/ZV9_2cells/accepted_hits.sorted.bam","title":"Generating Gene Counts"},{"location":"rna-seq/handout/handout/#isoform-expression-and-transcriptome-assembly","text":"For non-model organisms and genomes with draft assemblies and incomplete annotations, it is a common practice to take and assembly based approach to generate gene structures followed by the quantification step. There are a number of reference based transcript assemblers available that can be used for this purpose such as, cufflinks, stringtie. These assemblers can give gene or isoform level assemblies that can be used to perform a gene/isoform level quantification. These assemblers require an alignment of reads with a reference genome or transcriptome as an input. The second optional input is a known gene structure in GTF or GFF format. These gene annotation files contain information of known genes and may be helpful in reconstruction of transcripts that are of low abundance on your sample. StringTie will also assemble and construct a transcriptome containing any unannotated genes. Furthermore, its output data is able to directly be utilised in differential expression resources such as Cuffdiff, Ballgown or other packers in R (edgeR, DESeq2). To run string tie, the basic command is: Stringtie has a number of parameters in order to perform transcriptome assembly and quantification. To view them all type: stringtie --help We aim to reconstruct the transcriptome for both samples by using the Ensembl annotation as a guide. The Ensembl annotation for Danio rerio is available in annotation/Danio_rerio.Zv9.66.gtf . The general format of the stringtie command is: stringtie &lt;aligned_reads_file.bam&gt; [options] Where the input is the aligned reads (either in SAM or BAM format). Perform transcriptome assembly, strictly using the supplied GTF annotations, for the 2cells and 6h data using cufflinks: For instance, to run StringTie on 2cells BAM file: stringtie -p 8 -G annotation/Danio_rerio.Zv9.66.gtf -o stringtie/ZV9_2cells.gft tophat/ZV9_2cells/accepted_hits.sorted.bam Repeat this command on the 6h BAM file: stringtie -p 8 -G annotation/Danio_rerio.Zv9.66.gtf -o stringtie/ZV9_6h.gft tophat/ZV9_6h/accepted_hits.sorted.bam : -p = number of processing threads : -G = reference annotation file : -o = output file location and name. Most RNA sequencing experiments will have multiple samples. As such, Stringtie has a function to merge all transcripts assembled across the samples, which enables inter-sample comparisons to be made. It does this by first assembling all transcripts within each sample, merging the the transcripts and re-running the assembly based off a merged list of genes. Prior to running a merge function within stringtie: We need to create a mergelist.txt file before utilising the merge function. This is a list of all .gtf files created by stringtie in the above step and can be made in a text editor such as vi or nano. For this exercise, there is a file pre-made for you. The txt file will direct the merge function to look for the names of all the .gtf files that we want to merge. To merge StringTie files ls stringtie/*.gtf &gt;mergelist.txt less mergelist.txt stringtie --merge -p 8 -G annotation/Danio_rerio.Zv9.66.gtf -o stringtie/stringtie_merged.gtf mergelist.txt (mergelist.txt is a text file that will have a list of all .gtf generated by stringtie in the above steps) Note how a reference gene annotation (Danio_rerio.Zv9.66.gtf) is utilised in this case. This makes use of information we already have in order to assemble the transcriptome of our RNA seq data. The next step is an optional step to see how your merged annotation compares to reference gene annotation provided. gffcompare is a tool that determines how many of your annotations match the reference (either fully or partial) and how many were completely novel. The output of this ulitily gives you: Sensitivity is defined as the proportion of genes from the annotation that are correctly reconstructed Precision (also known as positive predictive value) captures the proportion of the output that overlaps the annotation To compare how merged annotation compares to reference gffcompare -G -r annotation/Danio_rerio.Zv9.66.gtf stringtie/stringtie_merged.gtf : -r = reference genome in form of gtf : -G = tells gffcompare to compare all transcripts in the input file (stringtie/stringtie_merged.gtf), even those that might be redundant : -o = all output of gffcompare has a gffcmp. prefix, unless otherwise defined by user Now that we are happy with our merged annotation, we need to estimate the abundance of each transcript within each sample. Stringtie is designed to create output files that are immediately usable in next step of our RNA sequencing pipeline. In this next example, we can direct stringtie to simultaneously create a .ctab file required for ballgown. To estimate abundances and create table counts for 2cell stringtie -e -B -p 8 -G stringtie/stringtie_merged.gtf -o ballgown/ZV9_6h.gtf tophat/ZV9_6h/accepted_hits.sorted.bam To estimate abundances and create table counts for 2cell stringtie -e -B -p 8 -G stringtie/stringtie_merged.gtf -o ballgown/ZV9_2cells.gtf tophat/ZV9_2cells/accepted_hits.sorted.bam : -e = limits the processing of read alignments to only estimate and output the assembled transcripts matching the reference transcripts (given by -G). Reads with no reference transcripts will be entirely skipped : -B = enables output of Ballgown input table files (*.ctab) containing coverage data for the reference transcripts given with the -G option. If -o is used, StringTie will write the *.ctab files in the same directory as the output GTF. : -p = refers to number of processing threads : -G = refers to a reference annotation (in GTF or GFF format)","title":"Isoform Expression and Transcriptome Assembly"},{"location":"rna-seq/handout/handout/#visulaizing-transcript-assembly","text":"Go back to IGV and load the pre-computed, GTF-guided transcriptome assembly for the 2cells data ( stringtie/ZV9_2cells.gtf ). Rename the track as \u201c2cells GTF-Guided Transcripts\u201d. In the search box type ENSDART00000082297 in order for the browser to zoom in to the gene of interest. Do you observe any difference between the Ensembl GTF annotations and the GTF-guided transcripts assembled by cufflinks (the \u201c2cells GTF-Guided Transcripts\u201d track)? Yes. It appears that the Ensembl annotations may have truncated the last exon. However, our data also doesn\u2019t contain reads that span between the last two exons.","title":"Visulaizing transcript assembly"},{"location":"rna-seq/handout/handout/#differential-gene-expression-analysis-using-edger","text":"","title":"Differential Gene Expression Analysis using edgeR"},{"location":"rna-seq/handout/handout/#experiment-design","text":"The example we are working through today follows a case Study set out in the edgeR Users Guide (4.3 Androgen-treated prostate cancer cells (RNA-Seq, two groups) which is based on an experiment conducted by Li et al. (2008, Proc Natl Acad Sci USA, 105, 20179-84). The researches used a prostate cancer cell line (LNCaP cells). These cells are sensitive to stimulation by male hormones (androgens). Three replicate RNA samples were collected from LNCaP cells treated with an androgen hormone (DHT). Four replicates were collected from cells treated with an inactive compound. Each of the seven samples was run on a lane (7 lanes) of an Illumina flow cell to produce 35 bp reads. The experimental design was therefore: [H] rrr Lane & Treatment & Label \\ 1 & Control & Con1\\ 2 & Control & Con2\\ 3 & Control & Con3\\ 4 & Control & Con4\\ 5 & DHT & DHT1\\ 6 & DHT & DHT2\\ 7 & DHT & DHT3\\ [tab:experimental d esign] This workflow requires raw gene count files and these can be generated using a utility called featureCounts as demonstrated above. We are using a pre-computed gene counts data (stored in pnas_expression.txt ) for this exercise.","title":"Experiment Design"},{"location":"rna-seq/handout/handout/#prepare-the-environment_1","text":"Prepare the environment and load R: cd /home/trainee/rnaseq/edgeR R (press enter) Once on the R prompt. Load libraries: library(edgeR) library(biomaRt) library(gplots) library(\"limma\") library(\"RColorBrewer\") library(\"org.Hs.eg.db\")","title":"Prepare the Environment"},{"location":"rna-seq/handout/handout/#read-in-data","text":"Read in count table and experimental design: data <- read.delim ( \"pnas_expression.txt\" , row.names = 1 , header = T ) targets <- read.delim ( \"Targets.txt\" , header = T ) colnames ( data ) <- targets $ Label head ( data , n = 20 )","title":"Read in Data"},{"location":"rna-seq/handout/handout/#add-gene-annotation","text":"The data set only includes the Ensembl gene id and the counts. It is useful to have other annotations such as the gene symbol and entrez id. Next we will add in these annotations. We will use the BiomaRt package to do this. We start by using the useMart function of BiomaRt to access the human data base of ensemble gene ids. human&lt;-useMart(host=\"www.ensembl.org\", \"ENSEMBL_MART_ENSEMBL\", dataset=\"hsapiens_gene_ensembl\") attributes=c(\"ensembl_gene_id\", \"entrezgene\",\"hgnc_symbol\") We create a vector of our ensemble gene ids. ensembl_names&lt;-rownames(data) head(ensembl_names) We then use the function getBM to get the gene symbol data we want.This takes about a minute. genemap&lt;-getBM(attributes, filters=\"ensembl_gene_id\", values=ensembl_names, mart=human) Have a look at the start of the genemap dataframe. head(genemap) We then match the data we have retrieved to our dataset. idx &lt;-match(ensembl_names, genemap$ensembl_gene_id) data$entrezgene &lt;-genemap$entrezgene [ idx ] data$hgnc_symbol &lt;-genemap$hgnc_symbol [ idx ] Ann &lt;- cbind(rownames(data), data$hgnc_symbol, data$entrezgene) colnames(Ann)&lt;-c(\"Ensembl\", \"Symbol\", \"Entrez\") Ann&lt;-as.data.frame(Ann) Let\u2019s check and see that this additional information is there. head(data)","title":"Add Gene annotation"},{"location":"rna-seq/handout/handout/#data-checks","text":"Create DGEList object: treatment &lt;-factor(c(rep(\"Control\",4), rep(\"DHT\",3)), levels=c(\"Control\", \"DHT\")) y &lt;-DGEList(counts=data[,1:7], group=treatment, genes=Ann) Check the dimensions of the object: dim(y) We see we have 37435 rows (i.e. genes) and 7 columns (samples). Now we will filter out genes with low counts by only keeping those rows where the count per million (cpm) is at least 1 in at least three samples: keep <- rowSums ( cpm ( y ) > 1 ) >= 3 y <- y [ keep , ] How many rows (genes) are retained now dim(y) would give you 16494 How many genes were filtered out? do 37435-16494. As we have removed the lowly expressed genes the total number of counts per sample has not changed greatly. Let us check the total number of reads per sample in the original data (data) and now after filtering. Before: colSums(data[,1:7]) After filtering: colSums(y$counts) We will now perform normalization to take account of different library size: y <- calcNormFactors ( y ) We will check the calculated normalization factors: y$samples Lets have a look at whether the samples cluster by condition. (You should produce a plot as shown in Figure 4): plotMDS(y, col=as.numeric(y$samples$group)) [H] [fig:MDS plot] Does the MDS plot indicate a difference in gene expression between the Controls and the DHT treated samples? The MDS plot shows us that the controls are separated from the DHT treated cells. This indicates that there is a difference in gene expression between the conditions. We will now estimate the dispersion. We start by estimating the common dispersion. The common dispersion estimates the overall Biological Coefficient of Variation (BCV) of the dataset averaged over all genes. By using verbose we get the Disp and BCV values printed on the screen y &lt;- estimateCommonDisp(y, verbose=T) What value to you see for BCV? We now estimate gene-specific dispersion. y &lt;- estimateTagwiseDisp(y) We will plot the tagwise dispersion and the common dispersion (You should obtain a plot as shown in the Figure 5): plotBCV(y) [H] [fig:BCV plot] We see here that the common dispersion estimates the overall Biological Coefficient of Variation (BCV) of the dataset averaged over all genes. The common dispersion is 0.02 and the BCV is the square root of the common dispersion (sqrt[0.02] = 0.14). A BCV of 14% is typical for cell line experiment. As you can see from the plot the BCV of some genes (generally those with low expression) can be much higher than the common dispersion. For example we see genes with a reasonable level of expression with tagwise dispersion of 0.4 indicating 40% variation between samples. If we used the common dispersion for these genes instead of the tagwise dispersion what effect would this have? If we simply used the common dispersion for these genes we would underestimate biological variability, which in turn affects whether these genes would be identified as being differentially expressed between conditions.It is recommended to use the tagwise dispersion, which takes account of gene-to-gene variability. Now that we have normalized our data and also calculated the variability of gene expression between samples we are in a position to perform differential expression testing.As this is a simple comparison between two conditions, androgen treatment and placebo treatment we can use the exact test for the negative binomial distribution (Robinson and Smyth, 2008).","title":"Data checks"},{"location":"rna-seq/handout/handout/#testing-for-differential-expression","text":"We now test for differentially expressed BCV genes: et <- exactTest ( y ) Now we will use the topTags function to adjust for multiple testing. We will use the Benjimini Hochberg (\u201cBH\u201d) method and we will produce a table of results: res <- topTags ( et , n = nrow ( y $ counts ), adjust.method = \"BH\" ) $ table Let\u2019s have a look at the first rows of the table: head(res) To get a summary of the number of differentially expressed genes we can use the decideTestsDGE function. summary ( de <- decideTestsDGE ( et )) This tells us that 2096 genes are downregulated and 2339 genes are upregulated at 5% FDR.We will now make subsets of the most significant upregulated and downregulated genes. alpha=0.05 lfc=1.5 edgeR_res_sig&lt;-res[res$FDR&lt;alpha,] edgeR_res_sig_lfc &lt;-edgeR_res_sig[abs(edgeR_res_sig$logFC) &gt;= lfc,]head(edgeR_res_sig, n=20)nrow(edgeR_res_sig)nrow(edgeR_res_sig_lfc) We can write out these results to our current directory. write.table(edgeR_res_sig , \"edgeR_res_sig.txt\", sep=\"\\t\", col.names=NA, quote=F) write.table(edgeR_res_sig_lfc , \"edgeR_res_sig_lfc.txt\", sep=\"\\t\", col.names=NA, quote=F) How many differentially expressed genes are there? 4435 How many upregulated genes and downregulated genes do we have? 2339 2096 q() You can save your workspace by typing Y on prompt. Please note that the output files you are creating are saved in your present working directory. If you are not sure where you are in the file system try typing pwd on your command prompt to find out.","title":"Testing for Differential Expression"},{"location":"rna-seq/handout/handout/#references","text":"Trapnell, C., Pachter, L. & Salzberg, S. L. TopHat: discovering splice junctions with RNA-Seq. Bioinformatics 25, 1105-1111 (2009). Trapnell, C. et al. Transcript assembly and quantification by RNA-Seq reveals unannotated transcripts and isoform switching during cell differentiation. Nat. Biotechnol. 28, 511-515 (2010). Langmead, B., Trapnell, C., Pop, M. & Salzberg, S. L. Ultrafast and memory-efficient alignment of short DNA sequences to the human genome. Genome Biol. 10, R25 (2009). Roberts, A., Pimentel, H., Trapnell, C. & Pachter, L. Identification of novel transcripts in annotated genomes using RNA-Seq. Bioinformatics 27, 2325-2329 (2011). Roberts, A., Trapnell, C., Donaghey, J., Rinn, J. L. & Pachter, L. Improving RNA-Seq expression estimates by correcting for fragment bias. Genome Biol. 12, R22 (2011). Robinson MD, McCarthy DJ and Smyth GK. edgeR: a Bioconductor package for differential expression analysis of digital gene expression data. Bioinformatics, 26 (2010). Robinson MD and Smyth GK Moderated statistical tests for assessing differences in tag abundance. Bioinformatics, 23, pp. -6. Robinson MD and Smyth GK (2008). Small-sample estimation of negative binomial dispersion, with applications to SAGE data.\u201d Biostatistics, 9. McCarthy, J. D, Chen, Yunshun, Smyth and K. G (2012). Differential expression analysis of multifactor RNA-Seq experiments with respect to biological variation. Nucleic Acids Research, 40(10), pp. -9.","title":"References"},{"location":"rna-seq/presentations/","text":"This directory is intended to store presentation files used to provide context to the hands-on aspect of the module.","title":"Home"},{"location":"rna-seq/tools/","text":"The tools used in the module\u2019s tutorial exercises are captured in a tools.yaml file. This file can then be used by a workshop deployment script to download and install the tools required by the module.","title":"Home"},{"location":"scaffolding/","text":"denovo-module-scaffolding Bioinformatics Training Platform (BTP) Module: Contig Scaffolding Topic Contig Scaffolding Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes Run an assembly using long mate pair (LMP) reads. Compare results of LMP assemblies with those from the short reads. Run scaffolding and assess the assembly statistics` Time Required 3.5 hrs License The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"denovo-module-scaffolding"},{"location":"scaffolding/#denovo-module-scaffolding","text":"Bioinformatics Training Platform (BTP) Module: Contig Scaffolding Topic Contig Scaffolding Target Audience Biologists Non-bioinformaticians Little to no programming expereience Prerequisites None Key Learning Outcomes Run an assembly using long mate pair (LMP) reads. Compare results of LMP assemblies with those from the short reads. Run scaffolding and assess the assembly statistics` Time Required 3.5 hrs","title":"denovo-module-scaffolding"},{"location":"scaffolding/#license","text":"The contents of this repository are released under the Creative Commons Attribution 3.0 Unported License. For a summary of what this means, please see: http://creativecommons.org/licenses/by/3.0/deed.en_GB","title":"License"},{"location":"scaffolding/datasets/","text":"Information about what datasets are required for a module\u2019s tutorial exercises are described in a data.yaml file. As well describing where to source the dataset files from, we also need to describe where on the filesystem the files should reside and who should own them.","title":"Home"},{"location":"scaffolding/handout/handout/","text":"Key Learning Outcomes After completing this module the trainee should be able to: Run an assembly using long mate pair (LMP) reads. Compare results of LMP assemblies with those from the short reads. Run scaffolding and assess the assembly statistics` Resources You\u2019ll be Using Tools Used Kmer Analysis Tool kit : \\ https://github.com/TGAC/KAT Nextclip : \\ https://github.com/richardmleggett/nextclip Abyss : \\ http://www.bcgsc.ca/platform/bioinfo/software/abyss Soap Denovo : \\ http://soap.genomics.org.cn/soapdenovo.html SOAPec : \\ http://soap.genomics.org.cn/about.html Scaffolding with long mate pair libraries: Chalara scaffolding using LMP Scaffold is made of two or more contigs joined together using the read pair information. In the absence of a high-quality reference genome, new genome assemblies are often evaluated on the basis of the number of scaffolds and contigs required to represent the genome. Other criteria such as the alignment of reads back to assemblies, N50, and the length of contigs and scaffolds relative to the size of the genome can also be used to assess the quality of assemblies. In this excercise we will be using a long mate pair data to perform assembly and scaffolding and we will focus on how using LMP reads can affect the assemblies. Most part of this tutorial has been precomputed and made available to you in interest of time. We will spend more time exploring and discussing the results. Pair-end assembly using Chalara dataset Before doing a scaffolding, we need to build an assembly using the pair-end reads first. Lets go to the correct location where the files are: cd /home/trainee/scaffolding/cha_raw/test_1 You DO NOT need to run this command. This has already been done for you. abyss-pe name=cha1 k=27 in=\"../LIB2570_raw_R1.fastq ../LIB2570_raw_R2.fastq\" np=4 Let\u2019s look at the stats by doing: less cha1-stats.tab [H] lllllllllll n & n:500 & L50 & min & N80 & N50 & N20 & E-size & max & sum & name \\ 394789 & 11681 & 2094 & 500 & 2880 & 6254 & 12303 & 7936 & 44414 & 44.07e6 & cha1-unitigs.fa\\ 394199 & 11673 & 2097 & 500 & 2887 & 6255 & 12303 & 7937 & 44414 & 44.11e6 & cha1-contigs.fa\\ 394161 & 11647 & 2095 & 500 & 2898 & 6269 & 12303 & 7944 & 44414 & 44.12e6 & cha1-scaffolds.fa\\ [tab:chak27] cd /home/trainee/scaffolding/cha_raw/test_2 This is also a pre-computed example for you: abyss-pe name=cha2 k=61 in=\"../LIB2570_raw_R1.fastq ../LIB2570_raw_R2.fastq\" np=4 Again, we need to look at the statstics of our new assembly using k=61: less cha2-stats.tab It should be looking like this: [H] lllllllllll n & n:500 & L50 & min & N80 & N50 & N20 & E-size & max & sum & name \\ 130547 & 12596 & 1770 & 500 & 3352 & 8379 & 16380 & 10518 & 54300 & 50.75e6 & cha2-unitigs.fa\\ 130210 & 12575 & 1771 & 500 & 3363 & 8382 & 16380 & 10525 & 54300 & 50.78e6 & cha2-contigs.fa\\ 130182 & 12555 & 1769 & 500 & 3377 & 8394 & 16380 & 10534 & 54300 & 50.78e6 & cha2-scaffolds.fa\\ [tab:chak61] The assembly contiguity is worse than fusarium, why? Genome characteristics. Data: Coverage Error distributions Read sizes Fragment sizes Kmer spectra Chalara scaffolding using long mate-pair reads(LMP) Let\u2019s put some LMP in there. cd /home/trainee/scaffolding/cha_raw/test_3 This is also a pre-computed example for you: abyss-pe name=chalmp1 k=61 in=\"../LIB2570_raw_R1.fastq ../LIB2570_raw_R2.fastq\" mp=\"lmp1\" lmp1=\"../LIB8209_raw_R1.fastq ../LIB8209_raw_R2.fastq\" np=4 less chalmp1-stats.tab [H] lllllllllll n & n:500 & L50 & min & N80 & N50 & N20 & E-size & max & sum & name \\ 130548 & 12596 & 1770 & 500 & 3352 & 8379 & 16380 & 10518 & 54300 & 50.75e6 & chalmp1-unitigs.fa\\ 130211 & 12575 & 1771 & 500 & 3363 & 8382 & 16380 & 10525 & 54300 & 50.78e6 & chalmp1-contigs.fa\\ 130148 & 12545 & 1769 & 500 & 3380 & 8400 & 16380 & 10535 & 54300 & 50.78e6 & chalmp1-scaffolds.fa\\ [tab:chak27] So\u2026 what happened? Let\u2019s check the data by: Kmer spectra Fragment sizes Any hints on the protocol? And a not-so-obvious property: kat plot spectra-mx --intersection -x 30 -y 15000000 -o pe_vs_lmp-main.mx.spectra-mx.png pe_vs_lmp-main.mx Do you remember LMP require pre-processing? Let\u2019s try with processed LMP. Prior task (already made) preprocess the LMP with nextclip. cd /home/trainee/scaffolding/cha_proc/test_1 This is also a pre-computed example for you: abyss-pe name=chalmpproc1 k=61 in=\"../../cha_raw/LIB2570_raw_R1.fastq ../../cha_raw/LIB2570_raw_R2.fastq\" mp=\"proclmp1\" proclmp1=\"../LIB8209_preproc_R1.fastq ../LIB8209_preproc_R2.fastq\" np=4 less chalmpproc1-stats.tab [H] lllllllllll n & n:500 & L50 & min & N80 & N50 & N20 & E-size & max & sum & name \\ 130548 & 12596 & 1770 & 500 & 3352 & 8379 & 16380 & 10518 & 54300 & 50.75e6 & chalmpproc1-unitigs.fa\\ 130211 & 12575 & 1771 & 500 & 3363 & 8382 & 16380 & 10525 & 54300 & 50.78e6 & chalmpproc1-contigs.fa\\ 122061 & 6679 & 167 & 500 & 18609 & 87510 & 187171 & 106178 & 397967 & 51.15e6 & chalmpproc1-scaffolds.fa\\ [tab:chaklmpk61] That\u2019s much better! Chalara: beyond first pass Do you remember these? Genome characteristics. Data: Coverage Error distributions Read sizes Fragment sizes Kmer spectra Let\u2019s think a bit and try to improve the assembly\u2026 If you look at the pre-processed LMP, do you notice anything peculiar?\\ Needs to fill Testing the inclussion of heavily pre-procesed LMP coverage into the DBG cd /home/trainee/scaffolding/cha_proc/test_2 This is also a pre-computed example for you: abyss-pe name=chalmp2 k=61 se=\"../LIB8209_preproc_single.fastq\" lib=\"lmp1\" lmp1=\"../LIB8209_preproc_R1.fastq ../LIB8209_preproc_R2.fastq\" np=4 &gt;chaproclmp2.log 2&gt;&amp;1 less chalmp2-stats.tab Let\u2019s look at the stats. [H] lllllllllll n & n:500 & L50 & min & N80 & N50 & N20 & E-size & max & sum & name \\ 128306 & 10150 & 1182 & 500 & 4954 & 12585 & 24777 & 15693 & 68684 & 51.03e6 & chaproclmp4-unitigs.fa\\ 118939 & 5772 & 362 & 500 & 15717 & 41870 & 82033 & 52819 & 252066 & 52.24e6 & chaproclmp4-contigs.fa\\ 116139 & 4014 & 141 & 500 & 41145 & 109273 & 224986 & 133450 & 460979 & 52.56e6 & chaproclmp4-scaffolds.fa\\ [tab:chaklmp2-k61] References Robert Ekblom* andJochen B. W. Wolf. \u201cA field guide to whole-genome sequencing, assembly and annotation\u201d Evolutionary Applications Special Issue: Evolutionary Conservation Volume 7, Issue 9, pages 1026 \u2013 1042, November 2014 De novo genome assembly: what every biologist should know Nature Methods 9, 333 \u2013 337 (2012) doi:10.1038/nmeth.1935","title":"Scaffolding"},{"location":"scaffolding/handout/handout/#key-learning-outcomes","text":"After completing this module the trainee should be able to: Run an assembly using long mate pair (LMP) reads. Compare results of LMP assemblies with those from the short reads. Run scaffolding and assess the assembly statistics`","title":"Key Learning Outcomes"},{"location":"scaffolding/handout/handout/#resources-youll-be-using","text":"","title":"Resources You\u2019ll be Using"},{"location":"scaffolding/handout/handout/#tools-used","text":"Kmer Analysis Tool kit : \\ https://github.com/TGAC/KAT Nextclip : \\ https://github.com/richardmleggett/nextclip Abyss : \\ http://www.bcgsc.ca/platform/bioinfo/software/abyss Soap Denovo : \\ http://soap.genomics.org.cn/soapdenovo.html SOAPec : \\ http://soap.genomics.org.cn/about.html","title":"Tools Used"},{"location":"scaffolding/handout/handout/#scaffolding-with-long-mate-pair-libraries-chalara-scaffolding-using-lmp","text":"Scaffold is made of two or more contigs joined together using the read pair information. In the absence of a high-quality reference genome, new genome assemblies are often evaluated on the basis of the number of scaffolds and contigs required to represent the genome. Other criteria such as the alignment of reads back to assemblies, N50, and the length of contigs and scaffolds relative to the size of the genome can also be used to assess the quality of assemblies. In this excercise we will be using a long mate pair data to perform assembly and scaffolding and we will focus on how using LMP reads can affect the assemblies. Most part of this tutorial has been precomputed and made available to you in interest of time. We will spend more time exploring and discussing the results.","title":"Scaffolding with long mate pair libraries: Chalara scaffolding using LMP"},{"location":"scaffolding/handout/handout/#pair-end-assembly-using-chalara-dataset","text":"Before doing a scaffolding, we need to build an assembly using the pair-end reads first. Lets go to the correct location where the files are: cd /home/trainee/scaffolding/cha_raw/test_1 You DO NOT need to run this command. This has already been done for you. abyss-pe name=cha1 k=27 in=\"../LIB2570_raw_R1.fastq ../LIB2570_raw_R2.fastq\" np=4 Let\u2019s look at the stats by doing: less cha1-stats.tab [H] lllllllllll n & n:500 & L50 & min & N80 & N50 & N20 & E-size & max & sum & name \\ 394789 & 11681 & 2094 & 500 & 2880 & 6254 & 12303 & 7936 & 44414 & 44.07e6 & cha1-unitigs.fa\\ 394199 & 11673 & 2097 & 500 & 2887 & 6255 & 12303 & 7937 & 44414 & 44.11e6 & cha1-contigs.fa\\ 394161 & 11647 & 2095 & 500 & 2898 & 6269 & 12303 & 7944 & 44414 & 44.12e6 & cha1-scaffolds.fa\\ [tab:chak27] cd /home/trainee/scaffolding/cha_raw/test_2 This is also a pre-computed example for you: abyss-pe name=cha2 k=61 in=\"../LIB2570_raw_R1.fastq ../LIB2570_raw_R2.fastq\" np=4 Again, we need to look at the statstics of our new assembly using k=61: less cha2-stats.tab It should be looking like this: [H] lllllllllll n & n:500 & L50 & min & N80 & N50 & N20 & E-size & max & sum & name \\ 130547 & 12596 & 1770 & 500 & 3352 & 8379 & 16380 & 10518 & 54300 & 50.75e6 & cha2-unitigs.fa\\ 130210 & 12575 & 1771 & 500 & 3363 & 8382 & 16380 & 10525 & 54300 & 50.78e6 & cha2-contigs.fa\\ 130182 & 12555 & 1769 & 500 & 3377 & 8394 & 16380 & 10534 & 54300 & 50.78e6 & cha2-scaffolds.fa\\ [tab:chak61] The assembly contiguity is worse than fusarium, why? Genome characteristics. Data: Coverage Error distributions Read sizes Fragment sizes Kmer spectra","title":"Pair-end assembly using Chalara dataset"},{"location":"scaffolding/handout/handout/#chalara-scaffolding-using-long-mate-pair-readslmp","text":"Let\u2019s put some LMP in there. cd /home/trainee/scaffolding/cha_raw/test_3 This is also a pre-computed example for you: abyss-pe name=chalmp1 k=61 in=\"../LIB2570_raw_R1.fastq ../LIB2570_raw_R2.fastq\" mp=\"lmp1\" lmp1=\"../LIB8209_raw_R1.fastq ../LIB8209_raw_R2.fastq\" np=4 less chalmp1-stats.tab [H] lllllllllll n & n:500 & L50 & min & N80 & N50 & N20 & E-size & max & sum & name \\ 130548 & 12596 & 1770 & 500 & 3352 & 8379 & 16380 & 10518 & 54300 & 50.75e6 & chalmp1-unitigs.fa\\ 130211 & 12575 & 1771 & 500 & 3363 & 8382 & 16380 & 10525 & 54300 & 50.78e6 & chalmp1-contigs.fa\\ 130148 & 12545 & 1769 & 500 & 3380 & 8400 & 16380 & 10535 & 54300 & 50.78e6 & chalmp1-scaffolds.fa\\ [tab:chak27] So\u2026 what happened? Let\u2019s check the data by: Kmer spectra Fragment sizes Any hints on the protocol? And a not-so-obvious property: kat plot spectra-mx --intersection -x 30 -y 15000000 -o pe_vs_lmp-main.mx.spectra-mx.png pe_vs_lmp-main.mx Do you remember LMP require pre-processing? Let\u2019s try with processed LMP. Prior task (already made) preprocess the LMP with nextclip. cd /home/trainee/scaffolding/cha_proc/test_1 This is also a pre-computed example for you: abyss-pe name=chalmpproc1 k=61 in=\"../../cha_raw/LIB2570_raw_R1.fastq ../../cha_raw/LIB2570_raw_R2.fastq\" mp=\"proclmp1\" proclmp1=\"../LIB8209_preproc_R1.fastq ../LIB8209_preproc_R2.fastq\" np=4 less chalmpproc1-stats.tab [H] lllllllllll n & n:500 & L50 & min & N80 & N50 & N20 & E-size & max & sum & name \\ 130548 & 12596 & 1770 & 500 & 3352 & 8379 & 16380 & 10518 & 54300 & 50.75e6 & chalmpproc1-unitigs.fa\\ 130211 & 12575 & 1771 & 500 & 3363 & 8382 & 16380 & 10525 & 54300 & 50.78e6 & chalmpproc1-contigs.fa\\ 122061 & 6679 & 167 & 500 & 18609 & 87510 & 187171 & 106178 & 397967 & 51.15e6 & chalmpproc1-scaffolds.fa\\ [tab:chaklmpk61] That\u2019s much better!","title":"Chalara scaffolding using long mate-pair reads(LMP)"},{"location":"scaffolding/handout/handout/#chalara-beyond-first-pass","text":"Do you remember these? Genome characteristics. Data: Coverage Error distributions Read sizes Fragment sizes Kmer spectra Let\u2019s think a bit and try to improve the assembly\u2026 If you look at the pre-processed LMP, do you notice anything peculiar?\\ Needs to fill Testing the inclussion of heavily pre-procesed LMP coverage into the DBG cd /home/trainee/scaffolding/cha_proc/test_2 This is also a pre-computed example for you: abyss-pe name=chalmp2 k=61 se=\"../LIB8209_preproc_single.fastq\" lib=\"lmp1\" lmp1=\"../LIB8209_preproc_R1.fastq ../LIB8209_preproc_R2.fastq\" np=4 &gt;chaproclmp2.log 2&gt;&amp;1 less chalmp2-stats.tab Let\u2019s look at the stats. [H] lllllllllll n & n:500 & L50 & min & N80 & N50 & N20 & E-size & max & sum & name \\ 128306 & 10150 & 1182 & 500 & 4954 & 12585 & 24777 & 15693 & 68684 & 51.03e6 & chaproclmp4-unitigs.fa\\ 118939 & 5772 & 362 & 500 & 15717 & 41870 & 82033 & 52819 & 252066 & 52.24e6 & chaproclmp4-contigs.fa\\ 116139 & 4014 & 141 & 500 & 41145 & 109273 & 224986 & 133450 & 460979 & 52.56e6 & chaproclmp4-scaffolds.fa\\ [tab:chaklmp2-k61]","title":"Chalara: beyond first pass"},{"location":"scaffolding/handout/handout/#references","text":"Robert Ekblom* andJochen B. W. Wolf. \u201cA field guide to whole-genome sequencing, assembly and annotation\u201d Evolutionary Applications Special Issue: Evolutionary Conservation Volume 7, Issue 9, pages 1026 \u2013 1042, November 2014 De novo genome assembly: what every biologist should know Nature Methods 9, 333 \u2013 337 (2012) doi:10.1038/nmeth.1935","title":"References"},{"location":"scaffolding/presentations/","text":"This directory is intended to store presentation files used to provide context to the hands-on aspect of the module.","title":"Home"},{"location":"scaffolding/tools/","text":"The tools used in the module\u2019s tutorial exercises are captured in a tools.yaml file. This file can then be used by a workshop deployment script to download and install the tools required by the module.","title":"Home"},{"location":"trainers/trainers/","text":"The Trainers Mr Simon Michnowichz HPC Team Monash eResearch Centre Mr Jerico Revote Cloud Team Monash eResearch Centre Ms Gulrez Chahal ARMI Monash University Dr. Sonika Tyagi Bioinformatics Lab Head School of Biological Sciences Monash University","title":"The Trainers"},{"location":"trainers/trainers/#the-trainers","text":"Mr Simon Michnowichz HPC Team Monash eResearch Centre Mr Jerico Revote Cloud Team Monash eResearch Centre Ms Gulrez Chahal ARMI Monash University Dr. Sonika Tyagi Bioinformatics Lab Head School of Biological Sciences Monash University","title":"The Trainers"}]}